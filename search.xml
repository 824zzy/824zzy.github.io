<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Navigating Misinformation - How to identify and verify what you see on the web</title>
      <link href="/2022/05/26/self-directed-course-navigating-misinformation/"/>
      <url>/2022/05/26/self-directed-course-navigating-misinformation/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>This article is a learning note of the self-directed course <a href="https://journalismcourses.org/course/misinformation/">Navigating Misinformation - How to identify and verify what you see on the web</a>.</p><p>The main purpose of this course is to help researchers learn how fact-checkers identify and verify online content, namely how responsible reporting works in an age of misinformation/disinformation.<br>The topics below are involved in this course:</p><ol><li>Discovery of problematic content</li><li>Basic verification of online sources</li><li>Advanced verification of online sources<ol><li>How date and time stamps work on social posts</li><li>How to geo-locate where a photo or video was taken</li><li>Tools overview to help determine the time in a photo or video</li><li>Verification challenge</li></ol></li></ol><h2 id="Discovery-of-problematic-content"><a href="#Discovery-of-problematic-content" class="headerlink" title="Discovery of problematic content"></a>Discovery of problematic content</h2><p>To keep track of the misleading claims and content, journalists are monitoring multiple social media. The first and foremost thing to figure out is <strong>what should be monitored - groups and/or topics</strong>, and what you choose will depend on the social platform. In general, journalists use Reddit, Facebook and Twitter as information sources.</p><h3 id="Information-sources"><a href="#Information-sources" class="headerlink" title="Information sources"></a>Information sources</h3><h4 id="Reddit"><a href="#Reddit" class="headerlink" title="Reddit"></a>Reddit</h4><p>Reddit is the eighth most popular website in the world even more popular than Twitter. <strong>Misinformation ends up circulating widely on Facebook and Twitter often appears on Reddit first</strong>. Reddit is made up of a collection of open forums called <strong>subreddit</strong>, the subreddit can be discovered through the general search page. Once you have found an interesting subreddit, you can search for its name to discover similar subreddits. Also, keep an eye out for new subreddits mentioned in the comments.</p><h4 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h4><p>There are two key ways to monitor Twitter activity: <strong>terms and lists</strong>.<br>The terms include keywords, domains, hashtags and usernames. More specifically, journalists focus on those websites and particular accounts that are likely to produce misleading content, and tweets that include certain keywords or hashtags, like “snowflakes” and “#Lockherup”. The <a href="https://developer.twitter.com/en/docs/api-reference-index">Twitter Search API</a> provides a powerful way to form a query. Below are the example of Twitter search operators:</p><p><img src="https://firstdraftnews.org/wp-content/uploads/2017/07/tweets_monitoring.png" alt="Twitter search operators"></p><p>On the other hand, using Twitter lists is another effective way of quickly putting together groups of accounts to monitor. The lists can be created by any Twitter user who is following a group of accounts as a unit. Journalists use Twitter lists to capitalize on the expertise of other journalists, however, Twitter hasn’t provided an API to easily search Twitter lists based on keywords. Thus, we have to utilize a Google hack to search through Twitter lists.<br>The hack is: for any topic keywords that you are interested in, add the query <code>site:twitter.com/*/lists [keywords]</code> in the google search bar. Google will return keyword-related public lists of all Twitter users. What’s more, by going to the list creator’s profile and clicking <code>More</code> and then <code>Lists</code>, you can find more lists that potentially attract you.<br>And by keep doing so recursively, you can combine the lists that you have found into a super list.</p><h4 id="Facebook"><a href="#Facebook" class="headerlink" title="Facebook"></a>Facebook</h4><p>The potential to monitor Facebook is narrower due to two reasons. First, the content available is designated public by users. Second, Facebook does not support direct, programmatic access to the public feed.</p><h3 id="Monitoring-Reddit-Facebook-and-Instagram-with-CrowdTangle"><a href="#Monitoring-Reddit-Facebook-and-Instagram-with-CrowdTangle" class="headerlink" title="Monitoring Reddit, Facebook and Instagram with CrowdTangle"></a>Monitoring Reddit, Facebook and Instagram with CrowdTangle</h3><p><a href="https://www.crowdtangle.com/">Crowdtangle</a> was made free after being acquired by Facebook. It takes search queries, groups and pages and creates custom social feeds for Facebook, Instagram, and Reddit. If you give it a search query, it creates a feed of posts from the platform that match that query. If you give it a list of accounts, it creates a feed of posts from those accounts.</p><p><img src="https://firstdraftnews.org/wp-content/uploads/2017/08/crowdtangle.png" alt="crowdtange user interface"></p><h3 id="Monitoring-Twitter-with-TweetDeck"><a href="#Monitoring-Twitter-with-TweetDeck" class="headerlink" title="Monitoring Twitter with TweetDeck"></a>Monitoring Twitter with TweetDeck</h3><p>By far, the easiest way to monitor multiple Twitter streams in real-time is TweetDeck. With Tweetdeck, you can arrange an unlimited number of real-time streams of tweets side-by-side in columns that can easily be cycled through.</p><p><img src="https://firstdraftnews.org/wp-content/uploads/2017/08/tweetdeck.png" alt="TweetDeck"></p><h2 id="Basic-verification-of-online-sources"><a href="#Basic-verification-of-online-sources" class="headerlink" title="Basic verification of online sources"></a>Basic verification of online sources</h2><p>When attempting to verify a piece of content, journalists always investigate five elements:</p><ol><li><p><strong>Provenance</strong>: verify if the content is original</p><p> If we are not looking at the original, all the metadata about the source and date will be wrong and useless. The journalists are facing the challenge that footage can easily jump from platform to platform or prevail inside a platform, thus we should always be suspicious about the content originality.</p></li><li><p><strong>Source</strong>: verify who created the content</p><p> Note that source means who captured the content instead of who uploaded the content. To verify the source, one can depend on two aspects: directly contact the user and check the user location and event location are the same.</p></li><li><p><strong>Date</strong>: verify when the content captured</p><p> Never assuming the content uploaded date is when the content was captured.</p></li><li><p><strong>Location</strong>: verify where the content captured</p><p> The geolocation can be easily manipulated on social media platforms, so it is better to double-check the location on a map or satellite image.</p></li><li><p><strong>Motivation</strong>: verify why the content captured</p><p> The user can be an accidental eyewitness or a responsible stakeholder.</p></li></ol><p>With the help of reversed image search tools such as <a href="https://images.google.com/">Google Images</a> and <a href="https://chrome.google.com/webstore/detail/reveye-reverse-image-sear/keaaclcjhehbbapnphnmpiklalfhelgf?hl=en">RevEye</a>, one can easily accomplish the verification.</p><h2 id="Advanced-verification-of-online-sources"><a href="#Advanced-verification-of-online-sources" class="headerlink" title="Advanced verification of online sources"></a>Advanced verification of online sources</h2><ol><li><p>Wolfram Alpha</p><p> Wolfram Alpha is a knowledge engine that brings available information from across the web. It has a powerful tool for checking the weather from any particular location on any date. When you are trying to <strong>double-check the date on an image or video</strong>, it can be very useful.</p></li><li><p>Shadow Analysis</p><p> Check if the shadow is in the right shape, right length and the same direction.</p></li><li><p>Geo-location</p><p> Only a very small percentage of social media posts are geo-tagged by users themselves. Luckily <a href="https://www.google.com/maps/">high-quality satellite and street view imagery</a> allows you to pace yourself on the map and stand in the place that the user was standing when they captured the footage.</p></li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://firstdraftnews.org/articles/monitor-social-media/">How to begin to monitor social media for misinformation</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Factchecking </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Deecamp-28组AGI第一次沙龙分享活动纪要</title>
      <link href="/2019/07/21/AGI-salon-1/"/>
      <url>/2019/07/21/AGI-salon-1/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Deecamp-28组AGI沙龙活动纪要"><a href="#Deecamp-28组AGI沙龙活动纪要" class="headerlink" title="Deecamp-28组AGI沙龙活动纪要"></a>Deecamp-28组AGI沙龙活动纪要</h2><ul><li>日期：2019年7月21日周日晚上8点-10点30分</li><li>地点：国科大教一楼132</li><li>轮值主持人： 朱正源</li><li>沙龙主题: 量化投资模型分享及实践展示</li></ul><h2 id="沙龙内容"><a href="#沙龙内容" class="headerlink" title="沙龙内容"></a>沙龙内容</h2><h3 id="主持人宣讲"><a href="#主持人宣讲" class="headerlink" title="主持人宣讲"></a>主持人宣讲</h3><p><img src="https://user-images.githubusercontent.com/13566583/61606864-d7ecfc00-ac7e-11e9-9dac-80333cea971e.jpg" alt="agi-host"></p><h3 id="刘兆丰"><a href="#刘兆丰" class="headerlink" title="刘兆丰"></a>刘兆丰</h3><blockquote><p>分享了<strong>Using Deep Reinforcement Learning to Trade</strong>, 介绍了强化学习的发展历程，讲解了一种结合深度神经网络、递归神经网络和强化学习的量化交易决策模型，并展示了该模型在真实数据中的实验结果。<br><img src="https://user-images.githubusercontent.com/13566583/61606998-6bbec800-ac7f-11e9-8897-4da210ada6f3.jpg" alt="8061563762930_ pic_hd"><br><img src="https://user-images.githubusercontent.com/13566583/61607007-737e6c80-ac7f-11e9-814f-7a1fad39e1a6.jpg" alt="8071563762947_ pic_hd"></p></blockquote><h3 id="朱正源"><a href="#朱正源" class="headerlink" title="朱正源"></a>朱正源</h3><blockquote><p>分享了<a href="https://docs.google.com/presentation/d/1W7RGD3X_MZB3dfzaTrQdGYzv_zuay2JpYOTQUjXzK5A/edit#slide=id.g4461849552_8_1825">Introduction to Quantitative Investment with Deep Learning</a>, 分享了有关量化投资方向的股票预测模型，使用工业届常用的时间序列回归模型，通过预先构建多种假设，使用LSTM进行滑窗预测，介绍了量化交易中的真实情况，股市有风险，入市需谨慎～<br><img src="https://user-images.githubusercontent.com/13566583/61607014-7bd6a780-ac7f-11e9-801e-524c6cbea4c3.jpg" alt="8081563762955_ pic_hd"><br><img src="https://user-images.githubusercontent.com/13566583/61607019-7da06b00-ac7f-11e9-84ab-703ba1ad8525.jpg" alt="8091563762967_ pic_hd"></p></blockquote><h3 id="葛景琳"><a href="#葛景琳" class="headerlink" title="葛景琳"></a>葛景琳</h3><blockquote><p><strong>设计人员分享</strong>：介绍前期调研的几个阶段及调研目的，分享新产品项目推进的流程概况。</p></blockquote><p><img src="https://user-images.githubusercontent.com/13566583/61606932-23070f00-ac7f-11e9-903b-2874c5ec77fb.jpg" alt="8101563762973_ pic_hd"><br><img src="https://user-images.githubusercontent.com/13566583/61606934-269a9600-ac7f-11e9-8e03-d950207daf6f.jpg" alt="8111563762981_ pic_hd"></p><h2 id="集体合照"><a href="#集体合照" class="headerlink" title="集体合照"></a>集体合照</h2><p><img src="https://user-images.githubusercontent.com/13566583/61606966-4b8f0900-ac7f-11e9-876f-d2d8e60502ce.jpg" alt="8041563762837_ pic_hd"></p><h2 id="沙龙讨论内容"><a href="#沙龙讨论内容" class="headerlink" title="沙龙讨论内容"></a>沙龙讨论内容</h2><ol><li>在活动开始前建议调整设备</li><li>沙龙注重时间控制</li><li>不要拘束，不用师兄师姐的称呼，不要过分自谦</li><li>Demo展示具体形式需要等待产业导师就位再做定夺</li><li>下次沙龙暂定于下个没有课的晚上</li></ol><h2 id="特别鸣谢Deecamp全体人员对本沙龙的支持与帮助"><a href="#特别鸣谢Deecamp全体人员对本沙龙的支持与帮助" class="headerlink" title="特别鸣谢Deecamp全体人员对本沙龙的支持与帮助~"></a>特别鸣谢Deecamp全体人员对本沙龙的支持与帮助~</h2>]]></content>
      
      
      <categories>
          
          <category> Quant </category>
          
      </categories>
      
      
        <tags>
            
            <tag> agi </tag>
            
            <tag> salon </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepInvestment introduction</title>
      <link href="/2019/07/12/DeepInvestment/"/>
      <url>/2019/07/12/DeepInvestment/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="The-slide-shows-all-you-need"><a href="#The-slide-shows-all-you-need" class="headerlink" title="The slide shows all you need"></a>The slide shows all you need</h2><p><strong>Whose money I want to make</strong>: Essentially according to Game Theory</p><p><a href="https://docs.google.com/presentation/d/1W7RGD3X_MZB3dfzaTrQdGYzv_zuay2JpYOTQUjXzK5A/edit#slide=id.g4461849552_8_1825">Introduction to Quantitative Investment with Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deepLearning </tag>
            
            <tag> quantitativeInvestment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mutation test and Deep Learning</title>
      <link href="/2019/06/09/mutationtest-deeplearning/"/>
      <url>/2019/06/09/mutationtest-deeplearning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Brief-Introduction-to-Mutation-Test"><a href="#Brief-Introduction-to-Mutation-Test" class="headerlink" title="Brief Introduction to Mutation Test"></a>Brief Introduction to Mutation Test</h2><blockquote><p>Mutation testing is a mature technology for testing data quality assessment in traditional software.<br>Mutation testing is a form of white-box testing.<br>Mutation testing (or mutation analysis or program mutation) is used to design new software tests and evaluate the quality of existing software tests. </p></blockquote><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>The goals of mutation testing are multiple:</p><ul><li>identify weakly tested pieces of code (those for which mutants are not killed)</li><li>identify weak tests (those that never kill mutants)</li><li>compute the mutation score</li><li>learn about error propagation and state infection in the program</li></ul><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Selecting some <strong>mutation operations</strong>, and applying them to the source code for each executable code segment in turn. </p><p>The result of using a mutation operation on a program is called a mutant heterogeneity. </p><p><strong>If the test unit can detect the error (ie, a test fails), then the mutant is said to have been killed.</strong></p><h4 id="Foo"><a href="#Foo" class="headerlink" title="Foo"></a>Foo</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">x: <span class="built_in">int</span>, y: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">If x&gt;<span class="number">0</span> <span class="keyword">and</span> y&gt;<span class="number">0</span>:</span><br><span class="line">z = x</span><br><span class="line"><span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">x: <span class="built_in">int</span>, y: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">If x&gt;<span class="number">0</span> <span class="keyword">and</span> y&gt;=<span class="number">0</span>:</span><br><span class="line">z = x</span><br><span class="line"><span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><p>Given some test cases, we find that unit test cannot find variants<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Success</span><br><span class="line">assertEquals(<span class="number">2</span>, foo(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">assertEquals(<span class="number">0</span>, foo(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">assertEquals(<span class="number">0</span>, foo(-<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><br>Add new tests to achieve the effect of eliminating variants:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line">assertEquals(<span class="number">0</span>, foo(<span class="number">2</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><h4 id="Bar"><a href="#Bar" class="headerlink" title="Bar"></a>Bar</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>(<span class="params">a: <span class="built_in">int</span>, b:<span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> a <span class="keyword">and</span> b:</span><br><span class="line">        c = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"><span class="comment"># Here is an mutation which operator is `and`</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>(<span class="params">a: <span class="built_in">int</span>, b:<span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> a <span class="keyword">or</span> b:</span><br><span class="line">        c = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><p>Given a test case that will absolutely pass:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Success</span><br><span class="line">assertEquals(<span class="number">1</span>, foo(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><br>But we need to kill the mutation by adding more test cases:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Failed:</span><br><span class="line">assertEquals(<span class="number">0</span>, foo(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">assertEquals(<span class="number">0</span>, foo(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">assertEquals(<span class="number">1</span>, foo(<span class="number">0</span>, <span class="number">0</span>))   </span><br></pre></td></tr></table></figure></p><h3 id="Inspriation"><a href="#Inspriation" class="headerlink" title="Inspriation"></a>Inspriation</h3><p>In deep learning, you can also create variants by changing the operators in the model. </p><p>Adding the idea of ​​the mutation test to the deep learning model, if the performance of the model after the mutation is unchanged, then there is a problem with the test set</p><p>It is necessary to add or generate higher quality test data to achieve the data enhancement effect.</p><h2 id="A-comparison-of-traditional-and-DL-software-development"><a href="#A-comparison-of-traditional-and-DL-software-development" class="headerlink" title="A comparison of traditional and DL software development"></a>A comparison of traditional and DL software development</h2><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18gy1g3uypojkkyj20z20kc77j.jpg" alt=""></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/Mutation_testing">wiki: Mutation Testing</a></li><li><a href="https://www.testwo.com/article/869">突变测试——通过一个简单的例子快速学习这种有趣的测试技术</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deepLearning </tag>
            
            <tag> mutationTest </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Comparison of ON-LSTM and DIORA</title>
      <link href="/2019/05/31/ON-LSTM-and-DIORA/"/>
      <url>/2019/05/31/ON-LSTM-and-DIORA/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="ON-LSTM"><a href="#ON-LSTM" class="headerlink" title="ON-LSTM"></a>ON-LSTM</h2><p>Insprition under the hood: How to introduce grammar tree structure into LSTM in an unsupervised apporach.</p><h3 id="Introduction-Ordered-Neurons-ON"><a href="#Introduction-Ordered-Neurons-ON" class="headerlink" title="Introduction: Ordered Neurons(ON)"></a>Introduction: Ordered Neurons(ON)</h3><ol><li>The neurons inside ON-LSTM are specifically <code>ordered</code> to <code>express richer information</code>: Change the order of update frequency.</li><li>The specific order of neurons is to integrate the hierarchical structure (tree structure) into the LSTM, allowing the LSTM to <code>automatically learn the hierarchical structure</code>.</li><li><code>High/Low level information</code>: Should keep longer/shorter in corresponding coding interval.</li><li><code>cumax()</code>: A special function to internate special $F1$ and $F2$gate.</li></ol><h3 id="The-nuts-and-bolts-in-Mathematic"><a href="#The-nuts-and-bolts-in-Mathematic" class="headerlink" title="The nuts and bolts in Mathematic"></a>The nuts and bolts in Mathematic</h3><div class="row"><iframe src="https://drive.google.com/file/d/1UjxnKAcMtydDEr_-PuvVMRLK_2pjx80M/preview" style="width:100%; height:550px"></iframe></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://kexue.fm/archives/6621">ON-LSTM：用有序神经元表达层次结构</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using Scheduled Sample to improve sentence quality</title>
      <link href="/2019/05/10/schedule-sampling/"/>
      <url>/2019/05/10/schedule-sampling/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><strong>Note that the author is not Yoshua Bengio</strong></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>In Seq2Seq sequence learning task, using Scheduled Sampling can improve performance of RNN model.</p><p>The ditribution bewteen traning stage and evaluating stage are different and reults in  <strong>error accumulation question</strong> in evaluating stage. </p><p>The former methods deal with this error accumullation problem is <code>Teacher Forcing</code>.</p><p>Scheduled Sampling can solve the problem through take generated words as input for decoder in certain probability. </p><p>Note that scheduled sampling is only applied in training stage.</p><h2 id="Algorithm-Details"><a href="#Algorithm-Details" class="headerlink" title="Algorithm Details"></a>Algorithm Details</h2><p>In training stage, when generate word $t$, Instead of take ground truth word $y_{t<em>1}$ as input, Scheduled Sampling take previous generated word $g</em>{t-1}$ in certain probability.</p><p>Assume that in $i_{th}$ mini-batch, Schduled Sampling define a probability $\epsilon_i$ to control the input of decoder. And $\epsilon_i$ is a probability variable that decreasing as $i$ increasing.</p><p>There are three decreasing methods:<br>$$Linear Decay: \epsilon_i = max(\epsilon, (k-c)*i), where \epsilon restrict minimum of \epsilon_i, k and c controll the range of decay$$<br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2wav8w8fvj20hs0d841u.jpg" alt=""></p><p><strong>Warning</strong>:<br>In time step $t$, Scheduled Sampling will take $y_{t-1}$ according to $\epsilon<em>i$ as input. And take $g</em>{t-1}$ according to $1-\epsilon_i$ as input.</p><p>As a result, decoder will tend to use generated word as input.</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--scheduled_sampling_start&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, <span class="built_in">help</span>=<span class="string">&#x27;at what epoch to start decay gt probability, -1 means never&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--scheduled_sampling_increase_every&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>,<span class="built_in">help</span>=<span class="string">&#x27;every how many epochs to increase scheduled sampling probability&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--scheduled_sampling_increase_prob&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.05</span>,<span class="built_in">help</span>=<span class="string">&#x27;How much to update the prob&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--scheduled_sampling_max_prob&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.25</span>,<span class="built_in">help</span>=<span class="string">&#x27;Maximum scheduled sampling prob.&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="Assign-scheduled-sampling-probability"><a href="#Assign-scheduled-sampling-probability" class="headerlink" title="Assign scheduled sampling probability"></a>Assign scheduled sampling probability</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scheduled sampling probability is min(epoch*0.01, 0.25)</span></span><br><span class="line">frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every</span><br><span class="line">opt.ss_prob = <span class="built_in">min</span>(opt.scheduled_sampling_increase_prob * frac, opt.scheduled_sampling_max_prob)</span><br><span class="line">model.ss_prob = opt.ss_prob</span><br><span class="line"></span><br><span class="line"><span class="comment"># choose the word when decoding</span></span><br><span class="line"><span class="keyword">if</span> self.ss_prob &gt; <span class="number">0.0</span>:</span><br><span class="line">    sample_prob = torch.FloatTensor(batch_size).uniform_(<span class="number">0</span>, <span class="number">1</span>).cuda()</span><br><span class="line">    sample_mask = sample_prob &lt; self.ss_prob</span><br><span class="line">    <span class="keyword">if</span> sample_mask.<span class="built_in">sum</span>() == <span class="number">0</span>: <span class="comment"># use ground truth</span></span><br><span class="line">        last_word = caption[:, i].clone()</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># use previous generated words</span></span><br><span class="line">        sample_ind = sample_mask.nonzero().view(-<span class="number">1</span>)</span><br><span class="line">        last_word = caption[:, i].data.clone()</span><br><span class="line">        <span class="comment"># fetch prev distribution: shape Nx(M+1)</span></span><br><span class="line">        prob_prev = torch.exp(log_probs.data)</span><br><span class="line">        last_word.index_copy_(<span class="number">0</span>, sample_ind,</span><br><span class="line">                                torch.multinomial(prob_prev, <span class="number">1</span>).view(-<span class="number">1</span>).index_select(<span class="number">0</span>, sample_ind))</span><br><span class="line">        last_word = Variable(last_word)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    last_word = caption[:, i].clone()</span><br></pre></td></tr></table></figure><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2wdkxqmgvj20sg09ywnl.jpg" alt=""></p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ul><li><a href="https://cloud.tencent.com/developer/article/1081168">【序列到序列学习】使用Scheduled Sampling改善翻译质量</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> videoCaptioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>basic_knowledge_supplement</title>
      <link href="/2019/05/06/basic-knowledge-supplement/"/>
      <url>/2019/05/06/basic-knowledge-supplement/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a>Basic knowledge</h2><h3 id="Bias-and-variance"><a href="#Bias-and-variance" class="headerlink" title="Bias and variance"></a>Bias and variance</h3><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2rmgy3ib1j208s0dhtbb.jpg" alt=""></p><ol><li>Bias:</li></ol><p>Represent fitting ability, a naive model will lead to high bias because of underfitting.</p><ol><li>Variance</li></ol><p>Represent stability, a complex model will lead to high variance beacause of overfitting.</p><p>$$Generalization error = Bias^2 + Variance + Irreducible Error$$</p><h3 id="Generative-model-and-Discriminative-Model"><a href="#Generative-model-and-Discriminative-Model" class="headerlink" title="Generative model and Discriminative Model"></a>Generative model and Discriminative Model</h3><ol><li>Discriminative Model</li></ol><p>Learn a <code>function</code> or <code>conditional probability model P(X|Y)</code>(posterior probability) directly.</p><ol><li>Generative Model<br>Learn a <code>joint probability model P(X, Y)</code> then to calculate <code>P(Y|X)</code></li></ol><h3 id="Search-hyper-parameter"><a href="#Search-hyper-parameter" class="headerlink" title="Search hyper-parameter"></a>Search hyper-parameter</h3><ol><li>Grid search</li><li>Random search</li></ol><h3 id="Euclidean-distance-and-Cosine-distance"><a href="#Euclidean-distance-and-Cosine-distance" class="headerlink" title="Euclidean distance and Cosine distance"></a>Euclidean distance and Cosine distance</h3><p>Example: A=[2, 2, 2] B=[5, 5, 5] represents <code>two</code> review scores of <code>three</code> movie.<br>the Euclidean distance is $\sqrt{3^2 + 3^2 + 3^2}$, and the Cosine distance is $1$. As a result, Cosine distance can avoid of difference</p><p>After normalization, essentially they are the same,<br>$$D=(x-y)^2 = x^2+y^2-2|x||y|cosA = 2-2cosA，D=2(1-cosA)$$</p><h3 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h3><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2rmrezxi4j20wq042t9q.jpg" alt=""></p><ol><li>accuracy: $ACC = \frac{TP+TN}{TP+FN+FP+FN}$</li><li>precison: $P = \frac{TP}{TP+FP}$</li><li>recall: $R = \frac{TP}{TP+FN}$</li><li>F1: $F_1 = \frac{2TP}{2TP+FP+FN}$</li></ol><h3 id="deal-with-missing-value"><a href="#deal-with-missing-value" class="headerlink" title="deal with missing value"></a>deal with missing value</h3><ol><li>More missing value: drop feature column.</li><li>Less missing value: fill a value<ol><li>Fill outlier: <code>data.fillna(0)</code></li><li>Fill mean value: <code>data.fillna(data.mean())</code></li></ol></li></ol><h3 id="Describe-your-project"><a href="#Describe-your-project" class="headerlink" title="Describe your project"></a>Describe your project</h3><ol><li>Abstract reality to math problem</li><li>Describe your data</li><li>Proprocessing and feature selection</li><li>Model training and tuning</li></ol><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><h3 id="Logistic-regreesion"><a href="#Logistic-regreesion" class="headerlink" title="Logistic regreesion"></a>Logistic regreesion</h3><h4 id="Defination"><a href="#Defination" class="headerlink" title="Defination"></a>Defination</h4><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2ro85d70dj209q017mwy.jpg" alt=""></p><h4 id="Loss-negative-log-los"><a href="#Loss-negative-log-los" class="headerlink" title="Loss: negative log los"></a>Loss: negative log los</h4><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2roe9667aj20ay04y0sp.jpg" alt=""></p><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ol><li>ID3: use <code>information gain</code></li><li>C4.5: use <code>information gain rate</code><h3 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h3><h4 id="Boosting-AdaBoost-GBDT"><a href="#Boosting-AdaBoost-GBDT" class="headerlink" title="Boosting: AdaBoost GBDT"></a>Boosting: <code>AdaBoost</code> <code>GBDT</code></h4></li></ol><p>Seiral strategy, new learning machine is based on previous one</p><h4 id="GBDT-Gradient-Boosting-Decision-Tree"><a href="#GBDT-Gradient-Boosting-Decision-Tree" class="headerlink" title="GBDT(Gradient Boosting Decision Tree)"></a>GBDT(Gradient Boosting Decision Tree)</h4><h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h4><h4 id="Bagging-Random-forest-and-Dropout-in-Neural-Network"><a href="#Bagging-Random-forest-and-Dropout-in-Neural-Network" class="headerlink" title="Bagging: Random forest and Dropout in Neural Network"></a>Bagging: <code>Random forest</code> and <code>Dropout in Neural Network</code></h4><p>Parallel strategy, no dependency between learning machines.</p><h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h2><h3 id="Overfitting-and-underfitting"><a href="#Overfitting-and-underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h3><h4 id="Deal-with-overfitting"><a href="#Deal-with-overfitting" class="headerlink" title="Deal with overfitting"></a>Deal with overfitting</h4><ol><li><p>Data enhancement</p><ol><li>image: translation, rotation, scaling</li><li>GAN: generate new data</li><li>NLP: generate new data via neural machine translation</li></ol></li><li><p>Decrease the complexity of model</p><ol><li>neural network: decrease layer numbers and neuron numbers</li><li>decision tree: decrease tree depth and pruning</li></ol></li><li><p>Constrain weight:</p><ol><li>L1 regularization</li><li>L2 regularization</li></ol></li><li><p>Ensemble learning:</p><ol><li>Neural network: Dropout</li><li>Decision tree: random forest, GBDT</li></ol></li><li><p>early stopping</p></li></ol><h4 id="Deal-with-underfitting"><a href="#Deal-with-underfitting" class="headerlink" title="Deal with underfitting"></a>Deal with underfitting</h4><ol><li>add new feature</li><li>add model complexity</li><li>decrease regularization</li></ol><h3 id="Back-propagation-TODO-https-github-com-imhuay-Algorithm-Interview-Notes-Chinese-blob-master-A-E6-B7-B1-E5-BA-A6-E5-AD-A6-E4-B9-A0-A-E6-B7-B1-E5-BA-A6-E5-AD-A6-E4-B9-A0-E5-9F-BA-E7-A1-80-md"><a href="#Back-propagation-TODO-https-github-com-imhuay-Algorithm-Interview-Notes-Chinese-blob-master-A-E6-B7-B1-E5-BA-A6-E5-AD-A6-E4-B9-A0-A-E6-B7-B1-E5-BA-A6-E5-AD-A6-E4-B9-A0-E5-9F-BA-E7-A1-80-md" class="headerlink" title="Back-propagation TODO:https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"></a>Back-propagation TODO:<a href="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md">https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md</a></h3><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2ubtex9hwj207101kjr8.jpg" alt=""></p><blockquote><p>上标 (l) 表示网络的层，(L) 表示输出层（最后一层）；下标 j 和 k 指示神经元的位置；w_jk 表示 l 层的第 j 个神经元与(l-1)层第 k 个神经元连线上的权重</p></blockquote><p>MSE as loss function:<br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2ubuojv5xj207z03ljr9.jpg" alt=""></p><p>another expression:<br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g2ubub5wrkj20e907y0sv.jpg" alt=""></p><h3 id="Activation-function-improve-ability-of-expression"><a href="#Activation-function-improve-ability-of-expression" class="headerlink" title="Activation function: improve ability of expression"></a>Activation function: improve ability of expression</h3><h4 id="sigmoid-z"><a href="#sigmoid-z" class="headerlink" title="sigmoid(z)"></a>sigmoid(z)</h4><p>$$\sigma(z)=\frac{1}{1+exp(-z)}, where the range is [0, 1]$$</p><p>the derivative of simoid is:<br>TODO: to f(x)<br>$$f’(x)=f(x)(1-f(x))$$</p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Goal: restrict data point to same distribution through normalization data before each layer.</p><h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><p>Stochastic Gradient Descent, update weights each mini-batch</p><h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>Add former gradients with decay into current gradient.</p><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Dynamically adjust learning rate when training. </p><p>Learning rate is in reverse ratio to the sum of parameters.</p><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Dynamically adjust learning rate when training. </p><p>utilize first order moment estisourmation and second order moment estimation to make sure the steadiness.</p><h4 id="How-to-deal-with-L1-not-differentiable"><a href="#How-to-deal-with-L1-not-differentiable" class="headerlink" title="How to deal with L1 not differentiable"></a>How to deal with L1 not differentiable</h4><p>Update parameters along the axis direction.</p><h3 id="How-to-initialize-the-neural-network"><a href="#How-to-initialize-the-neural-network" class="headerlink" title="How to initialize the neural network"></a>How to initialize the neural network</h3><p>Init network with <strong>Gaussian Distribution</strong> or <strong>Uniform Distribution</strong>.</p><p>Glorot Initializer:<br>$$W_{i,j}~U(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}})$$</p><h1 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h1><h2 id="Models-and-History"><a href="#Models-and-History" class="headerlink" title="Models and History"></a>Models and History</h2><ul><li>2015 VGGNet(16/19):    Very Deep Convolutional Networks for Large-Scale Image Recognition, ICLR 2015.</li><li>2015 GoogleNet:    </li><li>2016 Inception-v1/v2/v3:    Rethinking the Inception Architecture for Computer Vision, CVPR 2016.</li><li>2016 ResNet:    Deep Residual Learning for Image Recognition, CVPR 2016.</li><li>2017 Xception:    Xception: Deep Learning with Depthwise Separable Convolutions, CVPR 2017.</li><li>2017 InceptionResNet-v1/v2、Inception-v4</li><li>2017 MobileNet:    MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv 2017.</li><li>2017 DenseNet:    Densely Connected Convolutional Networks, CVPR 2017.</li><li>2017 NASNet:    Learning Transferable Architectures for Scalable Image Recognition, arXiv 2017.</li><li>2018 MobileNetV2:    MobileNetV2: Inverted Residuals and Linear Bottlenecks, CVPR 2018.</li></ul><h2 id="Basic-knowledge-1"><a href="#Basic-knowledge-1" class="headerlink" title="Basic knowledge"></a>Basic knowledge</h2><h1 id="Practice-experience"><a href="#Practice-experience" class="headerlink" title="Practice experience"></a>Practice experience</h1><h2 id="Loss-function-decline-to-0-0000"><a href="#Loss-function-decline-to-0-0000" class="headerlink" title="Loss function decline to 0.0000"></a>Loss function decline to 0.0000</h2><p>Because of <strong>overflow</strong> in Tensorflow or other framework. it is better to initialize parameters in a reasonable interval. The solution is <strong>Xavier initialization</strong> and <strong>Kaiming initialization</strong>.</p><h2 id="Do-not-normaolize-the-bias-in-neural-network"><a href="#Do-not-normaolize-the-bias-in-neural-network" class="headerlink" title="Do not normaolize the bias in neural network"></a>Do not normaolize the bias in neural network</h2><p>That will lead to underfitting because of sparse $b$</p><h2 id="Do-not-set-learning-rate-too-large"><a href="#Do-not-set-learning-rate-too-large" class="headerlink" title="Do not set learning rate too large"></a>Do not set learning rate too large</h2><p>When using Adam optimizer, try $10^{-3}$ to $10^{-4}$</p><h2 id="Do-not-add-activation-before-sotmax-layer"><a href="#Do-not-add-activation-before-sotmax-layer" class="headerlink" title="Do not add activation before sotmax layer"></a>Do not add activation before sotmax layer</h2><h2 id="Do-not-forget-to-shuffle-training-data"><a href="#Do-not-forget-to-shuffle-training-data" class="headerlink" title="Do not forget to shuffle training data"></a>Do not forget to shuffle training data</h2><p>For the sake of overfitting</p><h2 id="Do-not-use-same-label-in-a-batch"><a href="#Do-not-use-same-label-in-a-batch" class="headerlink" title="Do not use same label in a batch"></a>Do not use same label in a batch</h2><p>For the sake of overfitting</p><h2 id="Do-not-use-vanilla-SGD-optimizer"><a href="#Do-not-use-vanilla-SGD-optimizer" class="headerlink" title="Do not use vanilla SGD optimizer"></a>Do not use vanilla SGD optimizer</h2><p>Avoid getting into saddle point</p><h2 id="Please-checkout-gradient-in-each-layer"><a href="#Please-checkout-gradient-in-each-layer" class="headerlink" title="Please checkout gradient in each layer"></a>Please checkout gradient in each layer</h2><p>For the sake of potential gradient explosion, we need to use <strong>gradient clip</strong> to cut off gradient</p><h2 id="Please-checkout-your-labels-are-not-random"><a href="#Please-checkout-your-labels-are-not-random" class="headerlink" title="Please checkout your labels are not random"></a>Please checkout your labels are not random</h2><h2 id="Problem-of-classification-confidence"><a href="#Problem-of-classification-confidence" class="headerlink" title="Problem of classification confidence"></a>Problem of classification confidence</h2><p>Symptom: When losses increasing, but the accuracy still increasing</p><p>For the sake of <strong>confidence</strong>: [0.9,0.01,0.02,0.07] in epoch 5 VS [0.5,0.4,0.05,0.05] in epoch 20.</p><p>Overall, this phenomenon is kind of <strong>overfitting</strong>.</p><h2 id="Do-not-use-batch-normalization-layer-with-small-batch-size"><a href="#Do-not-use-batch-normalization-layer-with-small-batch-size" class="headerlink" title="Do not use batch normalization layer with small batch size"></a>Do not use batch normalization layer with small batch size</h2><p>The data in batch size can not represent the statistical feature over whole dataset。</p><h2 id="Set-BN-layer-in-the-front-of-Activation-or-behind-Activation"><a href="#Set-BN-layer-in-the-front-of-Activation-or-behind-Activation" class="headerlink" title="Set BN layer in the front of Activation or behind Activation"></a>Set BN layer in the front of Activation or behind Activation</h2><h2 id="Improperly-Use-dropout-in-Conv-layer-may-lead-to-worse-performance"><a href="#Improperly-Use-dropout-in-Conv-layer-may-lead-to-worse-performance" class="headerlink" title="Improperly Use dropout in Conv layer may lead to worse performance"></a>Improperly Use dropout in Conv layer may lead to worse performance</h2><p>It is better to use dropout layer in a low probability such as 0.1 or 0.2.</p><p>Just like add some noise to Conv layer for normalization.</p><h2 id="Do-not-initiate-weight-to-0-but-bias-can"><a href="#Do-not-initiate-weight-to-0-but-bias-can" class="headerlink" title="Do not initiate weight to 0, but bias can"></a>Do not initiate weight to 0, but bias can</h2><h2 id="Do-not-forget-your-bias-in-each-FNN-layer"><a href="#Do-not-forget-your-bias-in-each-FNN-layer" class="headerlink" title="Do not forget your bias in each FNN layer"></a>Do not forget your bias in each FNN layer</h2><h2 id="Evaluation-accuracy-better-than-training-accuracy"><a href="#Evaluation-accuracy-better-than-training-accuracy" class="headerlink" title="Evaluation accuracy better than training accuracy"></a>Evaluation accuracy better than training accuracy</h2><p>Because the distributions between training set and test set have large difference.</p><p>Try methods in transfer learning.</p><h2 id="KL-divergence-goes-negative-number"><a href="#KL-divergence-goes-negative-number" class="headerlink" title="KL divergence goes negative number"></a>KL divergence goes negative number</h2><p>Need to pay attention to softmax for computing probability.</p><h2 id="Nan-values-appear-in-numeral-calculation"><a href="#Nan-values-appear-in-numeral-calculation" class="headerlink" title="Nan values appear in numeral calculation"></a>Nan values appear in numeral calculation</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.csdn.net/LoseInVain/article/details/83021356">深度学习debug沉思录</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The first and second generation of Neural Turing Machine</title>
      <link href="/2019/05/05/Hybrid-computing-using-a-neural-network-with-dynamic-external-memory/"/>
      <url>/2019/05/05/Hybrid-computing-using-a-neural-network-with-dynamic-external-memory/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Hand-writing-pdf-version"><a href="#Hand-writing-pdf-version" class="headerlink" title="Hand-writing pdf version:"></a>Hand-writing pdf version:</h2><div class="row"><iframe src="https://drive.google.com/file/d/1H63IlKB8ekJWUO8rcKfGHYtBhZfRRnR4/preview" style="width:100%; height:550px"></iframe></div><h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>Computer has a CPU and a RAM.</p><p>Differential Neural Computer has a neural network as <strong>the controller</strong> that take the role of <strong>the CPU</strong>.<br>The memory is an $N <em> W$ <strong>matrix</strong> that take the role of <em>*the RAM</em></em>, where $N$ means the locations and $W$ means the length of each pieces of memory.</p><h2 id="Memory-augmentation-and-attention-mechanism"><a href="#Memory-augmentation-and-attention-mechanism" class="headerlink" title="Memory augmentation and attention mechanism"></a>Memory augmentation and attention mechanism</h2><blockquote><p>The episodic memories or evenet memories are known to depend on the hippocampus in the human brain.</p></blockquote><p>The main point is that the memory of the network is external to the network itself.</p><p>The attention mechanism defines some distributions over the $N$ locations.<br>Each $i-th$ component of a weighting vector will communicate how much attention the controller should give to the content in the $i-th$ location of the memory.</p><h2 id="Differntiability"><a href="#Differntiability" class="headerlink" title="Differntiability"></a>Differntiability</h2><p>Every unit and operation in this structure is differentiable.</p><h2 id="Weightings"><a href="#Weightings" class="headerlink" title="Weightings"></a>Weightings</h2><p>The controller wants to do something which involves memory, and it doesn’t just look at every location of the memor.<br>Instead, it focues its attention on those locations which contain the information it is looking for.</p><p>The weighting produced for an input is a distribution over the N locations for their relative importance in a particular process(reading or writing).</p><p>Note that the weightings are produced by means by a vector emitted by the controller, which is called <strong>interface vector</strong>. The </p><h2 id="Three-interactions-between-controller-and-memory"><a href="#Three-interactions-between-controller-and-memory" class="headerlink" title="Three interactions between controller and memory"></a>Three interactions between controller and memory</h2><p>The controller and memory are mediated by the <strong>interface vector</strong>.</p><h3 id="Content-lookup"><a href="#Content-lookup" class="headerlink" title="Content lookup"></a>Content lookup</h3><p>A particular set of values within the interface vector, which we will collect in something called key vector, is compared to the content of each location. This comparison is made by means of a similarity measure.</p><h3 id="Temporal-memory-linkage"><a href="#Temporal-memory-linkage" class="headerlink" title="Temporal memory linkage"></a>Temporal memory linkage</h3><p>The transitions between consecutively written locations are recorded in an $N * N$ matrix, called temproal link matrix “L”. The sequence by which the controller writes in the memory is an information by itself, and it is something we want to store.</p><p>DNC stores the ‘temporal link’ to keep track of the order things where written in, and records the current ‘usage’ level of each memory location.</p><h3 id="Dynamic-memory-allocation"><a href="#Dynamic-memory-allocation" class="headerlink" title="Dynamic memory allocation"></a>Dynamic memory allocation</h3><p>Each location has a usage level represented as a number from 0 to 1. A weighting that picks out an unused location is sent to the write head, so that it knows where to store new information. The word “dynamic” refers to the ability of the controller to reallocate memory that is no longer required, erasing its content.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://towardsdatascience.com/rps-intro-to-differentiable-neural-computers-e6640b5aa73a">Differentiable Neural Computers: An Overview</a></li><li><a href="https://deepmind.com/blog/differentiable-neural-computers/">Deepmind-&gt;Differentiable neural computers</a></li><li><a href="https://slideplayer.com/slide/14373603/">DNC-slide</a></li><li><a href="https://www.slideshare.net/databricks/demystifying-differentiable-neural-computers-and-their-brain-inspired-origin-with-luis-leal">Demystifying Differentiable Neural Computers and Their Brain Inspired Origin with Luis Leal</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> agi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inspiration of On-intelligence</title>
      <link href="/2019/04/24/on-intelligence/"/>
      <url>/2019/04/24/on-intelligence/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>Please note that all these ideas may prove to be wrong or will be revised.</p><h2 id="Artificial-Intelligence-wrong-way"><a href="#Artificial-Intelligence-wrong-way" class="headerlink" title="Artificial Intelligence: wrong way"></a>Artificial Intelligence: wrong way</h2><p>We are on the wrong way of Artificial General Intelligence(AGI).</p><p>The biggest mistake is the belief that intelligence is defined by intelligent behavior.<br>Object detection or other tasks are the manifestations of intelligence not the intelligence itself.</p><p>The great brain uses vast amounts of memory to create a model of the world, everything you know and have learned is stored in this model.</p><p>The ability to make predictions about the future that is the crux of intelligence.</p><h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks:"></a>Neural Networks:</h2><ol><li>We must include time as brain function: real brains process rapidly changing streams of information.</li><li>The importance of feedback: In thalamus(丘脑), connections going backward toward the input exceed the connections going forward by almost a factor of ten. But <strong>back propagation is not really feedback</strong>, because it is only occurred during the learning phase.</li><li>Brain is organized as a repeating hierarchy.</li></ol><p>History shows that the best solution to scitific problems are simple and elegant.</p><h2 id="The-Human-Brain-all-your-knowledge-of-the-world-is-a-model-based-on-patterns"><a href="#The-Human-Brain-all-your-knowledge-of-the-world-is-a-model-based-on-patterns" class="headerlink" title="The Human Brain: all your knowledge of the world is a model based on patterns"></a>The Human Brain: all your knowledge of the world is a model based on patterns</h2><ol><li>The neocortex is about 2 milimeters thick and has six layers, each approximated by one card.</li><li>The mind is the creation of the cells in the brain. <strong>There is nothing else.</strong>And remember the cortex is built using a common repeated element.</li><li>The cortex uses the same computational tool to accomplish everything it does.</li></ol><p>According to <a href="https://en.wikipedia.org/wiki/Vernon_Benjamin_Mountcastle#Research_and_career">Mountcastle</a>‘s proposal:<br>The algorithm of cortex must be expressed independently of any particular function or sense.<br>The cortex does something universal that can be applied to any type of sensory or motor system.</p><blockquote><p>When scientists and engineers try to understand vision or make computer that can “see”, they devise terminologies and techniques specific to vision.<br>They talk about edges, textures, and three-dimensional representations.<br>If they want to understand spoken language, they build algorithms based on rules of grammar, syntax and semantics.</p></blockquote><p><strong>But these approaches are not how the brain solves these problems, and are therefore likely to fail.</strong></p><p>Attention mechanism:<br>About three times every second, your eyes make a sudden movement called a saccade.<br>Many vision research ignore saccades and the rapidly changing patterns of vision.</p><p>Existence may be objective, but the spatial-temporal pattern flowing into the axon bundles in our brains are all we have to go on.</p><h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><p>The brain does not “compute” the answers to problems, it <strong>retrieves the answers from memory</strong>.<br>The entire cortex is a memory system rather than a computer at all.</p><p>The memory is <code>invariant representations</code>, which handle variations in the world automatically.</p><ul><li><p>The neocortex stores <strong>sequences of patterns</strong><br>There are thousands of detailed memories stored in the synapses of our brains that are rarely used.<br>At any point in time we recall only a tiny fraction of what we know.(remind A-Z is easy, Z-A is hard)</p></li><li><p>The neocortex recalls patterns <strong>auto-associatively</strong>.<br>Your eyes only see parts of a body, but your brain fills in the rest.<br>At any time, a piece can activate the whole. This is the essence of auto-associative memories or inferring.<br><strong>Thought and memories are associately linked, notice that random thoughts never really occur!</strong></p></li><li><p>The neocortex stores patterns in an <strong>invariant form</strong>.<br>We do not remember or recall things with complete fidelity.<br>Because the brain remembers the important relationships in the world, independent of the details.<br>To make a specific prediction, the brain must combine knowledge of the invariant structure with the most recent details.</p><blockquote><p>When listening to a familar song played on a piano, your cortex predicts the next note before it is played. And when listening to people speak, you often know what they are going to say before they have finished speaking.</p></blockquote></li><li><p>The neocortex stores patterns in a <strong>hierarchy</strong>.</p></li></ul><h2 id="A-New-Framework-of-Intelligence-Hierarchy"><a href="#A-New-Framework-of-Intelligence-Hierarchy" class="headerlink" title="A New Framework of Intelligence: Hierarchy"></a>A New Framework of Intelligence: Hierarchy</h2><p>The brain is using memories to form predictions about what it expects to experience before experience it.<br>When prediction is violated, attention is drawn to the error.<br>Incorret predictions result in confusion and prompt you to pay attention.<br><strong>Your brain has made a model of the world and is constantly checking that model against reality.</strong></p><p>By comparing the actual sensory input with recalled memory, the animal not only understands where it is but can see into the future.</p><h2 id="How-the-Cortex-Works"><a href="#How-the-Cortex-Works" class="headerlink" title="How the Cortex Works"></a>How the Cortex Works</h2><p>If you don’t have a picture of puzzle’s solution, <strong>the bottom-up method</strong> is sometimes the only way to proceed.</p><p>Here is an interesting metaphor: </p><blockquote><p>Many of puzzle pieces will not be used in the ultimate solution, but you don’t know which one or how many.</p></blockquote><p>I can not approve the ideas from Hawkins in this part. Still we don’t know how the cortex works actually.</p><h2 id="How-the-Cortex-Learns"><a href="#How-the-Cortex-Learns" class="headerlink" title="How the Cortex Learns"></a>How the Cortex Learns</h2><blockquote><p>Donlad O.Hebb, Hebbian learing: When two neurons fire at the same tiem, the synapses between them get strengthened</p></blockquote><ol><li>Forming the classifications of patterns.</li><li>Building memory sequences.</li></ol><p>Note that prior to neocortex, the brain has:</p><ol><li>The Basal ganglia(基底神经节): Primitive motor system.</li><li>The cerebellum(小脑): Leared precise timing relationships of evenets.</li><li>The hippocampus(海马体): stored memories of specific events and places.</li></ol><p><strong>The hippocampus is the top region of the neocortex, not a separate structure.</strong></p><p>There are many more secrets to be discovered than we currently know</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/On_Intelligence">On intelligence, Jeff Hawkins</a></li><li><a href="https://en.wikipedia.org/wiki/Vernon_Benjamin_Mountcastle#Research_and_career">Mountcastle</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> agi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>零样本学习的视频描述</title>
      <link href="/2019/04/07/zero-shot-for-VD/"/>
      <url>/2019/04/07/zero-shot-for-VD/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Hand-writing-pdf-version"><a href="#Hand-writing-pdf-version" class="headerlink" title="Hand-writing pdf version"></a>Hand-writing pdf version</h2><div class="row"><iframe src="https://drive.google.com/file/d/1XtGej5wnl5hiJebI38wYpQrI6TIjndSs/preview" style="width:100%; height:550px"></iframe></div><h2 id="Hand-writing-image-version"><a href="#Hand-writing-image-version" class="headerlink" title="Hand-writing image version"></a>Hand-writing image version</h2><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g1u7a6lduaj20zf19u1d4.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g1u7aj7ktxj20zf19uk31.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g1u7as81fnj20zf19utfw.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> VideoCaptioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zeroshot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于分层强化学习的视频描述</title>
      <link href="/2019/03/10/HRL-video-captioning/"/>
      <url>/2019/03/10/HRL-video-captioning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Video Captioning via Hierarchical Reinforcement Learning</p></li><li><p>论文链接：<a href="https://ieeexplore.ieee.org/document/8578541/">https://ieeexplore.ieee.org/document/8578541/</a></p></li><li><p>论文源码：</p><ul><li>None</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><hr><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>视频描述中细粒度的动作描述仍然是该领域中一个巨大的挑战。该论文创新点分为两部分：1. 通过层级化的强化学习框架，使用高层manager识别粗粒度的视频信息并控制描述生成的目标，使用低层的worker识别细粒度的动作并完成目标。2. 提出Charades数据集。</p><hr><h1 id="Video-Captioning-via-Hierarchical-Reinforcement-Learning"><a href="#Video-Captioning-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Video Captioning via Hierarchical Reinforcement Learning"></a>Video Captioning via Hierarchical Reinforcement Learning</h1><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g0xt4i7vsnj20qs0k47lv.jpg" alt=""></p><h2 id="Framework-of-model"><a href="#Framework-of-model" class="headerlink" title="Framework of model"></a>Framework of model</h2><ol><li><p>Work processing</p><ul><li><p><strong>Pretrained CNN</strong> encoding stage we obtain:<br>video frame features: $v={v_i}$, where $i$ is index of frames.</p></li><li><p>Language Model encoding stage we obtain:<br>Worker : $h^{E_w}={h_i^{E_w}}$ from low-level <strong>Bi-LSTM</strong> encoder<br>Manager: $h^{E_m}={h_i^{E_m}}$ from high <strong>LSTM</strong> encoder</p></li><li><p>HRL agent decoding stage we obtain:<br>Language description:$a<em>{1}a</em>{2}…a_{T}$, where $T$ is the length of generated caption.</p></li></ul></li><li><p>Details in HRL agent:</p><ol><li>High-level manager:<ul><li>Operate at lower temporal resolution.</li><li>Emits a goal for worker to accomplish.</li></ul></li><li>Low-level worker<ul><li>Generate a word for each time step by following the goal.</li></ul></li><li>Internal critic <ul><li>Determin if the worker has accomplished the goal</li></ul></li></ol></li><li><p>Details in Policy Network:</p><ol><li>Attention Module:<ol><li>At each time step t: $c<em>t^W=\sum\alpha</em>{t,i}^{W}h^{E_w}_i$</li><li>Note that attention score $\alpha<em>{t,i}^{W}=\frac{exp(e</em>{t, i})}{\sum_{k=1}^{n}exp(e<em>t, k)}$, where $e</em>{t,i}=w^{T} tanh(W<em>{a} h</em>{i}^{E<em>w} + U</em>{a} h^{W}_{t-1})$</li></ol></li><li>Manager and Worker:<ol><li>Manage: take $[c_t^M, h_t^M]$ as input to produce goal. Goal is obtained through a MLP.</li><li>Worker: receive the goal $g_t$ and take the concatenation of $c_t^W, g<em>t, a</em>{t-1}$ as input, and outputs the probabilities of $\pi_t$ over all action $a_t$.</li></ol></li><li>Internal Critic:<ol><li>evaluate worker’s progress. Using an RNN struture takes a word sequence as input to discriminate whether end.</li><li>Internal Critic RNN take $h^I_{t-1}, a_t$ as input, and generate probability $p(z_t)$.</li></ol></li></ol></li><li><p>Details in Learning:</p><ol><li>Definition of Reward:<br>$R(a<em>t)$ = $\sum</em>{k=0} \gamma^{k} f(a_{t+k})$ , where　 $f(x)=CIDEr(sent+x)-CIDEr(sent)$ and $sent$ is previous generated caption.</li><li>Pseudo Code of HRL training algorithm:<pre><code class="py"><span class="keyword">import</span> training_pairs<span class="keyword">import</span> pretrained_CNN, internal_critic<span class="keyword">for</span> i <span class="keyword">in</span> range(M):Initial_random(minibatch)<span class="keyword">if</span> Train_Worker:  goal_exploration(enable=<span class="keyword">False</span>)  sampled_capt = LSTM() <span class="comment"># a_1, a_2, ..., a_T</span>  Reward = [r_i <span class="keyword">for</span> r_i <span class="keyword">in</span> calculate_R(sampled_caption)]  Manager(enable=<span class="keyword">False</span>)  worker_policy = Policy_gradient(Reward)<span class="keyword">elif</span> Train_Manager:  Initial_ramdom_process(N)  greedy_decoded_cap = LSTM()  Reward = [r_i <span class="keyword">for</span> r_i <span class="keyword">in</span> calculate_R(sampled_caption)]  Worker(enable=<span class="keyword">False</span>)  manager_policy = Policy_gradient(Reward)</code></pre></li></ol></li></ol><h2 id="All-in-one"><a href="#All-in-one" class="headerlink" title="All in one"></a>All in one</h2><p><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g0xt54v9puj21ao0p27c8.jpg" alt=""></p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ol><li><a href="http://ms-multimedia-challenge.com/2017/challenge">MSR-VTT</a><blockquote><p>该数据集包含50个小时的视频和26万个相关视频描述。</p></blockquote></li></ol><ol><li><a href="https://mila.quebec/en/publications/public-datasets/m-vad/">Charades</a><blockquote><p>Charades Captions:室内互动的9848个视频，包含157个动作的66500个注解，46个类别的物体的41104个标签，和共27847个文本描述。</p></blockquote></li></ol><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ol><li><p>实验可视化<br><img src="https://ws1.sinaimg.cn/large/ca26ff18gy1g0xs2qfw1rj220k0hce5u.jpg" alt=""></p></li><li><p>模型对比<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18gy1g0xs1f57tkj21120hwgpl.jpg" alt=""></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> VideoCaptioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reinforcement_learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HTM_theory</title>
      <link href="/2019/03/03/HTM-theory/"/>
      <url>/2019/03/03/HTM-theory/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ol><li>Overview<br>cellular structure are same.(bio pic)</li></ol><p>hierarchical structure.(need a pic)and each region is performing the same set of processes on the input data.</p><p>[SDR] sparse distributed representations(0 or 1)</p><p>Input: 1. Motor commands 2. sensory input</p><p>Encoder: takes a datatype and converts it into a sparse distributed representations. </p><p>Temporal means that systems learn continuously, every time it receives input it is attemptiing to predict what is going to happen next.</p><ol><li>Sparse Distributed Representation(SDR)<br>Terms: 1. n=Bit array length 2. w=Bits of array 3. sparsiity 4. Dense Bit Array Capacity=2**of bits<br>Bit array</li></ol><p>Capacity=n!/w!(n-w)!<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18gy1g0pke65e1dj218g11awlp.jpg" alt=""></p><p>OVERLAP/UNION<br>Similarity can be represented by overlap/union? of SDR.</p><p>MATCH</p><ol><li>Overlap Sets and Subsampling</li><li><p>Scalar Encoder(retina/cochlea)</p><ul><li>Scalar Encoder: consecutive one</li><li>Random Distributed Scalar Encoder: random one</li></ul></li><li><p>Data-time encoder </p></li><li>Input Space&amp; Connections<ul><li>Spactial Pooler: maintain a fixed sparsity &amp; maintain a overlap properties.</li></ul></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于时序结构的视频描述</title>
      <link href="/2019/03/02/describing-videos-by-exploiting-tempporal-structure/"/>
      <url>/2019/03/02/describing-videos-by-exploiting-tempporal-structure/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Describing Videos by Exploiting Temporal Structure</p></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1502.08029">https://arxiv.org/pdf/1502.08029</a></p></li><li><p>论文源码：</p><ul><li><a href="https://github.com/tsenghungchen/SA-tensorflow">https://github.com/tsenghungchen/SA-tensorflow</a></li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><hr><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>本文是蒙特利尔大学发表在ICCV2015的研究成果，其主要创新点在于提出了时序结构并且利用注意力机制达到了在2015年的SOTA。通过3D-CNN捕捉视频局部信息和注意力机制捕捉全局信息相结合，可以全面提升模型效果。<br>其另一个重要成果是MVAD电影片段描述数据集，此<a href="https://mila.quebec/en/publications/public-datasets/m-vad/">数据集</a>已经成为了当前视频描述领域主流的数据集。</p><hr><h2 id="Describing-Videos-by-Exploiting-Temporal-Structure"><a href="#Describing-Videos-by-Exploiting-Temporal-Structure" class="headerlink" title="Describing Videos by Exploiting Temporal Structure"></a>Describing Videos by Exploiting Temporal Structure</h2><h3 id="视频描述任务介绍："><a href="#视频描述任务介绍：" class="headerlink" title="视频描述任务介绍："></a>视频描述任务介绍：</h3><p>根据视频生成单句的描述，一例胜千言：</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcxfmvyxuj20si0hqqfg.jpg" alt=""></p><p>　　A monkey pulls a dog’s tail and is chased by the dog.</p><p>2015年较早的模型：<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcwjf53bsj214x0ksajx.jpg" alt="LSTM-YT模型"></p><h3 id="2015年之前的模型存在的问题"><a href="#2015年之前的模型存在的问题" class="headerlink" title="2015年之前的模型存在的问题"></a>2015年之前的模型存在的问题</h3><ol><li>输出的描述没有考虑到动态的<strong>时序结构</strong>。</li><li>之前的模型利用一个特征向量来表示视频中的所有帧，导致无法识别视频中物体出现的<strong>先后顺序</strong>。</li></ol><h3 id="论文思路以及创新点"><a href="#论文思路以及创新点" class="headerlink" title="论文思路以及创新点"></a>论文思路以及创新点</h3><ol><li>通过局部和全局的时序结构来产生视频描述：</li></ol><p><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0odfi9c82j20zk0eih1g.jpg" alt=""></p><p>  针对Decoder生成的每一个单词，模型都会关注视频中特定的某一帧。</p><ol><li>使用3-D CNN来捕捉视频中的动态时序特征。</li></ol><h3 id="模型结构设计"><a href="#模型结构设计" class="headerlink" title="模型结构设计"></a>模型结构设计</h3><p><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0oe02i6tjj21c80k47e5.jpg" alt=""></p><ul><li>Encoder(3-D CNN + 2-D GoogLeNet)的设置：3 * 3 * 3 的三维卷积核，并且是3-D CNN在行为识别数据集上预训练好的。</li></ul><p>每个卷积层后衔接ReLu激活函数和Local max-pooling， dropout参数设置为0.5。</p><p><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0oe2iz7iaj20v20ien2y.jpg" alt=""></p><ul><li>Decoder(LSTM)的设置：使用了additive attention作为注意力机制，下图为在两个数据集上的超参数设置：<br><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0osevs2qoj21620pwafz.jpg" alt=""></li></ul><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ol><li><a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">Microsoft Research Video Description dataset</a></li></ol><blockquote><p>1970条Youtobe视频片段：每条大约10到30秒，并且只包含了一个活动，其中没有对话。1200条用作训练，100条用作验证，670条用作测试。</p></blockquote><ol><li><a href="https://mila.quebec/en/publications/public-datasets/m-vad/">Montreal Video Annotation Dataset</a></li></ol><blockquote><p>数据集包含从92部电影的49000个视频片段，并且每个视频片段都被标注了描述语句。</p></blockquote><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li>BLEU</li></ul><p><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0os1pgs0mj20qe0kgqh7.jpg" alt=""></p><ul><li>METEOR</li></ul><p><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0os2ibx04j20su0mgh2g.jpg" alt=""></p><ul><li>CIDER</li><li>Perplexity</li></ul><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ol><li>实验可视化<br><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0ornzkuylj220a144u0x.jpg" alt="实验结果"></li></ol><p>柱状图表示每一帧生成对应颜色每个单词时的注意力权重。</p><ol><li>模型对比<br><img src="https://ws1.sinaimg.cn/large/ca26ff18ly1g0ormgxp41j22120ggten.jpg" alt="模型对比"></li></ol><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ul><li><a href="https://arxiv.org/pdf/1502.08029">Describing Videos by Exploiting Temporal Structure</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> VideoCaptioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> videoCaptioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频描述领域的第一篇深度模型论文</title>
      <link href="/2019/01/19/first-deep-model-in-video-captioning/"/>
      <url>/2019/01/19/first-deep-model-in-video-captioning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Translating Videos to Natural Language Using Deep Recurrent Neural Networks</p></li><li><p>论文链接：<a href="https://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf">https://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf</a></p></li><li><p>论文源码：</p><ul><li><a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube </a></li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><hr><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>假设我们在未来已经实现了通用人工智能，当我们回首向过去看，到底哪个时代会被投票选为最重要的“Aha Moment”呢？</p><p>作为没有预知未来能力的普通人。为了回答这个问题，首先需要明确的一点就是：我们现在究竟处在实现通用人工智能之前的哪个位置？</p><p>一个常用的比喻便是，如果把从开始尝试到最终实现通用人工智能比作一条一公里的公路的话。大部分人可能会认为我们已经走了200米到500米之间。但是真实的情况可能是，我们仅仅走过了5厘米不到。</p><p>因为在通往正确道路的各种尝试中，有很大一部分会犯方向性错误。当我们在错误的道路上越走越远的时候，那么肯定无法到达终点。推倒现有成果重新来过便是不可避免的。我们需要时时刻刻保持谨小慎微，以躲避“岔路口”。</p><p>现在有理由相信（其实是因为不得不掩耳盗铃），我们正走在一条正确的道路上。如果非要说现在的技术有哪些让我感觉不那么符合我的直觉的地方的话，我肯定会抢着回答：We are not living in the books or images.</p><p>公元前五亿年前，当我们还是扁形虫的时候，那时候我们便会在未知的环境中为了生存下去作出连续的决策。</p><p>公元前两亿年前，我们进化成啮齿类动物，并且拥有了一套完整的操作系统。不变的是，不断连续变化的生存环境。</p><p>公元前四百万年前，原始人类进化出了大脑皮层之后，终于拥有了进行推理和思考的能力。但是这一切是在他们发明文字和语言之前。</p><p>现如今，当人类巨灵正在尝试创造出超越本身智能的超智能体时，却神奇的忽略了超智能体也应该生存在不断变化的、充满危险的世界之中。</p><p>回到最开始的问题，我一定会把票投在利用神经模型来处理视频流的模型上。</p><hr><h2 id="Translating-Videos-to-Natural-Language-Using-Deep-Recurrent-Neural-Networks"><a href="#Translating-Videos-to-Natural-Language-Using-Deep-Recurrent-Neural-Networks" class="headerlink" title="Translating Videos to Natural Language Using Deep Recurrent Neural Networks"></a>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</h2><h3 id="视频描述任务介绍："><a href="#视频描述任务介绍：" class="headerlink" title="视频描述任务介绍："></a>视频描述任务介绍：</h3><p>根据视频生成单句的描述，一例胜千言：</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcxfmvyxuj20si0hqqfg.jpg" alt=""></p><p>　　A monkey pulls a dog’s tail and is chased by the dog.</p><h3 id="视频描述的前世："><a href="#视频描述的前世：" class="headerlink" title="视频描述的前世："></a>视频描述的前世：</h3><p>管道方法（PipeLine Approach）</p><ol><li>从视频中识别出<code>主语</code>、<code>动作</code>、<code>宾语</code>、<code>场景</code></li><li>计算被识别出实体的置信度</li><li>根据最高置信度的实体与预先设置好的模板，进行句子生成</li></ol><p>　在神经模型风靡之前，传统方法集中使用<strong>隐马尔科夫模型识别实体</strong>和<strong>条件随机场生成句子</strong></p><h3 id="神经模型的第一次尝试："><a href="#神经模型的第一次尝试：" class="headerlink" title="神经模型的第一次尝试："></a>神经模型的第一次尝试：</h3><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcwjf53bsj214x0ksajx.jpg" alt="LSTM-YT模型"></p><ol><li>从视频中，每十帧取出一帧进行分析</li></ol><p>　<img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzczmke1dbj20vg0astdq.jpg" alt=""><br>　人类眼睛的帧数是每秒24帧，从仿生学的观点出发，模型也不需要处理视频中所有的帧。再对视频帧进行缩放以便计算机进行处理。</p><ol><li>使用CNN提取特征并进行平均池化（Mean Pooling）</li></ol><p>　<img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzczv0iwy6j20x40mktg0.jpg" alt=""></p><ul><li><p>预训练的Alexnet[2012]:在120万张图片上进行预训练[ImageNet LSVRC-2012]，提取最后一层（第七层全连接层）的特征（4096维）。注意：提取的向量不是最后进行分类的1000维特征向量。</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzd0iak8vhj216o0eqacn.jpg" alt="Alexnet"></p></li><li><p>对所有的视频帧进行池化</p></li></ul><ol><li>句子生成</li></ol><p>　<img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzd06rquyyj20qk0nwgo6.jpg" alt="RNN生成句子"></p><h3 id="迁移学习和微调模型"><a href="#迁移学习和微调模型" class="headerlink" title="迁移学习和微调模型"></a>迁移学习和微调模型</h3><ol><li>在图片描述任务进行预训练</li></ol><p>　<img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzd0skao7sj216c0jm0zn.jpg" alt="transfer-learning from image captioning"></p><ol><li>微调（Fine-tuning）<br>　需要注意的是，在视频描述过程中：<ul><li>将输入从图片转换为视频；</li><li>添加了平均池化特征这个技巧；</li><li>模型进行训练的时候使用了更低的学习率</li></ul></li></ol><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ol><li><a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">Microsoft Research Video Description dataset</a></li></ol><blockquote><p>1970条Youtobe视频片段：每条大约10到30秒，并且只包含了一个活动，其中没有对话。1200条用作训练，100条用作验证，670条用作测试。</p></blockquote><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fzd1cmxyalj217g0lcdyf.jpg" alt="dataset"></p><ol><li><a href="https://blog.csdn.net/daniaokuye/article/details/78699138">MSCOCO数据集下载</a></li><li><a href="https://blog.csdn.net/gaoyueace/article/details/80564642">Flickr30k数据集下载</a></li></ol><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li>SVO(Subject, Verb, Object accuracy)</li><li>BLEU</li><li>METEOR</li><li>Human evaluation</li></ul><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ol><li>SVO正确率：</li></ol><p>　<img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fzd5gnjzg6j20p20f8n0d.jpg" alt="result on SVO"></p><ol><li>BLEU值和METEOR值</li></ol><p>　<img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fzd5p0xo78j20nm0aotak.jpg" alt="result on BLEU and METEOR"></p><h3 id="站在2019年回看2015年的论文"><a href="#站在2019年回看2015年的论文" class="headerlink" title="站在2019年回看2015年的论文"></a>站在2019年回看2015年的论文</h3><p>以19年的后见之明来考察这篇论文，虽然论文没有Attention和强化学习加持的，但是也开辟了用神经模型完成视频描述任务的先河。</p><p>回顾一下以前提出的问题，如何才能实现：</p><ol><li>常识推理。</li><li>空间位置。</li><li>根据不同粒度回复问题。</li></ol><p>答案很有可能在我们身上，大脑皮质中的前额皮质掌管着人格（就是你脑中出现的那个声音，就是他）。大脑皮质虽然仅仅是大脑最外层的两毫米厚的薄薄一层（<a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E8%84%91%E7%9A%AE%E8%B4%A8">没错，我确定就是两毫米</a>）,但是它起到的作用却是史无前例的。</p><p>以大脑皮质作为启发，最少我们也需要让人工大脑皮质也“生存”在一个类似于现实世界中的环境当中。因此视频是一个很好的起点，但也仅仅是个起点。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ul><li><a href="https://waitbutwhy.com/2017/04/neuralink.html">Neuralink and the Brain’s Magical Future</a></li><li><a href="https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf">Translating Videos to Natural Language Using Deep Recurrent Neural Networks – Slides</a></li><li><a href="https://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637">A Walk-through of AlexNet</a></li><li><a href="https://www.cs.utexas.edu/users/ml/clamp/videoDescription/#data">Collecting Multilingual Parallel Video Descriptions Using Mechanical Turk</a></li><li><a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E8%84%91%E7%9A%AE%E8%B4%A8">大脑皮质</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AGI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> videoCaptioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tips for Examination of Network Software Design</title>
      <link href="/2018/12/26/Network-software-design-exam-tips/"/>
      <url>/2018/12/26/Network-software-design-exam-tips/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Question-distribution"><a href="#Question-distribution" class="headerlink" title="Question distribution"></a>Question distribution</h2><ul><li>10 choice questions(20%)</li><li>10 true or false questions(20%)</li><li>6 essay questions(60%)</li></ul><h2 id="Examation-contains-three-parts-Network-amp-Design-amp-Programming"><a href="#Examation-contains-three-parts-Network-amp-Design-amp-Programming" class="headerlink" title="Examation contains three parts: Network &amp; Design &amp; Programming"></a>Examation contains three parts: Network &amp; Design &amp; Programming</h2><h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><ul><li>IP address:<ul><li><code>public address</code>: an IP address that can be <strong>accessed over the Internet</strong>. And your public IP address is the <strong>globally unique</strong> and <strong>can be found</strong>, and can only be assigned to a unique device.</li><li><code>private IP address</code>: The devices with private IP address will <strong>use your router’s public IP address to communicate</strong>. Note that to allow direct access to a local device which is assign a private IP address, a Network Address Translator(NAT) should be used.</li><li><code>how to compute total ip address in a subnet</code>: <ol><li>transform ip address into binary address.</li><li>count zero from tail to first one.</li><li>subtract 2(reserve address and broadcast address)</li></ol></li></ul></li></ul><hr><ul><li>Port(some default ports for common protocol):<ul><li><strong>http</strong>: 80</li><li><strong>https</strong>: 443</li><li><strong>ntp</strong>: 123</li><li><strong>ssh/tcp</strong>: 22</li><li><strong>mongoDB</strong>: 27017</li><li>DNS: 53</li><li>FTP: 21</li><li>Telnet: 23</li></ul></li></ul><hr><ul><li><p>DNS(duplicated):The Domain Name System(DNS) is the <strong>phonebook</strong> of the Internet. </p><ul><li>DNS <strong>translate domain names to IP address</strong> so browsers can load Internet resources.</li><li>DNS is hierarchical with a few authoritative serves at the top level.<ol><li>Your router or ISP provides information about DNS server to contact when doing a look up.</li><li>Low level DNS servers cache mappings, which could become stale due to DNS propagation delays. </li><li>DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live(TTL)</li></ol></li></ul></li></ul><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fynty6a1xwj20e50vnjt4.jpg" alt=""></p><hr><ul><li>CDN(duplicated):<ul><li>Definition: <strong>CDN(Content dilivery network/Content distributed network) is a geographically distributed network</strong> of proxy servers and their data centers. The goal is to distribute service spatially relative to end-users to <strong>provide high availability and high performance</strong>.</li><li>Improve performance in two ways:<ul><li>Users receive content at <strong>data centers close to them</strong>.</li><li>Your servers do not have to serve requests that the CND fulfills.</li></ul></li></ul></li></ul><hr><ul><li>Main routing protocol:<ul><li><code>OSPF</code>(Open Shortest Path First): OSPF is an <strong>interior gateway protocal</strong>.</li><li><code>IS-IS</code>(Intermediate System-to-Intermediate System): It is just like OSPF. IS-IS associates routers into areas of intra-area and inter-area.</li><li><code>BGP</code>(Border Gateway Protocol): It is used as the edge of your network. BGP <strong>constructs a routing table of networks</strong> reachable among Autonomous Systems(AS) number defined by the user.</li></ul></li></ul><hr><h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><h2 id="software-requirements-analysis-not-key-point"><a href="#software-requirements-analysis-not-key-point" class="headerlink" title="- software requirements analysis: not key point"></a>- software requirements analysis: not key point</h2><ul><li>main principles and key technologies for High concurrency programming.(duplicated)<ol><li><code>Security</code>, or correctness, is when a program executes concurrently with the expected results<ol><li>The <strong>visibility</strong></li><li>The <strong>order</strong></li><li>The <strong>atomic</strong></li></ol></li><li><code>Activeness</code>: Program must confront to <strong>deadlocks and livelocks</strong></li><li><code>Performance</code>: <strong>Less context switching, less kernel calls, less consistent traffic</strong>, and so on.</li></ol></li></ul><hr><ul><li>generic phases of software engineering<ol><li><code>Requirements analysis</code></li><li><code>Software Design</code></li><li><code>Implementation</code></li><li><code>Verification/Testing</code></li><li><code>Deployment</code></li><li><code>Maintenance</code></li></ol></li></ul><hr><ul><li>Agile Development(duplicated)<ol><li><code>Agile Values</code>:<ol><li>Individuals and <strong>interactions</strong> over processes and tools</li><li>Working software over <strong>comprehensive documentation</strong></li><li><strong>Customer collaboration</strong> over contract negotiation</li><li><strong>Responding to change</strong> over following a plan</li></ol></li><li><code>Agile Methods</code>:<ol><li>Frequently <strong>deliver small incremental</strong> units of functionality</li><li><strong>Define, build, test and evaluate cycles</strong></li><li>Maximize speed of <strong>feedback loop</strong></li></ol></li></ol></li></ul><hr><h3 id="Programming"><a href="#Programming" class="headerlink" title="Programming"></a>Programming</h3><ul><li>MVC(model-view-controller) model:<ul><li>MVC is an <strong>architectural pattern</strong> commonly used for developing <strong>user interfaces</strong> and allowing for effcient <strong>code reuse</strong> and <strong>paraller development</strong>.<ul><li>Model[probe]: an object carrying data. It can also have logic to <strong>update controller</strong> if its data changes.</li><li>View[frontend]: it can be any <strong>output representation of information</strong>, such as chart or a diagram.</li><li>Controller[backend]: accpet input and converts it to commands for the model or view</li></ul></li></ul></li></ul><hr><ul><li>NoSQL database(duplicated):<ol><li>NoSQL is <code>Not only SQL</code>, it has the advantages below: <ul><li><strong>Not using</strong> the <strong>relational model</strong> nor the SQL language. It is a collection of <strong>data items represented in a key-value store, document-store, wide column store, or a graph database</strong>.</li><li>Designed to run on <strong>large clusters</strong></li><li><strong>No schema</strong></li><li>Open Source</li></ul></li><li>NoSQL properties in detail:<ul><li><strong>Flexible scalability</strong></li><li><strong>Dynamic schema</strong> of data</li><li><strong>Efficient reading</strong></li><li><strong>Cost saving</strong></li></ul></li><li>NoSQL Technologies:<ul><li><strong>MapReduce</strong> programming model</li><li><strong>Key-value</strong> stores</li><li><strong>Document databases</strong></li><li><strong>Column-family stores</strong></li><li><strong>Graph databases</strong> </li></ul></li></ol></li></ul><hr><ul><li>Websocket(duplicated):<ul><li>WebSocket is a <strong>computer communications protocal</strong>, providing <strong>Bidirectional full-duplex communication channels</strong> over a single TCP connection and it is defined in <strong>RFC6445</strong>.</li><li>WebSocket is a different protocol from HTTP. Both protocols are located at <strong>layer 7 in the OSI model</strong> and depend on <strong>TCP at layer 4</strong>.</li><li>The WebSocket protocol enables interaction between a web client and a web server with lower overheads, facilitating real-time data transfer from and to the server.</li><li>working progress<ul><li>There are four main functions in Tornado<ul><li><code>open()</code>: Invoked when a new websocket is opened.</li><li><code>on_message(message)</code>: Handle incoming messages on the WebSocket</li><li><code>on_close()</code>: Invoke when the WebSocket is closed.</li><li><code>write_message(message)</code>: Sends the given message to the client of this Web Socket.</li></ul></li></ul></li></ul></li></ul><hr><ul><li>Differences between <code>git</code> and <code>svn</code>:<ol><li>Git is a <strong>distrubuted</strong> version control system; SVN is a non-distributed version control system.</li><li>Git has a <strong>centralized</strong> server and repository; SVN has <strong>non-centralized</strong> server and repository.</li><li>The content in Git is stored as <strong>metadata</strong>; SVN stores <strong>files of content</strong>.</li><li>Git branches are <strong>easier</strong> to work with than SVN branches.</li><li>Git does not have the <strong>global revision number</strong> feature like SVN has.</li><li>Git has <strong>better content protection</strong> than SVN,</li><li>Git was developed for <strong>Linux kernel</strong> by Linus Torvalds; SVN was deveploped by <strong>CollabNet</strong>.</li></ol></li></ul><h2 id="Essay-Questions"><a href="#Essay-Questions" class="headerlink" title="Essay Questions"></a>Essay Questions</h2><h3 id="Main-role-of-IP-address-port-DNS-CDN-for-network-software-design"><a href="#Main-role-of-IP-address-port-DNS-CDN-for-network-software-design" class="headerlink" title="Main role of IP address, port, DNS, CDN for network software design"></a>Main role of IP address, port, DNS, CDN for network software design</h3><ol><li>An internet Protocal address(IP address) is <strong>a numerical label</strong> assigned to each device connected to a computer network that <strong>uses the Internet Protocal for communication</strong>.</li><li>In computer networking, <strong>a port is an endpoint of communication</strong> and <strong>a logical construct that identifies a specific process</strong> or a type of network device.</li><li><strong>DNS(Domain Name System)</strong> is a <strong>hierarchical decentralized naming system</strong> for computers connected to the Internet or a private network,</li><li><strong>CDN(Content dilivery network/Content distributed network) is a geographically distributed network</strong> of proxy servers and their data centers. The goal is to distribute service spatially relative to end-users to <strong>provide high availability and high performance</strong>.</li></ol><h3 id="Difference-between-git-and-svn"><a href="#Difference-between-git-and-svn" class="headerlink" title="Difference between git and svn:"></a>Difference between <strong>git</strong> and <strong>svn</strong>:</h3><ol><li>Git is a <strong>distrubuted</strong> version control system; SVN is a non-distributed version control system.</li><li>Git has a <strong>centralized</strong> server and repository; SVN has <strong>non-centralized</strong> server and repository.</li><li>The content in Git is stored as <strong>metadata</strong>; SVN stores <strong>files of content</strong>.</li><li>Git branches are <strong>easier</strong> to work with than SVN branches.</li><li>Git does not have the <strong>global revision number</strong> feature like SVN has.</li><li>Git has <strong>better content protection</strong> than SVN,</li><li>Git was developed for <strong>Linux kernel</strong> by Linus Torvalds; SVN was deveploped by <strong>CollabNet</strong>.</li></ol><h3 id="main-principles-and-key-technologies-for-High-concurrency-programming"><a href="#main-principles-and-key-technologies-for-High-concurrency-programming" class="headerlink" title="main principles and key technologies for High concurrency programming"></a>main principles and key technologies for High concurrency programming</h3><ol><li><code>Security</code>, or correctness, is when a program executes concurrently with the expected results<ol><li>The <strong>visibility</strong></li><li>The <strong>order</strong></li><li>The <strong>atomic</strong></li></ol></li><li><code>Activeness</code>: Program must confront to <strong>deadlocks and livelocks</strong></li><li><code>Performance</code>: <strong>Less context switching, less kernel calls, less consistent traffic</strong>, and so on.</li></ol><h3 id="NoSQL-and-SQL-database"><a href="#NoSQL-and-SQL-database" class="headerlink" title="NoSQL and SQL database"></a>NoSQL and SQL database</h3><ol><li>RDBMS(Relational database management system)<ol><li>A relational database like SQL is a collection of <strong>data items organized by tables</strong>. It has features below:<ol><li><code>ACID</code> is a set of <strong>properties of relational database transactions</strong>.</li><li><code>Atomicity</code>: Each transaction is all or nothing</li><li><code>Consistency</code>: Any transaction will bring the database from one valid state to server.</li><li><code>Isolation</code>:  Executing transaction has been committed, it will remain so.</li></ol></li></ol></li><li><p>NoSQL</p><ol><li>NoSQL is <code>Not only SQL</code>, it has the advantages below: <ul><li><strong>Not using</strong> the <strong>relational model</strong> nor the SQL language. It is a collection of <strong>data items represented in a key-value store, document-store, wide column store, or a graph database</strong>.</li><li>Designed to run on <strong>large clusters</strong></li><li><strong>No schema</strong></li><li>Open Source</li></ul></li><li>NoSQL properties in detail:<ul><li><strong>Flexible scalability</strong></li><li><strong>Dynamic schema</strong> of data</li><li><strong>Efficient reading</strong></li><li><strong>Cost saving</strong></li></ul></li><li>NoSQL Technologies:<ul><li><strong>MapReduce</strong> programming model</li><li><strong>Key-value</strong> stores</li><li><strong>Document databases</strong></li><li><strong>Column-family stores</strong></li><li><strong>Graph databases</strong> </li></ul></li></ol></li><li><p>SQL VS NoSQL</p><ol><li>Relational data model VS Document data model</li><li>Structured data VS semi-structured data</li><li>strict schema VS dynamic/flexible schema</li><li>relational data VS Non-relational data</li></ol></li></ol><h3 id="Websocket-working-progress"><a href="#Websocket-working-progress" class="headerlink" title="Websocket(working progress)"></a>Websocket(working progress)</h3><ul><li>WebSocket is a <strong>computer communications protocal</strong>, providing <strong>Bidirectional full-duplex communication channels</strong> over a single TCP connection and it is defined in <strong>RFC6445</strong>.</li><li>WebSocket is a different protocol from HTTP. Both protocols are located at <strong>layer 7 in the OSI model</strong> and depend on <strong>TCP at layer 4</strong>.</li><li>The WebSocket protocol enables interaction between a web client and a web server with lower overheads, facilitating real-time data transfer from and to the server.</li><li>working progress<ul><li>There are four main functions in Tornado<ul><li><code>open()</code>: Invoked when a new websocket is opened.</li><li><code>on_message(message)</code>: Handle incoming messages on the WebSocket</li><li><code>on_close()</code>: Invoke when the WebSocket is closed.</li><li><code>write_message(message)</code>: Sends the given message to the client of this Web Socket.</li></ul></li></ul></li></ul><h3 id="Agile-Development-and-scrum"><a href="#Agile-Development-and-scrum" class="headerlink" title="Agile Development and scrum"></a>Agile Development and scrum</h3><ol><li>Agile Values:<ol><li>Individuals and interactions over processes and tools</li><li>Working software over comprehensive documentation</li><li>Customer collaboration over contract negotiation</li><li>Responding to change over following a plan</li></ol></li><li><p>Agile Methods:</p><ol><li>Frequently deliver small incremental units of functionality</li><li>Define, build, test and evaluate cycles</li><li>Maximize speed of feedback loop</li></ol></li><li><p>Scrum is 3 roles:</p><ol><li>Development Team</li><li>Product Owner</li><li>Scrum Master</li></ol></li><li><p>Scrum is 4 events:</p><ol><li>Sprint Planning</li><li>Daily Stand-up Meeting</li><li>Sprint Review</li><li>Sprint Retrospective</li></ol></li><li><p>Scrum is 4 artifacts:</p><ol><li>Product Backlog</li><li>Sprint Backlog</li><li>User Stories</li><li>Scrum Board</li></ol></li></ol><h3 id="Explain-how-DNS-work"><a href="#Explain-how-DNS-work" class="headerlink" title="Explain how DNS work"></a>Explain how DNS work</h3><p>Definition: The Domain Name System(DNS) is the <strong>phonebook</strong> of the Internet. </p><ul><li>DNS <strong>translate domain names to IP address</strong> so browsers can load Internet resources.</li><li>DNS is hierarchical with a few authoritative serves at the top level.<ol><li>Your router or ISP provides information about DNS server to contact when doing a look up.</li><li>Low level DNS servers cache mappings, which could become stale due to DNS propagation delays. </li><li>DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live(TTL)</li></ol></li></ul><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fynty6a1xwj20e50vnjt4.jpg" alt=""></p><h3 id="Difference-between-docker-and-virtual-host"><a href="#Difference-between-docker-and-virtual-host" class="headerlink" title="Difference between docker and virtual host"></a>Difference between docker and virtual host</h3><ol><li>Virtual Machine definition: Virtualization is the technique of importing a Guest operating system <strong>on top of a Host operating system</strong>.</li><li>Docker definition: A container image is <strong>a lightweight, stand-alone, executable package of a piece of software</strong> that includes everything needed to run it.</li><li>Docker is the service to run <strong>multiple containers on a machine</strong> (node) which can be on a vitual machine or on a physical machine.</li><li>A virtual machine is an <strong>entire operating system</strong> (which normally is not lightweight).</li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fymtosr7vyj20vt0f0whj.jpg" alt="Difference between docker and virtual machine"></p><h3 id="Type-of-software-test"><a href="#Type-of-software-test" class="headerlink" title="Type of software test"></a>Type of software test</h3><ul><li><p>Black Box testing: Black box testing is a software testing method where testers <strong>are not required to know coding or internal structure</strong> of the software. Black box testing method relies on testing software with various inputs and validating results against expected output.</p></li><li><p>White Box testing: White box testing strategy deals with the <strong>internal logic and structure of the code</strong>. The tests written based on the white box testing strategy incorporate coverage of the code written, branches, paths, statements and internal logic of the code etc.</p></li><li><p>Equivalence Partitioning:Equivalence Partitioning is also known as Equivalence Class Partitioning is a software testing technique and not a type of testing by itself. Equivalence partitioning technique is <strong>used in black box and gray box testing types</strong>. Equivalence partitioning <strong>classifies test data into Equivalence classes as positive Equivalence classes and negative Equivalence classes</strong>, such classification ensures both positive and negative conditions are tested.</p></li></ul><h3 id="Explain-how-CDN-work"><a href="#Explain-how-CDN-work" class="headerlink" title="Explain how CDN work"></a>Explain how CDN work</h3><ul><li>Definition: <strong>CDN(Content dilivery network/Content distributed network) is a geographically distributed network</strong> of proxy servers and their data centers. The goal is to distribute service spatially relative to end-users to <strong>provide high availability and high performance</strong>.</li><li>Improve performance in two ways:<ul><li>Users receive content at data centers close to them.</li><li>Your servers do not have to serve requests that the CND fulfills</li></ul></li></ul><p>To minimize the distance between the visitors and your website’s server, a CDN stores a cached version of its content in multiple geographical locations (a.k.a., points of presence, or PoPs). Each PoP contains a number of caching servers responsible for content delivery to visitors within its proximity.</p><p>In essence, CDN puts your content in many places at once, providing superior coverage to your users. For example, when someone in London accesses your US-hosted website, it is done through a local UK PoP. This is much quicker than having the visitor’s requests, and your responses, travel the full width of the Atlantic and back.<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fymv8bwce7j20d00kqgne.jpg" alt="CDN"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ul><li><a href="https://www.iplocation.net/public-vs-private-ip-address">What is the difference between public and private IP address?</a></li><li><a href="http://www.differencebetween.net/technology/software-technology/difference-between-git-and-svn/">Difference Between Git and SVN</a></li><li><a href="https://www.tutorialspoint.com/design_pattern/mvc_pattern.htm">Design Patterns - MVC Pattern</a></li><li><a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller">Model-view-controller</a></li><li><a href="https://en.wikipedia.org/wiki/IP_address">IP address</a></li><li><a href="https://en.wikipedia.org/wiki/Port_(computer_networking">Port(computer networking)</a>)</li><li><a href="https://en.wikipedia.org/wiki/Domain_Name_System">Domain Name System</a></li><li><a href="https://en.wikipedia.org/wiki/Content_delivery_network">Content delivery network/Content distributed network</a></li><li><a href="https://en.wikipedia.org/wiki/WebSocket">Websocket wiki</a></li><li><a href="https://www.tornadoweb.org/en/stable/websocket.html">tornado.websocket — Bidirectional communication to the browser</a></li><li><a href="https://wenku.baidu.com/view/df90877bf121dd36a22d827d.html">网络常见协议及端口号</a></li><li><a href="https://www.cloudflare.com/learning/dns/what-is-dns/">How does DNS work</a></li><li><a href="http://freewimaxinfo.com/routing-protocol-types.html">Routing Protocols Types (RIP, IGRP, OSPF, EGP, EIGRP, BGP, IS-IS)</a></li><li><a href="https://www.virtually-limitless.com/vcix-nv-study-guide/configure-dynamic-routing-protocols-ospf-bgp-is-is/">Configure dynamic routing protocols: OSPF, BGP, IS-IS</a></li><li><a href="https://stackoverflow.com/questions/48396690/docker-vs-virtual-machine">Docker vs Virtual Machine</a></li><li><a href="https://www.geeksforgeeks.org/types-software-testing/">Types of Software Testing</a></li><li><a href="https://www.testingexcellence.com/white-box-testing/">white-box-testing</a></li><li><a href="https://www.testingexcellence.com/types-of-software-testing-complete-list/">types-of-software-testing-complete-list</a></li><li><a href="https://blog.csdn.net/ITer_ZC/article/details/40748587">聊聊高并发专栏</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> BUPT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> exam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息论-信道容量迭代算法python实现版</title>
      <link href="/2018/12/26/information-theory-channel-capacity-iteration-algorithm/"/>
      <url>/2018/12/26/information-theory-channel-capacity-iteration-algorithm/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Talk-is-Cheap-Let-me-show-you-the-code"><a href="#Talk-is-Cheap-Let-me-show-you-the-code" class="headerlink" title="Talk is Cheap, Let me show you the code"></a>Talk is Cheap, Let me show you the code</h2><p><a href="https://colab.research.google.com/drive/1oFkI8WYPzQhvC1FLV7Fw8NR70EqNtENc">传送门</a></p>]]></content>
      
      
      <categories>
          
          <category> BUPT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> exam </tag>
            
            <tag> infomationTheory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neuralink and Brains&#39;Magical Future</title>
      <link href="/2018/12/25/Neuralink-and-Brains-Magical-Future/"/>
      <url>/2018/12/25/Neuralink-and-Brains-Magical-Future/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h3 id="Part1-The-Human-Colossus"><a href="#Part1-The-Human-Colossus" class="headerlink" title="Part1: The Human Colossus"></a>Part1: The Human Colossus</h3><p>In this part, the brief history of humanbeing is displayed. The process of evolution:</p><ol><li>Sponge(600 Million BC): Data is just like saved in <code>Cache</code>~</li><li>Jellyfish(580 Million BC): The first animal has <code>nerves net</code> to save data from environment. Note that nerves net not only exist in its head but also in the whole body.</li><li>Flatworm(550 Million BC) and Frog(265 Million BC): The flatworm has nervous system in charge of everything.</li><li>Rodent(225 Million BC) and Tree mammal(80 Million BC): More complex animals.</li><li>Hominid(4 Million BC): The early version of neocortex. Hominid could think(complex thoughts, reason through decisions, long-term plans). When language had appeared, knowledges are saved in an intricate system(neural net). Homimid already has enough knowledge from their ancestors.</li><li>Computer Colossus(1990s): Computer network that can not learning to think.</li></ol><h3 id="Part2-The-Brain"><a href="#Part2-The-Brain" class="headerlink" title="Part2: The Brain"></a>Part2: The Brain</h3><p>Three membranes around brain: dura mater, arachnoid mater, pia mater.<br>Looking into brain, there are three parts: neomammalian, paleomammalian, reptilian.</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fz4u66o1a6j20ob0ll4qp.jpg" alt=""></p><ol><li>The Reptilian Brain(爬行脑): the brain stem<ol><li>The medulla[mi’dula] oblongata[abon’gata] (延髓): control involuntary things like heart rate, breathing, and blood pressure.</li><li>The pons(脑桥): generate actions about the little things like bladder control, facial expressions.</li><li>The mid brain(中脑): eyes moving.</li><li>The cerebellum(小脑): Stay balanced.</li></ol></li></ol><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fz4ukscpc0j20j50h9wst.jpg" alt=""></p><ol><li>The Paleo-Mammalian Brain(古哺乳脑): the limbic system(边缘脑)<ol><li>The amygdala(杏仁核): deal with anxiety, fear, happy feeling.</li><li>The hippocampus(海马体): a board for memory to direction.</li><li>The thalamus(丘脑): sensory middleman that receives information from your sensory organ and sends them to your cortex for processing.</li></ol></li></ol><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fz502o3cl3j20m80fntl1.jpg" alt=""></p><ol><li>The Neo-Mammalian Brain(新哺乳脑): The Cortex(皮质)<ol><li>The frontal lobe(前叶): Handle with reasoning, planning, executive function. And <strong>the adult in your head</strong> call <strong>prefrontal cortex</strong>(前额皮质).</li><li>The parietal lobe(顶叶): Controls sense of touch.</li><li>The temporal lobe(额叶): where your memory lives</li><li>The occipital lobe(枕叶): entirely dedicated to vision.</li></ol></li></ol><p>Inspiration from neural nets:<br><code>Neuroplasticity</code>: Neurons’ ability to alter themselves chemically, structurally, and even functionally, allow your brain’s neural network to optimize itself to the external world. Neuroplasticity makes sure that human can grow and change and learn new things throughout their whole lives.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><ul><li><a href="https://waitbutwhy.com/2017/04/neuralink.html">neuralink from wait but why</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> agi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>意识先验</title>
      <link href="/2018/12/09/ConsciousnessPrior-Bengio/"/>
      <url>/2018/12/09/ConsciousnessPrior-Bengio/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="意识先验理论"><a href="#意识先验理论" class="headerlink" title="意识先验理论"></a>意识先验理论</h1><h2 id="如何理解意识先验"><a href="#如何理解意识先验" class="headerlink" title="如何理解意识先验"></a>如何理解意识先验</h2><p>首先，意识先验这篇论文没有实验结果，是一篇纯粹的开脑洞的、理论性的文章。</p><p>论文中提到的意识先验更多的是对<strong>不同层次</strong>的信息的<strong>表征</strong>提取。例如：人类创造了高层次的概念，如符号（自然语言）来简化我们的思维。</p><p>2007 年，Bengio 与 Yann LeCun 合著的论文着重强调表征必须是多层的、逐渐抽象的。13年，Bengio 在综述论文中，增加了对解纠缠（Disentangling）的强调。</p><h3 id="RNN是个很好的例子"><a href="#RNN是个很好的例子" class="headerlink" title="RNN是个很好的例子"></a>RNN是个很好的例子</h3><p>RNN的隐藏状态包含一个低维度的子状态，可以用来解释过去，帮助预测未来，也可以作为自然语言来呈现。</p><p><img src="https://github.com/824zzy/blogResources/blob/master/picResources/ConsciousnessPrior.png?raw=true" alt="意识先验网络示意图"></p><h2 id="表征RNN（Representation-RNN-F）"><a href="#表征RNN（Representation-RNN-F）" class="headerlink" title="表征RNN（Representation RNN / F）"></a>表征RNN（Representation RNN / F）</h2><p>$$h_t = F(s_t,h_t−1)$$</p><p>Bengio提出表征RNN($F$)和表征状态$h_t$。其中$F$是包含了大脑中所有的神经连接权重。它们可以看作是我们的知识和经验，将一种表示状态映射到另一种表示状态。</p><p>表征RNN与一个人在不同环境学习到的知识、学识和经验相对应。即使有相同的$F$, 人们的反应和未来的想法也会不尽相同。表征状态$h_t$对应大脑所有神经元状态的聚合。并且他们可以被看作是当时环境（最底层信息）的表征。</p><h2 id="意识RNN-Consciousness-RNN-C"><a href="#意识RNN-Consciousness-RNN-C" class="headerlink" title="意识RNN (Consciousness RNN / C)"></a>意识RNN (Consciousness RNN / C)</h2><p>$$c_t=C(h<em>t,c</em>{t-1},z_t)$$</p><p>没有人能够有意识地体会到大脑里所有神经元是如何运作的。因为只有一小部分神经元与大脑此时正在思考的想法和概念相对应。因此意识是大脑神经元一个小的子集，或者说是副产品（by-product）。</p><p>因此Bengio认为，意识RNN本身应该包含某种注意力机制（当前在神经机器翻译中使用的)。他引入注意力作为额外的机制来描述大脑选择关注什么，以及如何预测或行动。</p><p>简而言之，意识RNN应该只“注意”意识向量更新自身时的重要细节，以<strong>减少计算量</strong>。</p><h2 id="验证网络（Verifier-Network-V）"><a href="#验证网络（Verifier-Network-V）" class="headerlink" title="验证网络（Verifier Network / V）"></a>验证网络（Verifier Network / V）</h2><p>$$V(h<em>t,c</em>{t-k})\in R$$</p><p>Bengio的思想还包含了一种训练方法，他称之为验证网络$V$。网络的目标是将当前的$h<em>t$表示与之前的意识状态$c</em>{t-k}$相匹配。在他的设想中可以用变分自动编码器(VAE)或GAN进行训练。</p><h2 id="语言与符号主义的联结"><a href="#语言与符号主义的联结" class="headerlink" title="语言与符号主义的联结"></a>语言与符号主义的联结</h2><p>深度学习的主要目标之一就是设计出能够习得更好表征的算法。好的表征理应是高度抽象的、高维且稀疏的，但同时，也能和自然语言以及符号主义 AI 中的『高层次要素』联系在一起。</p><p>语言和符号人工智能的联系在于：语言是一种“选择性的过程”，语言中的语句可以忽略世界上的大部分细节，而专注于少数。符号人工智能只需要了解世界的一个特定方面，而不是拥有一切的模型。</p><p>Bengio关于如何使这一点具体化的想法是：先有一个“意识”，它迫使一个模型拥有不同类型的“意识流”，这些“意识流”可以独立运作，捕捉世界的不同方面。例如，如果我在想象与某人交谈，我对那个人、他们的行为以及我与他们的互动有一种意识，但我不会在那一刻对我的视觉流中的所有像素进行建模。</p><h3 id="思考：快与慢"><a href="#思考：快与慢" class="headerlink" title="思考：快与慢"></a>思考：快与慢</h3><p>人类的认知任务可以分为系统 1 认知（System 1 cognition）和系统 2 认知（System 2 cognition）。系统 1 认知任务是那些你可以在不到 1 秒时间内无意识完成的任务。例如你可以很快认出手上拿着的物体是一个瓶子，但是无法向其他人解释如何完成这项任务。这也是当前深度学习擅长的事情，「感知」。<br>系统 2 认知任务与系统 1 任务的方式完全相反，它们很「慢」且有意识。例如计算「23*56」，大多数人需要有意识地遵循一定的规则、按照步骤完成计算。完成的方法可以用语言解释，而另一个人可以理解并重现。这是算法，是计算机科学的本意，符号主义 AI 的目标，也属于此类。<br>人类联合完成系统 1 与系统 2 任务，人工智能也理应这样。</p><h2 id="还有很多问题需要解决"><a href="#还有很多问题需要解决" class="headerlink" title="还有很多问题需要解决"></a>还有很多问题需要解决</h2><h3 id="训练的目标函数是什么？"><a href="#训练的目标函数是什么？" class="headerlink" title="训练的目标函数是什么？"></a>训练的目标函数是什么？</h3><p>标准的深度学习算法的目标函数通常基于最大似然，但是我们很难指望最大似然的信号能够一路经由反向传播穿过用于预测的网络，穿过意识RNN，最终到达表征 RNN。</p><p>最大似然与意识先验的思想天然存在冲突。「人类从不在像素空间进行想象与生成任务，人类只在高度抽象的语义空间使用想象力，生成一张像素级的图像并非人类需要完成的任务。」因此，在训练目标里引入基于表征空间的项目就变得顺理成章。<strong>不在原始数据空间内定义目标函数</strong></p><h3 id="梯度下降是否适用于意识先验？"><a href="#梯度下降是否适用于意识先验？" class="headerlink" title="梯度下降是否适用于意识先验？"></a>梯度下降是否适用于意识先验？</h3><blockquote><p>Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals, O., Graves, A., Silver, D., &amp; Kavukcuoglu, K. (2016). Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343.</p></blockquote><p>除了目标函数之外，意识先验的优化方式也会和经典深度学习有所不同。Bengio： 什么样的优化方式最适合意识先验？我仍然不知道这个问题的答案。<br>在他看来，一类很有前景的研究是合成梯度（synthetic gradient）。</p><p>有了合成梯度之后，每一层的梯度可以单独更新了。但是当时间步继续拉长，问题仍然存在。理论上反向传播可以处理相当长的序列，但是鉴于人类处理时间的方式并非反向传播，可以轻松跨越任意时长，等「理论上」遇到一千乃至一万步的情况，实际上就不奏效了。</p><h3 id="信用分配的仍然是最大的问题"><a href="#信用分配的仍然是最大的问题" class="headerlink" title="信用分配的仍然是最大的问题"></a>信用分配的仍然是最大的问题</h3><blockquote><p>Ke, N. R., Goyal, A., Bilaniuk, O., Binas, J., Mozer, M. C., Pal, C., &amp; Bengio, Y. (2018). Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding. arXiv preprint arXiv:1809.03702.</p></blockquote><p>换言之，我们对时间的信用分配（credit assignment）问题的理解仍然有待提高。「比如你在开车的时候听到『卟』的一声，但是你没在意。三个小时之后你停下车，看到有一个轮胎漏气了，立刻，你的脑海里就会把瘪轮胎和三小时前的『卟』声联系起来——不需要逐个时间步回忆，直接跳到过去的某个时间，当场进行信用分配。」。受人脑的信用分配方式启发，Bengio 的团队尝试了一种稀疏注意回溯（Sparse Attentive Backtracking）方法。「我们有一篇关于时间信用分配的工作，是 NIPS 2018 的论文，能够跳过成千上万个时间步，利用对记忆的访问直接回到过去——就像人脑在获得一个提醒时所作的那样——直接对一件事进行信用分配。」</p><h2 id="关于意识先验的代码"><a href="#关于意识先验的代码" class="headerlink" title="关于意识先验的代码"></a>关于意识先验的代码</h2><ul><li>论文：<a href="https://ai-on.org/pdf/bengio-consciousness-prior.pdf">Experiments on the Consciousness Prior</a></li><li>代码：<a href="https://github.com/AI-ON/TheConsciousnessPrior/tree/master/src">TheConsciousnessPrior github</a></li></ul><h2 id="参考与引用"><a href="#参考与引用" class="headerlink" title="参考与引用"></a>参考与引用</h2><ul><li><a href="http://thegrandjanitor.com/2018/05/09/a-read-on-the-consciousness-prior-by-prof-yoshua-bengio/">A READ ON “THE CONSCIOUSNESS PRIOR” BY PROF. YOSHUA BENGIO</a></li><li><a href="https://www.quora.com/What-is-Yoshua-Bengios-new-Consciousness-Prior-paper-about">What is Yoshua Bengio’s new “Consciousness Prior” paper about?</a></li><li><a href="https://www.reddit.com/r/MachineLearning/comments/72h5zf/r_the_consciousness_prior/">reddit</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-11-29-7">Yoshua Bengio访谈笔记：用意识先验糅合符号主义与联结主义</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> agi </tag>
            
            <tag> bengio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最大概率汉语切分作业</title>
      <link href="/2018/12/06/Chinese-segmentation-Homework/"/>
      <url>/2018/12/06/Chinese-segmentation-Homework/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Talk-is-Cheap-Let-me-show-you-the-code"><a href="#Talk-is-Cheap-Let-me-show-you-the-code" class="headerlink" title="Talk is Cheap, Let me show you the code"></a>Talk is Cheap, Let me show you the code</h2><p><a href="https://colab.research.google.com/drive/1ns3HetlP-8Np6GdF-mjaa0zIaB8k9lDG#scrollTo=XX1EqIqD3Y9q">传送门</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chineseSegmentation </tag>
            
            <tag> bupt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据挖掘文本分类作业</title>
      <link href="/2018/12/06/DataMining-Homework/"/>
      <url>/2018/12/06/DataMining-Homework/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Talk-is-Cheap-Let-me-show-you-the-code"><a href="#Talk-is-Cheap-Let-me-show-you-the-code" class="headerlink" title="Talk is Cheap, Let me show you the code"></a>Talk is Cheap, Let me show you the code</h2><p><a href="https://colab.research.google.com/drive/1AIzOZinBCn7iHo8Dx6AgLMRBzy2WglA-#scrollTo=ZPYreCTDWQB_&amp;uniqifier=4">传送门</a></p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bupt </tag>
            
            <tag> dataMining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>世界模型（World Model）实验以及原理</title>
      <link href="/2018/11/24/world-model-experiment-and-priciple/"/>
      <url>/2018/11/24/world-model-experiment-and-priciple/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Basic-Concepts-in-Reinforcement-Learning"><a href="#Basic-Concepts-in-Reinforcement-Learning" class="headerlink" title="Basic Concepts in Reinforcement Learning"></a>Basic Concepts in Reinforcement Learning</h2><div class="row"><iframe src="https://drive.google.com/file/d/1LceWiaaxbhfrV1mEvwP0ojCefpY5A_gt/preview" style="width:100%; height:550px"></iframe></div><h2 id="世界模型的实验"><a href="#世界模型的实验" class="headerlink" title="世界模型的实验"></a>世界模型的实验</h2><p>[World models on colab]{<a href="https://colab.research.google.com,/drive/1sF2iUdhMbm2mwdECvy01l9iWEFiIodlb#scrollTo=pCg_b9DOwDN6}">https://colab.research.google.com,/drive/1sF2iUdhMbm2mwdECvy01l9iWEFiIodlb#scrollTo=pCg_b9DOwDN6}</a></p><h3 id="补充：在colab上显示游戏环境"><a href="#补充：在colab上显示游戏环境" class="headerlink" title="补充：在colab上显示游戏环境"></a>补充：在colab上显示游戏环境</h3><p>通过<code>xvfb</code>,我们可以很方便的在colab上观察训练过程与模型训练结果。</p><p>demo: <a href="https://colab.research.google.com/drive/13XzgZo_CZuMYrgbiIJiurD-_tdAuUJOl#scrollTo=4fOPouQND0FA">Policy Gradient display on Colab</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><p><a href="https://arxiv.org/pdf/1511.09249.pdf">On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</a></p></li><li><p><a href="https://ai.intel.com/demystifying-deep-reinforcement-learning/">Guest Post (Part I): Demystifying Deep Reinforcement Learning</a> </p></li><li><a href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/">Simple reinforcement learning methods to learn CartPole</a></li><li><a href="https://arxiv.org/pdf/1802.08864.pdf">One Big Net For Everything</a></li><li><a href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/">Simple reinforcement learning methods to learn CartPole</a></li><li><a href="https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459">Hallucinogenic Deep Reinforcement Learning Using Python and Keras</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> WorldModel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通用人工智能（AGI）</title>
      <link href="/2018/11/09/artificial-general-intelligence/"/>
      <url>/2018/11/09/artificial-general-intelligence/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Artificial-General-Intelligence-Lex-Fridman"><a href="#Artificial-General-Intelligence-Lex-Fridman" class="headerlink" title="Artificial General Intelligence(Lex Fridman)"></a>Artificial General Intelligence(Lex Fridman)</h2><h3 id="Something-about-Lex-Fridman"><a href="#Something-about-Lex-Fridman" class="headerlink" title="Something about Lex Fridman"></a>Something about Lex Fridman</h3><h3 id="MIT-AGI-Misson-Engineer-Intelligence"><a href="#MIT-AGI-Misson-Engineer-Intelligence" class="headerlink" title="MIT AGI Misson: Engineer Intelligence"></a>MIT AGI Misson: Engineer Intelligence</h3><p>Goals:</p><ol><li>avoid the pitfalls of “black box”: Media often reports AI like fiction. Hype is the first enemy to us.</li><li>avoid the pitfalls of “I am just a scientist”.</li></ol><h3 id="How-far-away-from-creating-intelligent-systems"><a href="#How-far-away-from-creating-intelligent-systems" class="headerlink" title="How far away from creating intelligent systems"></a>How far away from creating intelligent systems</h3><p>Analogy: we are in the dark room looking for a switch with no knowledge of where the light switch is.</p><p>Exploration travel for the sake of discovery and adventure is human compulsion.</p><h2 id="Building-machines-that-see-and-think-like-people-Josh-Tenenbaum"><a href="#Building-machines-that-see-and-think-like-people-Josh-Tenenbaum" class="headerlink" title="Building machines that see, and think like people(Josh Tenenbaum)"></a>Building machines that see, and think like people(Josh Tenenbaum)</h2><h3 id="Something-about-Josh-Tenenbaum"><a href="#Something-about-Josh-Tenenbaum" class="headerlink" title="Something about Josh Tenenbaum"></a>Something about Josh Tenenbaum</h3><h3 id="AI-technologies-no-real-AI"><a href="#AI-technologies-no-real-AI" class="headerlink" title="AI technologies no real AI"></a>AI technologies no real AI</h3><p>Intelligence is not just about pattern recognition, it is about modeling the world.</p><h3 id="Sally-port-visual-intelligence-of-our-near-term-focus"><a href="#Sally-port-visual-intelligence-of-our-near-term-focus" class="headerlink" title="Sally port: visual intelligence of our near-term focus"></a>Sally port: visual intelligence of our near-term focus</h3><p>Some part of your brain is tracking the whole world around you. And you track your world model to plan your actions</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx3a8gz9dfj20hs0am0v8.jpg" alt=""></p><h3 id="The-roots-for-common-sense"><a href="#The-roots-for-common-sense" class="headerlink" title="The roots for common sense"></a>The roots for common sense</h3><p>reverse engineer our brain, figure out how brain can formulate a goal and be able to acheive it. </p><h3 id="How-do-we-build-this-architecture"><a href="#How-do-we-build-this-architecture" class="headerlink" title="How do we build this architecture?"></a>How do we build this architecture?</h3><ul><li>symbolic language for knowledge representation</li><li>probabilistic inference in generative models to capture uncertainty</li><li>neural network for pattern recognition</li></ul><p><strong>Inference means that our model runs a few low precision simulations for a few time steps</strong><br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx3zgiybhzj20e6043js1.jpg" alt=""></p><p>Mental simulation engines based on probabilistic programs<br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx3zfqc8u1j208x0d8wg5.jpg" alt=""></p><h2 id="OpenAI-Meta-Learning-and-Self-play-Ilya-Sutskever"><a href="#OpenAI-Meta-Learning-and-Self-play-Ilya-Sutskever" class="headerlink" title="OpenAI Meta-Learning and Self-play(Ilya Sutskever)"></a>OpenAI Meta-Learning and Self-play(Ilya Sutskever)</h2><h3 id="Why-do-neural-networks-work"><a href="#Why-do-neural-networks-work" class="headerlink" title="Why do neural networks work?"></a>Why do neural networks work?</h3><p>shortest program that fits training data is the best possible generalization.</p><p>Reinforcement Learning is a good framework for building intelligent agent.</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx443zm4tuj20hd086acq.jpg" alt=""></p><p>But note that RL framework is not quite complete because it assumes the reward is given by the environnment. But in reality, <strong>the agent rewards itself</strong>.</p><h3 id="Reinfocement-Learning-algorithms-in-a-nutshell"><a href="#Reinfocement-Learning-algorithms-in-a-nutshell" class="headerlink" title="Reinfocement Learning algorithms in a nutshell"></a>Reinfocement Learning algorithms in a nutshell</h3><p>Try something new add randomness directions and compare the result to your expectation. </p><p>If the result was better than expected, do more of the same in the future.</p><h4 id="Model-free-RL-Two-classes-of-algorithms"><a href="#Model-free-RL-Two-classes-of-algorithms" class="headerlink" title="Model-free RL: Two classes of algorithms"></a>Model-free RL: Two classes of algorithms</h4><ul><li>Policy Gradients:<ol><li>Just take the gradient</li><li>Stable, easy to use</li><li>Very few tricks needed</li><li><strong>On policy</strong></li></ol></li><li>Q-learning based:<ol><li>Less stable, more sample efficient</li><li>won’t explain how it works</li><li><strong>Off policy</strong>: can be trained on data generated by some other policy</li></ol></li></ul><h4 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta-learning"></a>Meta-learning</h4><p>Our dream is:</p><ul><li>Learn to learn</li><li>Train a system on many tasks</li><li>Resulting system can solve new tasks quickly</li></ul><h3 id="Exploration-in-RL-a-key-challenge"><a href="#Exploration-in-RL-a-key-challenge" class="headerlink" title="Exploration in RL: a key challenge"></a>Exploration in RL: a key challenge</h3><p>Random behavior must generate some reward and you must get rewards from time to time, otherwise learning will not occur. So if the reward is too sparse, agent cannot learn.</p><h3 id="It-would-be-nice-if-learning-was-hierarchical"><a href="#It-would-be-nice-if-learning-was-hierarchical" class="headerlink" title="It would be nice if learning was hierarchical"></a>It would be nice if learning was hierarchical</h3><blockquote><p>Current RL learns by trying out random actions at each timestep</p></blockquote><p>Agent may require a real “model” to really solve this problem.</p><h3 id="Self-Play-that-is-very-cool"><a href="#Self-Play-that-is-very-cool" class="headerlink" title="Self-Play: that is very cool"></a><strong>Self-Play</strong>: that is very cool</h3><p>Crux: The agents create the environment by virture of the agent acting in the environment</p><p>Here comes the question: can we train AGI via self-play among multi-agents?</p><p>It’s unknown.</p><h2 id="MSRA-presentation-given-by-Yoshua-Bengio"><a href="#MSRA-presentation-given-by-Yoshua-Bengio" class="headerlink" title="MSRA presentation given by Yoshua Bengio"></a>MSRA presentation given by Yoshua Bengio</h2><h3 id="Principle-in-Bengio’s-idea"><a href="#Principle-in-Bengio’s-idea" class="headerlink" title="Principle in Bengio’s idea."></a>Principle in Bengio’s idea.</h3><h4 id="World-Models"><a href="#World-Models" class="headerlink" title="World Models"></a>World Models</h4><p>we(human-being) have a mental model, that could capture facts of our world to some extending and humans generalize better than other animals thanks to a more accurate internal model of the <strong>underlying causal relationships</strong> </p><h4 id="Shortcoming-in-current-model"><a href="#Shortcoming-in-current-model" class="headerlink" title="Shortcoming in current model"></a>Shortcoming in current model</h4><p>So long as our machine learning models ‘cheat’ by relying only on superficial statistical<br>regularities, however they remain vulnerable to out-of-distribution examples.</p><h3 id="Possible-solutions"><a href="#Possible-solutions" class="headerlink" title="Possible solutions"></a>Possible solutions</h3><h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>To predict future situations(e.g., the effect of planned actions) far from  anything seen before while involving known concepts, an essential component of reasoning intelligence and science.</p><h4 id="Invariance"><a href="#Invariance" class="headerlink" title="Invariance"></a>Invariance</h4><p>Our systems need to be invariant about deep understanding : models for recognition and generation clearly don’t understand in the crucial abstractions. </p><h4 id="Imagination"><a href="#Imagination" class="headerlink" title="Imagination"></a>Imagination</h4><p>Real-life applications often require generalizations in regimes not seen during training, so humans can project themselves in situation they have never seen or never experience.</p><h4 id="Subjective-Knowledge"><a href="#Subjective-Knowledge" class="headerlink" title="Subjective Knowledge"></a>Subjective Knowledge</h4><p>Our brain can come up with control policies that can influence specific aspects of the world: an agent acquires by interacting in the world which is that it’s not universal knowledge, it’s subjective knowledge</p><h3 id="Present-Development-Stage"><a href="#Present-Development-Stage" class="headerlink" title="Present Development Stage"></a>Present Development Stage</h3><h4 id="More-elements-as-prior"><a href="#More-elements-as-prior" class="headerlink" title="More elements as prior"></a>More elements as prior</h4><ol><li>Spatial &amp; temporal scales</li><li>Marginal independence</li><li>Simple dependencies between factors<ol><li>Consciousness prior: arXiv(1709.08568)</li></ol></li><li>Causal / Mechanism independence<ol><li>Controllable factors</li></ol></li></ol><h4 id="Content-based-Attention-Attention-Mechanism"><a href="#Content-based-Attention-Attention-Mechanism" class="headerlink" title="Content-based Attention(Attention Mechanism)"></a>Content-based Attention(Attention Mechanism)</h4><p>to select a few relevant abstract concepts making a thought.</p><h4 id="TODO-future-work"><a href="#TODO-future-work" class="headerlink" title="TODO future work"></a>TODO future work</h4><p>The ability to do credit assignment through very long  time spans. There are also shortcoming for current RNN architecture: I can remember something I did laster year.</p><h2 id="Godel-Machines-Meta-Learning-and-LSTMS-Jurgen-Schmidhuber"><a href="#Godel-Machines-Meta-Learning-and-LSTMS-Jurgen-Schmidhuber" class="headerlink" title="Godel Machines, Meta-Learning, and LSTMS(Jurgen Schmidhuber)"></a>Godel Machines, Meta-Learning, and LSTMS(Jurgen Schmidhuber)</h2><ul><li>Simplicity is beauty</li><li>History of science is a history of compression progress. </li><li>Humans are curious and curiosity strategy is a discovery of evolution(A guy who explores the unknown world has a higher chance of solving problems that he needs to survive in this world)</li><li>Consciousness may be a byproduct of problem-solving.</li><li>What we do now since 2015(now is 2018) is CM(controller model) system which we give the controller the opportunity to learn by itself </li></ul><h3 id="AMA-Ask-me-anthing-on-reddit-Jurgen-Schimidhuber"><a href="#AMA-Ask-me-anthing-on-reddit-Jurgen-Schimidhuber" class="headerlink" title="AMA(Ask me anthing) on reddit: Jurgen Schimidhuber"></a>AMA(Ask me anthing) on reddit: Jurgen Schimidhuber</h3><h4 id="Question-What’s-something-that’s-true-but-almost-nobody-agrees-with-you-on"><a href="#Question-What’s-something-that’s-true-but-almost-nobody-agrees-with-you-on" class="headerlink" title="Question: What’s something that’s true, but almost nobody agrees with you on?"></a>Question: What’s something that’s true, but almost nobody agrees with you on?</h4><p>Intelligence is just the product of a few principles that will be considered very simple in<br>hindsignt. There are partial justification:</p><p>Theoretically optimal in some abstract sense although they just consist of a few formulas:</p><p>Humanbeing make predictions based on observations. Every AI scientist wants to find a<br>theoretically optimal way of predicting:</p><p>Normally we do not know the true conditional probability: $P(next|past)$.<br>But assume we do know that $p$ is in some set $P$ of distrikkbutions.</p><p>Given $q$ in $P$, we obtain Bayiesmix: $M(x)=\sum_q w_q q(x)$. We can predict using $M$ instead of the optimal but unknown $p$, </p><p>Let $LM(n)$ and $Lp(n)$ be the total expected losses of the M-predictor and the p-predictor.</p><p>Then LM(n)-Lp(n) is at most of the order of $\sqrt{[Lp(n)]}$. That is, M is not much worse than p. And in general, no other predictor can do better than that!</p><p>Once we have an optimal predictor, in principle we alse should have an optimal decision maker or reinforcement learner that always picking those action sequences with the highest predicted success, that is a universal AI.</p><h4 id="His-favorite-Theory-of-Consciousness-TOC"><a href="#His-favorite-Theory-of-Consciousness-TOC" class="headerlink" title="His favorite Theory of Consciousness(TOC)"></a>His favorite Theory of Consciousness(TOC)</h4><p>Karl Popper famously said: “All life is problem solving.” No theory of consciousness is necessary to define the objectives of a general problem solver. From an AGI point of view, consciousness is at best a by-product of a general problem solving procedure.</p><p>Where do the symbols and self-symbols underlying consciousness and sentience come from? I think they come from data compression during problem solving.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.youtube.com/watch?v=-GV_A9Js2nM&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&amp;index=2&amp;t=0s">MIT AGI: Artificial General Intelligence(Lex Fridman)</a></li><li><a href="https://www.youtube.com/watch?v=7ROelYvo8f0&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&amp;index=8&amp;t=0s">MIT AGI: Building machines that see, learn, and think like people (Josh Tenenbaum)</a></li><li><a href="https://www.youtube.com/watch?v=9EN_HoEk3KY&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&amp;index=4&amp;t=0s">MIT AGI: OpenAI Meta-Learning and Self-Play (Ilya Sutskever)</a></li><li><a href="https://www.youtube.com/results?search_query=bengio&amp;pbjreload=10">Bengio Interview</a></li><li><a href="https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/">I am Jürgen Schmidhuber, AMA!</a></li><li><a href="https://www.youtube.com/watch?v=3FIo6evmweo">MIT AI: Godel Machines, Meta-Learning, and LSTMs (Juergen Schmidhuber)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> agi </tag>
            
            <tag> mit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>元学习（meta-learning）</title>
      <link href="/2018/11/09/meta-learning-introduction/"/>
      <url>/2018/11/09/meta-learning-introduction/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Two-problems-we-confront"><a href="#Two-problems-we-confront" class="headerlink" title="Two problems we confront"></a>Two problems we confront</h2><ol><li>Sample efficiency: models typically need 6000 samples per digit to recognize digit handwriting.</li><li>Poor transferablity: models don’t learn from previous experience or learned knowledge. </li></ol><p>So meta-learning is the solution to the two questions above. And we try to define it as “learning how to learn”. Our dream is:</p><ul><li>Learn to learn</li><li>Train a system on many tasks</li><li>Resulting system can solve new tasks quickly</li></ul><h2 id="Some-basic-concepts"><a href="#Some-basic-concepts" class="headerlink" title="Some basic concepts"></a>Some basic concepts</h2><h3 id="Few-shot-Learning"><a href="#Few-shot-Learning" class="headerlink" title="Few-shot Learning"></a>Few-shot Learning</h3><p>In deep learning, we use regularization to make sure we are not overfitting out model with a small dataset, but we are <strong>overfitting our task</strong>. Therefore what we learned cannot be generalized to other tasks.</p><p>We often get stuck when test samples that are <strong>not common</strong> in dataset.</p><p>In <strong>one-shot-learning</strong>, we will only provide one training sample per category. There is an example:</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx2w9gmja6j20m80fzk0s.jpg" alt=""></p><p>In this one-shot learning, we often train a RNN to learn the training data and labels. When we represent with a test input, we should predict its label correctly.</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx2wk8ezr8j20m806rq3e.jpg" alt=""></p><p>In meta-testing, we provide many datasets again with classes that never trained before. Once we have learned from hundred tasks, we should discover the general pattern in classifying objects.</p><h2 id="Recurrent-Models"><a href="#Recurrent-Models" class="headerlink" title="Recurrent Models"></a>Recurrent Models</h2><h3 id="Memory-Augmented-Neural-Networks"><a href="#Memory-Augmented-Neural-Networks" class="headerlink" title="Memory-Augmented Neural Networks"></a>Memory-Augmented Neural Networks</h3><p>One of the meta-learning methods using an external memory network with RNN. Note that in supervised learning, we provide both input and label in the same time step $t$. However, in this model, the label is not provided untild the next time step $t+1$(shown below).</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx2xk82uvzj20m807aaa7.jpg" alt=""></p><p>When updating the model, instead of updating the model immediately, we wait until a batch of tasks is completed. We later merge all we learned from these tasks for a single update.</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx2y80btkjj20m8058wex.jpg" alt=""></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://medium.com/@jonathan_hui/meta-learning-how-we-address-the-shortcomings-of-our-deep-networks-a008aa4b5b2b">RL — Meta-Learning</a></li><li><a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a">From zero to research — An introduction to Meta-learning</a></li><li><a href="https://www.youtube.com/watch?v=9EN_HoEk3KY&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&amp;index=4&amp;t=0s">MIT AGI: OpenAI Meta-Learning and Self-Play (Ilya Sutskever)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> ReinforcementLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>混合密度网络</title>
      <link href="/2018/10/31/mixture-density-network-note/"/>
      <url>/2018/10/31/mixture-density-network-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="My-Implementation-with-Tensorflow-Eager-Execution"><a href="#My-Implementation-with-Tensorflow-Eager-Execution" class="headerlink" title="My Implementation with Tensorflow Eager Execution"></a>My Implementation with Tensorflow Eager Execution</h2><p><a href="https://colab.research.google.com/drive/1113X6Yx-XglANAzl933nb6vUynpy6VYk#scrollTo=km0IIHkaPH0T">IPython Notebook on Colab</a></p><h2 id="Key-equations-for-Mixture-Density-Networks"><a href="#Key-equations-for-Mixture-Density-Networks" class="headerlink" title="Key equations for Mixture Density Networks"></a>Key equations for Mixture Density Networks</h2><div class="row"><iframe src="https://drive.google.com/file/d/1T7ROj1FnzSdJUjQyd_4T4sRK4klQ1s2y/preview" style="width:100%; height:550px"></iframe></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">Mixture Density Networks with Tensorflow</a></li><li><a href="http://blog.otoro.net/2015/06/14/mixture-density-networks/">Mixture Density Networks</a></li><li><a href="https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb">Mixture Density Networks with Pytorch</a></li><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-ncrg-94-004.pdf">Mixture Density Networks</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> WorldModel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> evolutionStategy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Evolution strategies</title>
      <link href="/2018/10/30/CMA-ES-note/"/>
      <url>/2018/10/30/CMA-ES-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h3 id="Handwriting-Version-for-this-post"><a href="#Handwriting-Version-for-this-post" class="headerlink" title="Handwriting Version for this post"></a>Handwriting Version for this post</h3><div class="row"><iframe src="https://drive.google.com/file/d/1A9MUO2PQ3OjwCoZVr8XBh-yKa5lovO42/preview" style="width:100%; height:550px"></iframe></div><h3 id="Problems-in-BackPropagation-and-Gradient-Descent"><a href="#Problems-in-BackPropagation-and-Gradient-Descent" class="headerlink" title="Problems in BackPropagation and Gradient Descent"></a>Problems in BackPropagation and Gradient Descent</h3><ol><li><p>the gradient of reward signals given to the agent is realised many timesteps in the future. Questions above can be seem as <strong>Credit Assignment</strong></p></li><li><p>there is the issue of being stuck in a local optimum.</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fwq7har0x0j206o06g3zn.jpg" width="50%" height="30%"></p><h3 id="Pseudo-code-of-Basic-Evolution-Strategy"><a href="#Pseudo-code-of-Basic-Evolution-Strategy" class="headerlink" title="Pseudo code of Basic Evolution Strategy"></a>Pseudo code of Basic Evolution Strategy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">solver = EvolutionStrategy()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  <span class="comment"># ask the ES to give us a set of candidate solutions</span></span><br><span class="line">  solutions = solver.ask()</span><br><span class="line">  <span class="comment"># create an array to hold the fitness results.</span></span><br><span class="line">  fitness_list = np.zeros(solver.popsize)</span><br><span class="line">  <span class="comment"># evaluate the fitness for each given solution.</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(solver.popsize):</span><br><span class="line">    fitness_list[i] = evaluate(solutions[i])</span><br><span class="line">  <span class="comment"># give list of fitness results back to ES</span></span><br><span class="line">  solver.tell(fitness_list)</span><br><span class="line">  <span class="comment"># get best parameter, fitness from ES</span></span><br><span class="line">  best_solution, best_fitness = solver.result()</span><br><span class="line">  <span class="keyword">if</span> best_fitness &gt; MY_REQUIRED_FITNESS:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="Advantages-in-Evolution-Strategies"><a href="#Advantages-in-Evolution-Strategies" class="headerlink" title="Advantages in Evolution Strategies"></a>Advantages in Evolution Strategies</h3><ol><li>Easier to scale in a distributed setting(easy to parallelize).</li><li>It does not suffer in settings with sparse rewards.</li><li>It has fewer hyperparameters.</li><li>It is effective at finding solutions for RL tasks.</li></ol><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplees.gif" width="30%" height="30%"></p><h3 id="Improvement-of-Covariance-Matrix-Adaptive-Evolution-Strategy"><a href="#Improvement-of-Covariance-Matrix-Adaptive-Evolution-Strategy" class="headerlink" title="Improvement of Covariance Matrix Adaptive Evolution Strategy"></a>Improvement of Covariance Matrix Adaptive Evolution Strategy</h3><p>We want to explore more and increase the standard deviation of our search space.<br>And there are times when we are confident we are close to good optima and just want to fine-tune the solution.<br><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes.gif" width="30%" height="30%"></p><h3 id="Details-of-algorithm"><a href="#Details-of-algorithm" class="headerlink" title="Details of algorithm"></a>Details of algorithm</h3><ol><li><p>Calculate the fitness score of each candidate solution in generation $(g)$.<br><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step1.png" width="30%" height="30%"></p></li><li><p>Isolates the best 25% of the population in generation $(g)$, in purple.</p></li></ol><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step2.png" width="30%" height="30%"></p><ol><li>Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.</li></ol><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step3.png" width="30%" height="30%"></p><ol><li>Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}​$.</li></ol><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step4.png" width="30%" height="30%"></p><h3 id="OpenAI-Evolution-Strategy"><a href="#OpenAI-Evolution-Strategy" class="headerlink" title="OpenAI Evolution Strategy"></a>OpenAI Evolution Strategy</h3><p>In particular, $\sigma$ is fixed to a constant number, and only the $\mu$ parameter is updated at each generation. </p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/openes.gif" width="30%" height="30%"></p><p>Although its performance is not the best, it is possible to scale to over a thousand parallel workers.</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx399cbkkfj20p205iq3y.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fx39a8kr44j20os0a2di4.jpg" alt=""></p><h3 id="Comparision-among-Evolution-Strategies"><a href="#Comparision-among-Evolution-Strategies" class="headerlink" title="Comparision among Evolution Strategies"></a>Comparision among Evolution Strategies</h3><p><img src="http://blog.otoro.net/assets/20171031/mnist_results.svg" alt=""></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">A Visual Guide to Evolution Strategy</a></li><li><a href="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/">Evolving Stable Strategies</a></li><li><a href="https://blog.openai.com/evolution-strategies/">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li><li><a href="https://worldmodels.github.io/">world model</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> WorldModel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> evolutionStategy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英国人工智能发展战略</title>
      <link href="/2018/10/15/UK-AI-strategy/"/>
      <url>/2018/10/15/UK-AI-strategy/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="英国2018年国家战略最新进展"><a href="#英国2018年国家战略最新进展" class="headerlink" title="英国2018年国家战略最新进展"></a>英国2018年国家战略最新进展</h2><p>英国政府于2018年4月公布了人工智能行业协议（AI Sector Deal）。这是英国政府产业战略的一部分，旨在将英国定位为人工智能领域的全球领导者。该协议涉及广泛的领域：促进公共和私人研发，投资于STEM教育，改善数字基础设施，开发人工智能人才，并领导全球关于数据伦理的对话。其中包括超过£3亿英镑用于私营部门投资的国内外科技公司,阿兰·图灵研究所创建图灵的奖学金,和促进伦理创新数据中心。该中心是该项目的一个关键项目，因为政府希望领导AI伦理的全球治理。该中心于2018年6月开始进行公众咨询。</p><p>在该行业协议公布的十天前，英国上议院人工智能特别委员会(House of Lords’s Select Committee on AI)发表了一份题为(AI in the UK: ready, willing, and able?)的报告。这份报告是为期10个月的调查的结果，该调查的任务是调查人工智能技术进步的经济、伦理和社会影响。该报告列出了一些建议供政府考虑，包括呼吁审查技术公司对数据的潜在垄断，鼓励开发审计数据集的新方法，并为与人工智能合作的英国中小企业创建一个增长基金。报告还指出，英国有机会领导人工智能的全球治理，并建议在2019年举办一次全球峰会，为人工智能的使用和发展建立国际规范。2018年6月，英国政府发布了一份针对上议院的官方回应，对报告中的每一项建议进行评论。</p><h2 id="英国人工智能分类以及体系结构"><a href="#英国人工智能分类以及体系结构" class="headerlink" title="英国人工智能分类以及体系结构"></a>英国人工智能分类以及体系结构</h2><h3 id="分类一"><a href="#分类一" class="headerlink" title="分类一"></a>分类一</h3><ol><li>医疗健康</li><li>自动驾驶</li><li>金融服务</li></ol><h3 id="分类二"><a href="#分类二" class="headerlink" title="分类二"></a>分类二</h3><ul><li>网络安全</li><li>个性化和综合医疗</li><li>个性化教育和培训</li><li>智能城市综合交通</li><li>提高基础设施的效率</li><li>个性化的公共服务</li><li>在医药和航空航天等关键领域进行数字化制造。</li></ul><h2 id="参考与引用"><a href="#参考与引用" class="headerlink" title="参考与引用"></a>参考与引用</h2><ol><li><a href="https://medium.com/politics-ai/an-overview-of-national-ai-strategies-2a70ec6edfd">https://medium.com/politics-ai/an-overview-of-national-ai-strategies-2a70ec6edfd</a></li><li><a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/652097/Growing_the_artificial_intelligence_industry_in_the_UK.pdf">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/652097/Growing_the_artificial_intelligence_industry_in_the_UK.pdf</a></li><li><a href="https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf">https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf</a></li><li><a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-deal/ai-sector-deal">https://www.gov.uk/government/publications/artificial-intelligence-sector-deal/ai-sector-deal</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> strategy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>胶囊（Capsule）网络</title>
      <link href="/2018/10/15/capsule-note-and-demo/"/>
      <url>/2018/10/15/capsule-note-and-demo/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="手写笔记"><a href="#手写笔记" class="headerlink" title="手写笔记"></a>手写笔记</h2><p>第一部分根据国外博客进行make sense的直观理解。<br>第二部分根据苏神博客的进行数学方面的推导加强理解。</p><div class="row"><iframe src="https://drive.google.com/file/d/1mA1dSM1q-12pfpr75842DWO0zcvsIFbt/preview" style="width:100%; height:550px"></iframe></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://kexue.fm/archives/5155">https://kexue.fm/archives/5155</a></li><li><a href="https://kexue.fm/archives/5112">https://kexue.fm/archives/5112</a></li><li><a href="https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418">https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418</a></li><li><a href="https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66">https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Capsule </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变分自编码器及其应用</title>
      <link href="/2018/10/03/VAE-learning/"/>
      <url>/2018/10/03/VAE-learning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>笔记主要参考变分自编码器的原论文<a href="https://arxiv.org/pdf/1312.6114.pdf">《Auto-Encoding Variational Bayes》</a>，与<a href="https://kexue.fm/archives/5253">苏神的博客</a></p><h2 id="VAE模型"><a href="#VAE模型" class="headerlink" title="VAE模型"></a>VAE模型</h2><p>VAE的目标（与GAN相同）：希望构建一个从隐变量$Z$生成目标数据$X$的模型。<br>VAE的核心是：<strong>进行分布之间的变换。</strong></p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvzqckwnkhj20my0aegn1.jpg" alt=""></p><p>VAE的Encoder有两个，一个用来计算均值，一个用来计算方差。</p><h3 id="问题所在"><a href="#问题所在" class="headerlink" title="问题所在"></a>问题所在</h3><p>但生成模型的难题是判断生成分布与真实分布的相似度。（即我们只知道抽样结果，不知道分布表达式）<br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzgu3recj20or0c7400.jpg" alt=""></p><h3 id="大部分教程所述的VAE"><a href="#大部分教程所述的VAE" class="headerlink" title="大部分教程所述的VAE"></a>大部分教程所述的VAE</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvxa4kzerpj20qt0dy40q.jpg" alt=""><br>模型思路是：先从标准正态分布中采样一个Z，然后根据Z来算一个X。<br>若VAE结构确实是这个图的话，我们其实完全不清楚：究竟经过重新采样出来的$Z_k$，是不是还对应着原来的$X_k$。</p><p>其实，在整个VAE模型中，我们并没有去使用$p(Z)$（隐变量空间的分布）是正态分布的假设，我们用的是假设$p(Z|X)$（后验分布）是正态分布!</p><p>但是，训练好的神经网络<br>并且VAE会让所有的$P(Z|X)$都向标准正态分布看齐：<br>$$p(Z)=\sum_X p(Z|X)p(X)=\sum_X \mathcal{N}(0,I)p(X)=\mathcal{N}(0,I) \sum_X p(X) = \mathcal{N}(0,I)$$</p><h3 id="真实的VAE"><a href="#真实的VAE" class="headerlink" title="真实的VAE"></a>真实的VAE</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzt27ie8j20rf0imjv6.jpg" alt=""><br>VAE是为每个样本构造<strong>专属</strong>的正态分布，然后采样来重构。</p><p>但是神经网络经过训练之后的方差会接近0。采样只会得到确定的结果。</p><p>因此还需要使所有的正态分布都向<strong>标准正态分布</strong>（模型的假设）看齐。为了使所有的P(Z|X)都向$\mathcal{N}(0,I)$看齐，我们需要：</p><h4 id="编码器：使用神经网络方法拟合参数"><a href="#编码器：使用神经网络方法拟合参数" class="headerlink" title="编码器：使用神经网络方法拟合参数"></a>编码器：使用神经网络方法拟合参数</h4><p>构建两个神经网络：$\mu_k=f_1(X<em>k), log</em>{\sigma^2}=f_2(X_k)$来拟合均值和方差。当二者尽量接近零的时候，分布也就达到了$\mathcal{N}(0,I)$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z_mean = Dense(latent_dim)(h)</span><br><span class="line">z_log_var = Dense(latent_dim)(h)</span><br></pre></td></tr></table></figure></p><p>针对两个损失的比例选取，使用KL散度$KL(N(\mu,\sigma^2)||N(0,I))$作为额外的loss。上式的计算结果为：</p><p>$$\mathcal{L}<em>{\mu,\sigma^2}=\frac{1}{2} \sum</em>{i=1}^d \Big(\mu<em>{(i)}^2 + \sigma</em>{(i)}^2 - \log \sigma_{(i)}^2 - 1\Big)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kl_loss = - <span class="number">0.5</span> * K.<span class="built_in">sum</span>(<span class="number">1</span> + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="解码器：保证生成能力"><a href="#解码器：保证生成能力" class="headerlink" title="解码器：保证生成能力"></a>解码器：保证生成能力</h4><p>我们最终的目标则是最小化误差$\mathcal{D}(\hat{X_k},X_k)^2$。</p><p>解码器重构$X$的过程是希望没噪声的，而$KL loss$则希望有高斯噪声的，两者是对立的。所以，VAE跟GAN一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的。</p><h4 id="reparameterization-trick（重参数技巧）"><a href="#reparameterization-trick（重参数技巧）" class="headerlink" title="reparameterization trick（重参数技巧）"></a>reparameterization trick（重参数技巧）</h4><p>在反向传播优化均值和方差的过程中，“采样”操作是<strong>不可导</strong>的，但是采样的结果是可导的。</p><p>因此可以利用标准正太分布采样出的$\epsilon$直接估算$Z=\mu+\epsilon\times\sigma$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sampling</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Reparameterization trick by sampling fr an isotropic unit Gaussian.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments:</span></span><br><span class="line"><span class="string">        args (tensor): mean and log of variance of Q(z|X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns:</span></span><br><span class="line"><span class="string">        z (tensor): sampled latent vector</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    z_mean, z_log_var = args</span><br><span class="line">    batch = K.shape(z_mean)[<span class="number">0</span>]</span><br><span class="line">    dim = K.int_shape(z_mean)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># by default, random_normal has mean=0 and std=1.0</span></span><br><span class="line">    epsilon = K.random_normal(shape=(batch, dim))</span><br><span class="line">    <span class="keyword">return</span> z_mean + K.exp(<span class="number">0.5</span> * z_log_var) * epsilon</span><br></pre></td></tr></table></figure></p><p>于是“采样”操作不再参与梯度下降，改为采样的结果参与，使得整个模型可以训练。</p><h2 id="DEMO-基于CNN和VAE的作诗机器人"><a href="#DEMO-基于CNN和VAE的作诗机器人" class="headerlink" title="DEMO:基于CNN和VAE的作诗机器人"></a>DEMO:基于CNN和VAE的作诗机器人</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzt1xl9cj20rf0imjv6.jpg" alt=""><br>先将每个字embedding为向量，然后用层叠CNN来做编码，接着池化得到一个encoder的结果，根据这个结果生成计算均值和方差，然后生成正态分布并重新采样。在解码截断，由于现在只有一个encoder的输出结果，而最后要输出多个字，所以先接了多个不同的全连接层，得到多样的输出，然后再接着全连接层。</p><h4 id="GCNN-Gated-Convolutional-Networks"><a href="#GCNN-Gated-Convolutional-Networks" class="headerlink" title="GCNN(Gated Convolutional Networks)"></a>GCNN(Gated Convolutional Networks)</h4><p>这里的CNN不是普通的CNN+ReLU，而是facebook提出的GCNN，其实就是做两个不同的、外形一样的CNN，一个不加激活函数，一个用sigmoid激活，然后把结果乘起来。这样一来sigmoid那部分就相当于起到了一个“门（gate）”的作用。</p><h2 id="参考与引用"><a href="#参考与引用" class="headerlink" title="参考与引用"></a>参考与引用</h2><ul><li><a href="https://kexue.fm/archives/5253">https://kexue.fm/archives/5253</a></li><li><a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></li><li><a href="https://kexue.fm/archives/5332">https://kexue.fm/archives/5332</a></li><li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">https://jaan.io/what-is-variational-autoencoder-vae-tutorial/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> VAE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从Pycharm到SpaceVim</title>
      <link href="/2018/09/27/space-vim-usage/"/>
      <url>/2018/09/27/space-vim-usage/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>Pycharm太吃内存，因此转向使用SpaceVim。基本配置记录如下：</p></blockquote><h2 id="环境基本配置"><a href="#环境基本配置" class="headerlink" title="环境基本配置"></a>环境基本配置</h2><h3 id="安装SpaceVim"><a href="#安装SpaceVim" class="headerlink" title="安装SpaceVim"></a>安装SpaceVim</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -sLf https://spacevim.org/install.sh | bash</span><br><span class="line"><span class="comment"># 如果当前环境没有安装curl，并且没有管理员权限</span></span><br><span class="line">wget https://spacevim.org/install.sh</span><br><span class="line">bash install.sh</span><br><span class="line"><span class="comment"># 启动vim并且安装插件</span></span><br></pre></td></tr></table></figure><h3 id="配置-SpaceVim-autoload-SpaceVim-vim"><a href="#配置-SpaceVim-autoload-SpaceVim-vim" class="headerlink" title="配置.SpaceVim/autoload/SpaceVim.vim"></a>配置.SpaceVim/autoload/SpaceVim.vim</h3><ul><li><code>set timeoutlen=10</code>:空格键延迟</li><li><code>let g:spacevim_relativenumber=0</code>:禁用相对行号</li></ul><h3 id="配置-SpaceVim-d-init-vim"><a href="#配置-SpaceVim-d-init-vim" class="headerlink" title="配置.SpaceVim.d/init.vim"></a>配置.SpaceVim.d/init.vim</h3><ul><li><code>[[layers]] \n name=&quot;lang#python&quot;</code>:使用python</li><li><code>disable_plugins=[&quot;neomake.vim&quot;]</code>:禁用插件（当使用conda的时候，这个插件会报错）</li></ul><h3 id="更新-SPUpdate"><a href="#更新-SPUpdate" class="headerlink" title="更新:SPUpdate"></a>更新<code>:SPUpdate</code></h3><h2 id="常用命令备忘"><a href="#常用命令备忘" class="headerlink" title="常用命令备忘"></a>常用命令备忘</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><ul><li><code>e</code>: 打开一个空的编辑器</li><li><code>:e file_name</code>: 打开文件（该操作<strong>也可以</strong>在nerdtree上完成）</li></ul><h3 id="space-b系列-缓冲区"><a href="#space-b系列-缓冲区" class="headerlink" title="space+b系列(缓冲区)"></a>space+b系列(缓冲区)</h3><ul><li><code>N+l</code>:在右侧新建buffer</li><li><code>d</code>:删除buffer</li></ul><h3 id="space-f系列-文件管理"><a href="#space-f系列-文件管理" class="headerlink" title="space+f系列(文件管理)"></a>space+f系列(文件管理)</h3><ul><li><code>t</code>:开关nerdtree </li></ul><h3 id="space-w系列-窗口管理"><a href="#space-w系列-窗口管理" class="headerlink" title="space+w系列(窗口管理)"></a>space+w系列(窗口管理)</h3><ul><li><code>o</code>:切换到下一个窗口</li><li><code>/</code>和<code>-</code>:分别在右侧和下方分隔窗口</li><li><code>F</code>: 新建Tab</li></ul><h3 id="space-t系列（Toggle管理）"><a href="#space-t系列（Toggle管理）" class="headerlink" title="space+t系列（Toggle管理）"></a>space+t系列（Toggle管理）</h3><ul><li><code>t</code>:打开tab管理</li></ul><h3 id="space-c系列-注释）"><a href="#space-c系列-注释）" class="headerlink" title="space+c系列(注释）"></a>space+c系列(注释）</h3><ul><li><code>l</code>:注释选中行（配合<code>V</code>模式）</li></ul><h3 id="space-s系列-搜索"><a href="#space-s系列-搜索" class="headerlink" title="space+s系列(搜索)"></a>space+s系列(搜索)</h3><ul><li><code>s</code>:直接在当前文件搜索</li></ul><h3 id="space-l系列（语言）"><a href="#space-l系列（语言）" class="headerlink" title="space+l系列（语言）"></a>space+l系列（语言）</h3><ul><li><code>r</code>:执行代码</li></ul><h2 id="参考与引用"><a href="#参考与引用" class="headerlink" title="参考与引用"></a>参考与引用</h2><ul><li><a href="https://spacevim.org/cn/layers">https://spacevim.org/cn/layers</a></li><li><a href="https://everettjf.gitbooks.io/spacevimtutorial">https://everettjf.gitbooks.io/spacevimtutorial</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDE </tag>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象差分注意力机制</title>
      <link href="/2018/09/21/Object-Difference-Attention-paper-note/"/>
      <url>/2018/09/21/Object-Difference-Attention-paper-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：1. 直接从论文pdf中获取2. 从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Object-Difference Attention: A Simple Relational Attention for Visual Question Answering</p></li><li><p>论文链接：<a href="http://www.acmmm.org/2018/accepted-papers/">http://www.acmmm.org/2018/accepted-papers/</a></p></li><li><p>论文源码：</p><ul><li>None</li></ul></li><li><p>关于作者：</p><ul><li>吴晨飞，北邮AI Lab博士</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><!-- Ex: 论文摘要的中文翻译最近对话生成的神经模型为会话代理生成响应提供了很大的希望，但往往是短视的，一次预测一个话语而忽略它们对未来结果的影响。对未来的对话方向进行建模对于产生连贯，有趣的对话至关重要，这种对话需要传统的NLP对话模式借鉴强化学习。在本文中，我们将展示如何整合这些目标，应用深度强化学习来模拟聊天机器人对话中的未来奖励。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性，连贯性和易于回答（与前瞻性功能相关）。我们在多样性，长度以及人类评判方面评估我们的模型，表明所提出的算法产生了更多的交互式响应，并设法在对话模拟中促进更持久的对话。这项工作标志着基于对话的长期成功学习神经对话模型的第一步。--><p>注意机制极大地促进了视觉问答技术(VQA)的发展。注意力分配在注意力机制中起着至关重要的作用，它根据对象(如图像区域或定界框)回答问题的重要性对图像中的对象(如图像区域或包围盒)进行不同的权重。现有的工作大多集中在融合图像特征和文本特征来计算注意力分布，而不需要比较<strong>不同的图像对象</strong>。作为注意力的一个主要属性，<strong>分离度</strong>取决于不同对象之间的比较。这种比较为更好地分配注意力提供了更多的信息。为了实现对目标的可感知性，我们提出了一种对象差分注意(ODA)方法，通过在图像中实现不同图像对象之间的差值运算来计算注意概率。实验结果表明，我们基于ODA的VQA模型得到了最先进的结果。此外，还提出了一种关系注意的一般形式。除了ODA之外，本文还介绍了其他一些相关的注意事项。实验结果表明，这些关系关注在不同类型的问题上都有优势。</p><h2 id="对象差分注意力机制：视觉问答中一个简单的关系注意力机制"><a href="#对象差分注意力机制：视觉问答中一个简单的关系注意力机制" class="headerlink" title="对象差分注意力机制：视觉问答中一个简单的关系注意力机制"></a>对象差分注意力机制：视觉问答中一个简单的关系注意力机制</h2><!-- Ex: ## 强化学习在对话生成领域的应用 --><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="本文术语"><a href="#本文术语" class="headerlink" title="本文术语"></a>本文术语</h4><!-- 针对论文中不常用的术语进行简短的解释，方便读者理解 --><ol><li><p>序列编码的方式：</p><ol><li><strong>RNN</strong>: $y<em>t=f(y</em>{t-1},x_t)$</li><li><strong>CNN</strong>: $y<em>t=f(x</em>{t-1},x<em>t,x</em>{t+1})$</li><li><strong>Attention</strong>: $y_t=f(x_t, A, B), if A = B = X: Self Attention$</li></ol></li><li><p>注意力机制的例子<br>$$Attention(Q,K,V)$$</p></li><li><p>应用于VQA的注意力机制编年史：</p><ol><li>one-step linear fusion</li><li>multi-step linear fusion</li><li>bilinear fusion</li><li>multi-feature attention</li></ol></li><li><p>Mutan机制</p></li></ol><h4 id="论文写作动机"><a href="#论文写作动机" class="headerlink" title="论文写作动机"></a>论文写作动机</h4><!-- 当前研究领域存在的问题Ex:标准的Seq-to-Seq模型用于对话系统时常常使用MLE作为模型的评价标准，但这往往导致下面两个主要缺点：系统倾向于产生一些普适性的回应，也就是dull response，这些响应可以回答很多问题但却并不是我们想要的，我们想要的是有趣、多样性、丰富的回应；系统的回复不具有前瞻性，有时会导致陷入死循环，导致对话轮次较少。也就是产生的响应没有考虑对方是否容易回答的情况。--><ol><li>现有的工作大多集中在融合图像特征和文本特征来计算注意力分布，而<strong>忽略了</strong>比较不同的图像对象之间的差异。<br> <img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvisv9uyyhj20i10cw46l.jpg" alt=""><br> 如上图，想要回答出问题<code>图中最高的花是什么？</code>，我们建立的模型就需要不仅仅关注潜在答案<code>玫瑰</code>，也应该关注<code>兰花</code>。</li><li>如何合理分配现有问题的注意力？</li></ol><h3 id="解决问题的方法"><a href="#解决问题的方法" class="headerlink" title="解决问题的方法"></a>解决问题的方法</h3><h4 id="玫瑰例子"><a href="#玫瑰例子" class="headerlink" title="玫瑰例子"></a>玫瑰例子</h4><p>对于回答<code>图中最高的花是什么？</code>，一共分几步？</p><ol><li>找到图中所有的花。</li><li>比较不同的花对于正确答案的重要性。</li></ol><p>正确的答案就会在<strong>比较</strong>的过程中产生。若以这个例子作为启发，一种新型的注意力机制的思路便产生了：ODA在问题的指导下，通过将每个图像对象与其他所有对象进行对比，计算出图像中物体的注意注意力分布。</p><h4 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjiq8dptpj20pw0bdgqo.jpg" alt=""></p><ol><li><p>将数据Embedding</p><ol><li>$V^f=RCNN(image)$,其中$v^f$是一个$m\times{d_v}$维的embedding，代表拉出的$m$个框。</li><li>$Q^f=GRU(question)$，其中$Q^f$代表$d_q$维的问题embedding。</li><li>$V=relu(Conv1d(V^f))$</li><li>$Q=relu(Linear(Q^f))$</li></ol></li><li><p>对象差分注意力<br>$$\hat{V}=softmax([(V_i-V<em>j)\odot{Q}]</em>{m\odot{md}}W_f)^{T}V$$<br>该模型的优点：</p><ol><li>通过对比(差分))，我们可以选择更重要的对象。</li><li>计算复杂度相对与传统注意力机制模型（Mutan）低。</li><li>”即插即用“的特性使得该模型十分容易应用到其他领域。</li></ol></li><li><p>决策阶段</p><ol><li><p>通过对$\hat{V}$计算$p$次，并且将结果拼接在一起。<br>$$\hat{Z}=[\hat{V}^{1};\hat{V}^{2};…;\hat{V}^{p}]$$</p><blockquote><p>可以参考Attention is all you need模型的multi-head</p></blockquote></li><li>将图片的特征和问题的特征相结合<br>$$H=\sum^s_{s=1}(\hat{Z}W_v^{(s)}\odot{QW_q^{(s)}})$$</li><li>预测<br>$$\hat{a}=\sigma(W_{h}H)$$</li></ol></li></ol><h4 id="扩展：相关性注意力"><a href="#扩展：相关性注意力" class="headerlink" title="扩展：相关性注意力"></a>扩展：相关性注意力</h4><p>针对模型中$(V_i-V_j)\odot{Q}$部分进行扩展，可以得到不同类型的注意力机制<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjt8ggw48j20dk06emya.jpg" alt=""></p><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>VQA1.0 dataset</li><li>VQA2.0 dataset</li><li>COCO-QA dataset</li></ul><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li>针对VQA1.0和VQA2.0，使用准确率：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtavlwoxj209701hgli.jpg" alt=""></li><li>针对COCO_QA使用：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtbn6b1pj207m00tdfo.jpg" alt=""></li></ul><h4 id="实验结果评价"><a href="#实验结果评价" class="headerlink" title="实验结果评价"></a>实验结果评价</h4><ul><li>在VQA1.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtf0nyn4j20qs0c8wh1.jpg" alt=""></li><li>在VQA2.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtfgxjdxj20ht05qmy9.jpg" alt=""></li><li>在VQA3.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtg34t3dj20mm05twfl.jpg" alt=""></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从感性的角度来说，对象差分注意力机制符合人类根据图片回答问题的思考过程。未来的研究方向应该是通过对世界的常识性知识建立一个世界模型，通过先验知识减少计算量和对大量带有标签的数据的依赖性。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ol><li><a href="https://kexue.fm/archives/4765">https://kexue.fm/archives/4765</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>世界模型的解读</title>
      <link href="/2018/09/21/world-model-to-learn-them-all/"/>
      <url>/2018/09/21/world-model-to-learn-them-all/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：World Models</p><!-- Ex: 1. 论文名：Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs. --></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1803.10122.pdf">https://arxiv.org/pdf/1803.10122.pdf</a></p><!-- Ex: 2. https://arxiv.org/abs/1606.01541  --></li><li><p>论文源码：</p><ul><li><a href="https://worldmodels.github.io/">https://worldmodels.github.io/</a></li></ul></li></ol><!--    - https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow    - https://github.com/agsarthak/Goal-oriented-Dialogue-Systems--><ol><li><p>关于作者：</p><!-- 建议从google schoolar获取详细信息  - first_author: position, times_cited--><ul><li>David Ha：</li><li>Jurgen Schimidhuber: LSTM之父，无需多言</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。</li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>通过探索并且建立流行的强化学习环境的生成神经网络模型。<strong>世界模型</strong>可以在无监督的情况下快速训练，以学习环境的压缩时空表示。通过使用从世界模型中提取的特征作为Agent的输入，可以训练一个非常紧凑和简单的策略来解决所需的任务。甚至可以训练Agent完全在它自己的世界模型所产生的<strong>梦</strong>中，并将这个策略转移回实际环境中。</p><h2 id="时代背景：可以预见的寒冬-—-Yann-Lecun"><a href="#时代背景：可以预见的寒冬-—-Yann-Lecun" class="headerlink" title="时代背景：可以预见的寒冬 —- Yann Lecun"></a>时代背景：可以预见的寒冬 —- Yann Lecun</h2><ul><li>深度学习缺少推断能力：关于世界的常识与对于任务背景的认知</li><li>从零开始学习真的很低效：深度学习需要一个记忆模块（预训练）</li><li>自监督学习（Learn how to learn）</li></ul><h2 id="世界模型"><a href="#世界模型" class="headerlink" title="世界模型"></a>世界模型</h2><h3 id="论文写作动机"><a href="#论文写作动机" class="headerlink" title="论文写作动机"></a>论文写作动机</h3><ol><li><p>哲学问题：人类究竟如何认识世界？<br> 人类通过有限的感知能力（眼睛、鼻子、耳朵、皮肤），逐渐建立一个自己的<strong>心智模型</strong>。人类的一切决策和动作则均根据每个人自己的内部模型的<strong>预测</strong>而产生。</p></li><li><p>人类如何处理日常生活的信息流？<br> 通过<strong>注意力机制</strong>学习客观世界时空方面的抽象表达。</p></li><li><p>人类的潜意识如何工作？<br> 以棒球为例子，击球手在如此短的时间（短于视觉信号到达大脑的时间！）内需要作出何时击球的动作。<br> 人类可以完成击球的原因便是因为人类天生的心智模型可以预测棒球的运动路线。</p></li><li><p>我们是否可以让模型根据环境自觉建立特有的模型进行自学习？</p></li><li><p>Jurgen历史性的工作总结：强化学习背景下的RNN-based世界模型!</p></li></ol><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><h4 id="总览Agent模型"><a href="#总览Agent模型" class="headerlink" title="总览Agent模型"></a>总览Agent模型</h4><ol><li><p>视觉感知元件：<strong>压缩</strong>视觉获取到的信息/环境表征</p></li><li><p>记忆元件：根据历史信息对客观环境进行<strong>预 测</strong></p></li><li><p>决策模块：根据视觉感知元件和记忆元件选择<strong>行动策略</strong></p></li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvm2l4tc84j20wf0ledkn.jpg" alt=""></p><h4 id="VAE-V-Model"><a href="#VAE-V-Model" class="headerlink" title="VAE(V) Model"></a>VAE(V) Model</h4><p>Agent通过VAE可以从观察的每一帧中学习出抽象的、压缩的表示。</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvm3sg25puj20wn09zq7g.jpg" alt=""></p><h4 id="MDN-RNN-M-Model"><a href="#MDN-RNN-M-Model" class="headerlink" title="MDN-RNN(M) Model"></a>MDN-RNN(M) Model</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmurcu30uj20u10l8jta.jpg" alt=""><br>其中下一时刻的预测$z<em>{t+1}$使用概率的形式表示为$P(z</em>{t+1}|a_t,z_t,h_t)$,其中$a_t$是在$t$时刻的动作。<br>并且在采样阶段，通过调整温度参数$\tau$来控制模型的模糊度。(这个参数对后续训练控制器$C$十分有效)</p><p>模型最上方的<strong>MDN</strong>表示<strong>Mixture Density Network</strong>，输出的是预测的z的高斯混合模型。</p><h4 id="Controller-C-Model"><a href="#Controller-C-Model" class="headerlink" title="Controller(C) Model"></a>Controller(C) Model</h4><p>这个模块用来根据最大累计Reward决定Agent下一个时刻的行动。论文中故意将这个模块设置的尽量小并且简单。</p><p>因此控制器是一个简单的单层线性模型：<br>$$a_t=W_c[z_t h_t]+b_c$$</p><p>特别指出，优化控制器参数的方法不是传统的梯度下降，而是<strong>Covariance-Matrix Adaptation Evolution Strategy</strong></p><h4 id="结合三个模块"><a href="#结合三个模块" class="headerlink" title="结合三个模块"></a>结合三个模块</h4><p>下面的流程图展示了$V$、$M$和$C$如何与环境进行交互：首先每个时间步$t$原始的观察输入由$V$进行处理生成压缩后的$z(t)$。随后$C$的输入是$z(t)$和$M$的隐状态$h(t)$。随后$C$输出动作矢量$a(t)$影响环境。$M$以当前时刻的$z(t)$和$a(t)$作为输入，预测下一时刻的隐状态$h(t+1)$。</p><blockquote><p>在代码中，对$M$模块的输入有很多种方式。我不太认同图中把$C$选择的动作也当做$M$的输入。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmvf1dkd5j20p50ic77b.jpg" alt=""></p><p>通过伪代码表示模型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rollout</span>(<span class="params">controller</span>):</span><br><span class="line">  obs = env.reset()</span><br><span class="line">  h = rnn.initial_state()</span><br><span class="line">  done = <span class="literal">False</span></span><br><span class="line">  cumulative_reward = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">    z = vae.encode(obs)</span><br><span class="line">    a = controller.action([z, h])</span><br><span class="line">    obs, reward, done = env.step(a)</span><br><span class="line">    cumulative_reward += reward</span><br><span class="line">    h = rnn.forward([a, z, h])</span><br><span class="line">  <span class="keyword">return</span> cumulative_reward</span><br></pre></td></tr></table></figure></p><h3 id="实验设计1"><a href="#实验设计1" class="headerlink" title="实验设计1"></a>实验设计1</h3><p>两个实验的环境均选自<code>OpenAI Gym</code></p><h4 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h4><p>CarRacing-v0(Car Racing Experiment)</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmw8osn7xj20vh0kt43n.jpg" alt=""></p><p>动作空间有</p><ol><li>左转</li><li>右转</li><li>加速</li><li>刹车</li></ol><h4 id="实验实现流程"><a href="#实验实现流程" class="headerlink" title="实验实现流程"></a>实验实现流程</h4><ol><li>根据随机的策略收集10，000次游戏过程</li><li>根据每个游戏过程的每一帧训练VAE模型，输出结果为$z\in \mathcal{R}^{32}$</li><li>训练MDN-RNN模型，输出结果为$P(z_{t+1}|a_t,z_t,h_t)$</li><li>定义控制器（c）,$a_t=W_c[z_t h_t]+b_c$</li><li>使用CMS-ES算法得到最大化累计Reward的$$W_b$与$b_c$</li></ol><p>模型参数共有：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmwg6zv1ej20gg063jru.jpg" alt=""></p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ol><li><p>只添加V Model(VAE)<br>如果没有M Model(MDN-RNN)模块，控制器的公式便为：$a<em>t=W</em>{c}z_t+b_c$。<br>实验结果表明这会导致Agent不稳定的驾驶行为。<br>在这种情况下，尝试控制器添加一层隐含层，虽然实验效果有所提升，但是仍然没能达到很好的效果。</p></li><li><p>世界模型完全体（VAE+MDN-RNN）<br>实验结果表明，Agent驾驶得更加稳定。<br>因为$h_t$包含了当前环境关于未来信息的概率分布，因此Agent可以向一级方程式选手和棒球手一样迅速做出判断。</p></li><li><p>世界模型与其他模型的对比：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxb7t9zdj20op09odi1.jpg" alt=""></p></li><li><p>对世界模型当前状态$z_{t+1}$进行可视化<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxeae7jyj20g00g4jwu.jpg" alt=""><br>上图将$\tau$设置为0.25（这个参数可以调节生成环境的模糊程度）</p></li></ol><h3 id="实验设计2"><a href="#实验设计2" class="headerlink" title="实验设计2"></a>实验设计2</h3><p><strong>我们是否可以让Agent在自己的梦境中学习，并且改变其对真实环境的策略</strong><br>如果世界模型对其<strong>目的</strong>有了充分的认识，那么我们就可以使用世界模型代替Agent真实观察到的环境。（类比我们下楼梯的时候，根本不需要小心翼翼地看着楼梯）<br>最终，Agent将不会直接观察到现实世界，而只会看到世界模型<strong>让</strong>它看到的事物。</p><h4 id="实验环境-1"><a href="#实验环境-1" class="headerlink" title="实验环境"></a>实验环境</h4><p>VizDoom Experiment</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxq3o7ihj20g10bywio.jpg" alt=""></p><p>游戏目的是控制Agent躲避怪物发出的火球。</p><h4 id="实验实现流程-1"><a href="#实验实现流程-1" class="headerlink" title="实验实现流程"></a>实验实现流程</h4><p>模型的M Model(MDN-RNN)主要负责预测Agent下一时刻（帧）是否会死亡。<br>当Agent在其世界模型中进行训练的时候，便不需要V Model对真实环境的像素进行编码了。</p><ol><li>从随机策略中选取10，000局游戏（同实验一）</li><li>根据每次游戏的每一帧训练VAE模型，得到$z\in \mathcal{R}^{64}$($z$的维度变成了64)，之后使用VAE模型将收集的图像转换为隐空间表示。</li><li>训练MDN-RNN模型，输出结果为$P(z<em>{t+1},d</em>{t+1}|a_t,z_t,h_t)$</li><li>定义控制器为$a_t=W_c[z_t h_t]$</li><li>使用CMA-ES算法从世界模型构建的虚拟环境中得到最大化累计生存时间的$W_c$</li></ol><p>模型的参数共有：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvn0lmh3rdj20li07dwf8.jpg" alt=""></p><h4 id="模糊化世界模型"><a href="#模糊化世界模型" class="headerlink" title="模糊化世界模型"></a>模糊化世界模型</h4><p>通过增加模糊度参数$\tau$，会使得游戏变得更难（世界模型生成的环境更加模糊）。<br>如果Agent在高模糊度参数表现的很好的话，那么在正常模式下通常表现的更好。</p><p>也就是说，即使<strong>V model(VAE)不能够正确的捕捉每一帧全部的信息</strong>，Agent也能够完成真实环境给定的任务。</p><p>实验结果表明，模糊度参数太低相当于没有利用这个参数，但是太高的话模型又相当于”近视“了。因此需要找到一个合适的模糊度参数值。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="泛化：迭代式训练程序"><a href="#泛化：迭代式训练程序" class="headerlink" title="泛化：迭代式训练程序"></a>泛化：迭代式训练程序</h4><ol><li>随机初始化M Model(MDN-RNN)和C Model(Controller)的参数</li><li>对真实环境进行N次试验。保存每次试验的动作$a_t$和观察$x_t$</li><li>训练M Model(MDN-RNN)，得到$P(x<em>{t+1},r</em>{t+1},a<em>{t+1},d</em>{t+1}|x_t,a_t,h_t)$；训练C Model(Controller)并且M中的最优化期望rewards。</li><li>回到第2步如果任务没有结束</li></ol><p>这个泛化程序的特点是从M model中不仅仅要得到预测的观察$x$和是否结束任务$done$，</p><p>一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。</p><h4 id="从信息到记忆：海马体的魔术"><a href="#从信息到记忆：海马体的魔术" class="headerlink" title="从信息到记忆：海马体的魔术"></a>从信息到记忆：海马体的魔术</h4><p>神经科学的研究（2017 Foster）发现了海马体重映现象：当动物休息或者睡觉的时候，其大脑会重新放映最近的经历。并且海马体重映现象对巩固记忆十分重要。</p><h4 id="注意力：只关心任务相关的特征"><a href="#注意力：只关心任务相关的特征" class="headerlink" title="注意力：只关心任务相关的特征"></a>注意力：只关心任务相关的特征</h4><p>神经科学的研究（2013 Pi）发现，主要视觉神经元只有在受到奖励的时候才会被从抑制状态激活。这表明人类通常从任务相关的特征中学习，而非接收到的所有特征。（该结论至少在成年人中成立）</p><h4 id="未来的展望"><a href="#未来的展望" class="headerlink" title="未来的展望"></a>未来的展望</h4><p>当前的问题主要出现在M Model(MDN-RNN)上：受限于RNN模型的信息存储能力。人类的大脑能够存储几十年甚至几百年的记忆，但是神经网络会因为梯度消失导致训练困难。</p><p>如果想让Agent可以探索更加复杂的世界，那么未来的工作可能是设计出一个可以<strong>代替MDN-RNN结构的模型</strong>，或者开发出一个<strong>外部记忆模块</strong>。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ol><li><a href="https://arxiv.org/pdf/1803.10122.pdf">https://arxiv.org/pdf/1803.10122.pdf</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> JurgenSchmidhuber </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自复制自动机理论读书笔记</title>
      <link href="/2018/09/18/theory-of-self-reproducing-automata/"/>
      <url>/2018/09/18/theory-of-self-reproducing-automata/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>未完待续已完成10%</p><h1 id="Theory-of-Self-Reproducing-Automata-自复制自动机理论"><a href="#Theory-of-Self-Reproducing-Automata-自复制自动机理论" class="headerlink" title="Theory of Self-Reproducing Automata(自复制自动机理论)"></a>Theory of Self-Reproducing Automata(自复制自动机理论)</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>冯诺依曼从20世纪40年代后期开始研究自动机理论。按照时间顺序他完成了五篇著作：</p><ol><li>自动机的通用逻辑理论（The General and Logical Theory of Automata）</li><li>复杂自动机的理论和结构（Theory and Organization of Complicated Automata）</li><li>概率逻辑：从不可靠组件合成可靠整体（Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components）</li><li>自动机理论：构建、复制以及同质性（The Theory of Automata:Constructions, Reproduction, Homogeneity）</li><li>计算机与人脑（The Computer and the Brain）</li></ol><p>冯诺依曼构思了一套基本逻辑单元构成的复杂系统理论。</p><blockquote><p>如果一个数学主题已经远离了所有的实证源头，而且仅仅跟一些非常‚抽象‛的领域有交叉的时候，这个数学主题就会濒临衰退了……无论这一阶段何时来到，唯一补救的办法就是在它的源头处重生：重新注入或多或少的实证经验 —— 冯诺依曼</p></blockquote><h2 id="冯诺依曼的自动机理论"><a href="#冯诺依曼的自动机理论" class="headerlink" title="冯诺依曼的自动机理论"></a>冯诺依曼的自动机理论</h2><h3 id="生物与人工自动机"><a href="#生物与人工自动机" class="headerlink" title="生物与人工自动机"></a>生物与人工自动机</h3><p>通过考察两种主要类型的自动机：人工的和生物自动机。</p><p>模拟与数字计算机是最重要的一类人工自动机，但通讯或信息处理目的而造的其他的人造系统也包括在其中，如电话和收音机广播系统等。生物自动机则包括了<code>神经系统</code>、<code>自复制</code>和<code>自修复系统</code>，以及<code>生命的进化与适应</code>等特性。</p><p>冯纽曼花费了很大的精力来比较生物与人工自动机的异同。我们可以将<br>这些结论概括成如下几个方面：</p><ol><li>模拟与数字的不同： <ul><li>自然生命体是一种混合体，同时包含了模拟与数字过程。</li><li>神经元是“有或无”的，因此数字真值函数逻辑是神经行为的一种初级近似。</li><li>神经元的激活有有赖于空间上的刺激加总，这些都是连续而非离散的过程。</li><li>在复杂的有机体中，数字运算通常与模拟过程交替进行。</li></ul></li><li>基本元件所用到的物理和生物的材料<ul><li>计算机的基本元件要比神经元大得多，而且需要更多的能量，但是它们的速度要快很多。</li><li>生物自动机是通过一种更加并行的方式工作的，而数字计算机则是串行结构。【注：此为冯诺依曼受限于时代背景的结论】</li><li>真空管和神经元尺寸的不同是由于它们所用材料的机械稳定性不同而引起的。真空管要更容易被损坏却不好修复。而当神经元膜受到破坏以后，会很容易地被修复。</li></ul></li><li>复杂性<ul><li>人，也包括天地万物，是一种比他们能够构建的人工自动机更复杂得多的生物自动机。</li><li>人类对于它自己的逻辑设计的细节理解要远远比不上对他所构建的最大型计算机的理解。</li></ul></li><li>逻辑组织<ul><li>在一个特定的计算机中，有高速的电子寄存器，有低速的磁芯，以及更慢的磁带单元。【注：当前时代背景下可以类比多级存储体系】</li><li>神经环路中的脉冲、神经阈值的改变、神经系统的组织以及基因中的编码就也构成了这层级实例。</li></ul></li><li>可靠性<ul><li>生物自动机在这方面显然要胜过人工自动机，就是因为它们有着强大的自检验自修复的功能。【注：癌症和衰老呢？】</li></ul></li></ol><h2 id="自动机理论的数学原理"><a href="#自动机理论的数学原理" class="headerlink" title="自动机理论的数学原理"></a>自动机理论的数学原理</h2><p>起始于数理逻辑，而朝向分析、概率以及热力学靠近。</p><h3 id="控制与信息理论"><a href="#控制与信息理论" class="headerlink" title="控制与信息理论"></a>控制与信息理论</h3><p>图灵机和<code>McCulloch &amp; Pitts</code>的神经网络分别处于信息理论的两个极端。</p><h4 id="McCulloch-amp-Pitts-的神经网络：组合方法"><a href="#McCulloch-amp-Pitts-的神经网络：组合方法" class="headerlink" title="McCulloch &amp; Pitts 的神经网络：组合方法"></a>McCulloch &amp; Pitts 的神经网络：<strong>组合方法</strong></h4><blockquote><p>神经网络由非常简单的零件组成复杂结构。因此只需要对底层的零件作公理化定义就可以得到非常复杂的组合</p></blockquote><p>神经元的定义如下：我们用一个小圆圈代表一个神经元，从圆圈延伸出的直线则代表神经突触。箭头表示某神经元的突触作用于另一个神经元之上，也就是信号的传送方向。神经元有两个状态：激发和非激发。</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fve2xio54pj20d908szlq.jpg" alt=""></p><p>人类神经元有神奇的涌现结果：没有轮廓的三角形，但是你的眼睛却可以帮你勾勒出它的轮廓。<br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fwexofxywsj20mu0iagov.jpg" alt=""></p><h4 id="图灵机"><a href="#图灵机" class="headerlink" title="图灵机"></a>图灵机</h4><blockquote><p>是对于整个自动机进行了公理化的定义，他仅仅定义了自动机的功能，并没有涉及到具体的零件。</p></blockquote><p>对于高复杂度的形式逻辑对象，很难提前预测它的行为，最好的办法就是把它实际制造出来运行。这是根据哥德尔定理得出的结论：</p><blockquote><p>从逻辑上说，对于一个对象的描述要比这个对象本身要高一个级别。因此，前者总是比后者要长。</p></blockquote><h2 id="大数之道"><a href="#大数之道" class="headerlink" title="大数之道"></a>大数之道</h2><p>生命应该是同概率完全整合在一起的，生命可以在<strong>错误里面持续运行</strong>！在生命中的误差，不会像在计算过程中那样不断的扩散放大。生命是十分完善且具有适应性的系统，一旦中间发生了某种问题，系统会自动地认识到这个问题的严重程度。</p><pre><code>1. 如果无关紧要，那么系统就会无视问题，继续运作2. 如果问题对系统比较重要，系统就会把发生故障的区域封闭起来，绕过它，通过其他补救渠道继续运行。</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Theory of Self-Reproducing Automata[von Neumann]</li><li><a href="http://swarmagents.cn.13442.m8849.cn/thesis/program/jake_358.pdf">http://swarmagents.cn.13442.m8849.cn/thesis/program/jake_358.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural-Machine-Translation-by-tensorflow</title>
      <link href="/2018/09/09/Neural-Machine-Translation-by-tensorflow/"/>
      <url>/2018/09/09/Neural-Machine-Translation-by-tensorflow/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="模型使用的公开数据集"><a href="#模型使用的公开数据集" class="headerlink" title="模型使用的公开数据集"></a>模型使用的公开数据集</h2><p>IWSLT Evaluation Campaign<br>WMT Evaluation Campaign</p><hr><h2 id="基本seq2seq模型"><a href="#基本seq2seq模型" class="headerlink" title="基本seq2seq模型"></a>基本seq2seq模型</h2>]]></content>
      
      
      <categories>
          
          <category> slide </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> slide </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记：A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models</title>
      <link href="/2018/09/09/paper-note-A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models/"/>
      <url>/2018/09/09/paper-note-A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：A Diversity-Promoting Objective Function for Neural Conversation Models</p><!-- Ex: 1. 论文名：Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs. --></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1510.03055.pdf">https://arxiv.org/pdf/1510.03055.pdf</a></p><!-- Ex: 2. https://arxiv.org/abs/1606.01541  --></li><li><p>论文源码：</p><ul><li>None</li></ul></li></ol><!--    - https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow    - https://github.com/agsarthak/Goal-oriented-Dialogue-Systems--><ol><li><p>关于作者：</p><!-- 建议从google schoolar获取详细信息  - first_author: position, times_cited--><ul><li>Jiwei Li：斯坦福大学博士毕业生，截至发稿被引次数：2156</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。</li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>文章提出使用最大互信息（Maximum Mutual Information MMI）代替原始的最大似然（Maximum Likelihood)作为目标函数，目的是使用互信息减小“I don’t Know”这类无聊响应的生成概率。</p><h2 id="一种促进神经对话模型多样性的目标函数"><a href="#一种促进神经对话模型多样性的目标函数" class="headerlink" title="一种促进神经对话模型多样性的目标函数"></a>一种促进神经对话模型多样性的目标函数</h2><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><ul><li>Seq2Seq模型：<br>  <img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3ej4gjdpj20cu09a41n.jpg" alt=""></li></ul><h3 id="论文写作动机"><a href="#论文写作动机" class="headerlink" title="论文写作动机"></a>论文写作动机</h3><p>越来越多的研究者开始探索数据驱动的对话生成方法。主要分为三派：</p><ul><li>基于短语的统计方法（Ritter 2011）: 传统的基于短语的翻译系统通过将源句分成多个块，然后逐句翻译来完成任务.</li><li>神经网络方法</li><li>Seq2Seq模型（Sordoni 2015）</li></ul><p>Seq2Seq神经网络模型生成的回复往往十分保守。(I don’t know)</p><h3 id="问题的解决思路"><a href="#问题的解决思路" class="headerlink" title="问题的解决思路"></a>问题的解决思路</h3><h4 id="最大互信息模型"><a href="#最大互信息模型" class="headerlink" title="最大互信息模型"></a>最大互信息模型</h4><ol><li><p>符号表示</p><ul><li>$S={s_1, s<em>2, …, S</em>{N_s}}$: 输入句子序列</li><li>$T={t_1, t<em>2, …, t</em>{N_s}, EOS}$: 目标句子序列，其中$EOS$表示句子结束。</li></ul></li><li><p>MMI评判标准</p><ol><li>MMI-antiLM:<br>对标准的目标函数：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3aq0qijej209x02i747.jpg" alt=""><br>进行了改进：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3da2abhvj20az01zglj.jpg" alt=""><br>在原始目标函数基础上添加了目标序列本身的概率$logp(T)$，$p(T)$就是一句话存在的概率，也就是一个模型，前面的lambda是惩罚因子，越大说明对语言模型惩罚力度越大。由于这里用的是减号，所以相当于在原本的目标上减去语言模型的概率，也就降低了“I don’t know”这类高频句子的出现概率。</li><li>MMI-bidi:<br>在标准的目标函数基础上添加$logp(S|T)$,也就是T的基础上产生S的概率，而且可以通过改变lambda的大小衡量二者的重要性。后者可以表示在响应输入模型时产生输入的概率，自然像“I don’t know”这种答案的概率会比较低，而这里使用的是相加，所以会降低这种相应的概率。<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3b3rrm7wj20ex04naad.jpg" alt=""></li></ol></li></ol><h4 id="MMI-antiLM"><a href="#MMI-antiLM" class="headerlink" title="MMI-antiLM"></a>MMI-antiLM</h4><p>如上所说，MMI-antiLM模型使用第一个目标函数，引入了$logp(T)$，如果lambda取值不合适可能会导致产生的响应不符合语言模型，所以在实际使用过程中会对其进行修正。由于解码过程中往往第一个单词或者前面几个单词是根据encode向量选择的，后面的单词更倾向于根据前面decode的单词和语言模型选择，而encode的信息影响较小。也就是说我们只需要对前面几个单词进行惩罚，后面的单词直接根据语言模型选择即可，这样就不会使整个句子不符合语言模型了。使用下式中的$U(T)$代替$p(T)$,式中$g(k)$表示要惩罚的句子长度：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3btiaol9j20dd08g0ta.jpg" alt=""><br>此外，我们还想要加入响应句子的长度这个因素，也作为模型相应的依据，所以将上面的目标函数修正为下式：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3bu955smj209q01la9y.jpg" alt=""></p><h4 id="MMI-bidi"><a href="#MMI-bidi" class="headerlink" title="MMI-bidi"></a>MMI-bidi</h4><p>MMI-bidi模型引入了$p(S|T)$项，这就需要先计算出完整的T序列再将其传入一个提前训练好的反向seq2seq模型中计算该项的值。但是考虑到S序列会产生无数个可能的T序列，我们不可能将每一个T都进行计算，所以这里引入beam-search只计算前200个序列T来代替。然后再计算两项和，进行得分重排。论文中也提到了这么做的缺点，比如最终的效果会依赖于选择的前N个序列的效果等等，但是实际的效果还是可以的。</p><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ol><li>Twitter Conversation Triple Dataset: 包含2300万个对话片段。</li><li>OpenSubtitiles Dataset</li></ol><h4 id="对比实验方法："><a href="#对比实验方法：" class="headerlink" title="对比实验方法："></a>对比实验方法：</h4><ol><li>SEQ2SEQ</li><li>SEQ2SEQ(greedy)</li><li>SMT(statistical machine translation): 2011</li><li>SMT + neural reranking: 2015</li></ol><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ol><li>BLEU</li><li>distinct-1</li><li>distinct-2</li></ol><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="实验结果评价"><a href="#实验结果评价" class="headerlink" title="实验结果评价"></a>实验结果评价</h4><p>最终在Twitter和OpenSubtitle两个数据集上面进行测试，效果展示BLEU得分都比标准的seq2seq模型要好。</p><ul><li>Twitter<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3coi0jx5j20qk06nq4x.jpg" alt=""></li><li>OpenSubtitle<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fv3cp1j5taj20cs05tgmj.jpg" alt=""></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。</p><!-- ### 批注版论文> 1. 黄色表示研究领域的问题> 2. 紫色表示论文叙述内容的重点> 3. 绿色表示该论文的解决思路> 4. 蓝色表示该论文的公式以及定义 --><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ol><li><a href="http://paperweek.ly/">http://paperweek.ly/</a></li><li><a href="https://scholar.google.com/">https://scholar.google.com/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>demo驱动学习：Image_Caption</title>
      <link href="/2018/08/26/Image-Caption-demo-by-tensorflow/"/>
      <url>/2018/08/26/Image-Caption-demo-by-tensorflow/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Introduction-to-demo"><a href="#Introduction-to-demo" class="headerlink" title="Introduction to demo"></a>Introduction to demo</h2><p>Source Code:<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb">image_captioning_with_attention</a></p><h3 id="Related-Papers"><a href="#Related-Papers" class="headerlink" title="Related Papers"></a>Related Papers</h3><p><a href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.</a></p><h3 id="Goal-of-this-end2end-model"><a href="#Goal-of-this-end2end-model" class="headerlink" title="Goal of this end2end model"></a>Goal of this end2end model</h3><ol><li>Generate a caption, such as “a surfer riding on a wave”, according to an image.<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fun2xxvjt8j20hs0buamq.jpg" alt=""></li><li>Use an attention based model that enables us to see which parts of the image the model focuses on as it generates a caption.<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fun2yatwwhj20zz0ehk1c.jpg" alt=""></li></ol><h3 id="Dateset"><a href="#Dateset" class="headerlink" title="Dateset"></a>Dateset</h3><p><strong>MS-COCO</strong>:This dataset contains &gt;82,000 images, each of which has been annotated with at least 5 different captions.</p><h2 id="Frame-work-of-demo"><a href="#Frame-work-of-demo" class="headerlink" title="Frame work of demo:"></a>Frame work of demo:</h2><ol><li>Download and prepare the MS-COCO dataset</li><li>Limit the size of the training set for faster training</li><li><p>Preprocess the images using InceptionV3: extract features from the last convolutional layer.</p><ol><li>Initialize InceptionV3 and load the pretrained Imagenet weights</li><li>Caching the features extracted from InceptionV3</li></ol></li><li><p>Preprocess and tokenize the captions</p><ol><li>First, tokenize the captions will give us a vocabulary of all the unique words in the data (e.g., “surfing”, “football”, etc).</li><li>Next, limit the vocabulary size to the top 5,000 words to save memory. We’ll replace all other words with the token “UNK” (for unknown).</li><li>Finally, we create a word –&gt; index mapping and vice-versa.</li><li>We will then pad all sequences to the be same length as the longest one.</li></ol></li><li><p>create a tf.data dataset to use for training our model.</p></li></ol><ol><li><p>Model</p><ol><li>extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).</li><li>This vector is then passed through the CNN Encoder(which consists of a single Fully connected layer).</li><li>The RNN(here GRU) attends over the image to predict the next word.</li></ol></li><li><p>Training</p><ol><li>We extract the features stored in the respective .npy files and then pass those features through the encoder.</li><li>The encoder output, hidden state(initialized to 0) and the decoder input (which is the start token) is passed to the decoder.</li><li>The decoder returns the predictions and the decoder hidden state.</li><li>The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.</li><li>Use teacher forcing to decide the next input to the decoder.</li><li>Teacher forcing is the technique where the target word is passed as the next input to the decoder.</li><li>The final step is to calculate the gradients and apply it to the optimizer and backpropagate.</li></ol></li><li><p>Caption</p><ol><li>The evaluate function is similar to the training loop, except we don’t use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.</li><li>Stop predicting when the model predicts the end token.</li><li>And store the attention weights for every time step.</li></ol></li></ol><h2 id="Problems-undesirable"><a href="#Problems-undesirable" class="headerlink" title="Problems undesirable"></a>Problems undesirable</h2><h3 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h3><ul><li>The code requires TensorFlow version <strong>&gt;=1.9</strong>. 1.10.0 is better.</li><li><code>cudatoolkit</code></li></ul><h3 id="GPU-lose-connect"><a href="#GPU-lose-connect" class="headerlink" title="GPU lose connect"></a>GPU lose connect</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Demo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> imageCaption </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual-Question-Learning(VQA)学习笔记</title>
      <link href="/2018/08/23/Visual-Question-Learning/"/>
      <url>/2018/08/23/Visual-Question-Learning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：1. 直接从论文pdf中获取2. 从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Visual Question Answering: Datasets, Algorithms, and Future Challenges</p></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1610.01465.pdf">https://arxiv.org/pdf/1610.01465.pdf</a></p><!-- Ex: https://arxiv.org/abs/1606.01541  --></li><li><p>论文源码</p><ul><li>None</li></ul></li><li><p>关于作者</p><ul><li>Kushal Kafle</li><li>Christopher Kanan</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><!-- Ex: 论文摘要的中文翻译最近对话生成的神经模型为会话代理生成响应提供了很大的希望，但往往是短视的，一次预测一个话语而忽略它们对未来结果的影响。对未来的对话方向进行建模对于产生连贯，有趣的对话至关重要，这种对话需要传统的NLP对话模式借鉴强化学习。在本文中，我们将展示如何整合这些目标，应用深度强化学习来模拟聊天机器人对话中的未来奖励。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性，连贯性和易于回答（与前瞻性功能相关）。我们在多样性，长度以及人类评判方面评估我们的模型，表明所提出的算法产生了更多的交互式响应，并设法在对话模拟中促进更持久的对话。这项工作标志着基于对话的长期成功学习神经对话模型的第一步。 --><p> 视觉问答(Visual Question answering, VQA)是近年来计算机视觉和自然语言处理领域的一个热点问题。在VQA中，一个算法需要回答关于图像的基于文本的问题。自2014年发布第一个VQA数据集以来，已经发布了更多的数据集，并提出了许多算法。在这篇综述中，我们从问题的形成、现有数据集、评估指标和算法的角度，批判性地研究了VQA的当前状态。特别地，我们讨论了当前数据集在正确训练和评估VQA算法方面的局限性。然后，我们详尽地回顾了VQA的现有算法。最后，我们讨论了未来VQA和图像理解研究的可能方向。</p><h2 id="视觉问答：数据集，算法和未来的挑战"><a href="#视觉问答：数据集，算法和未来的挑战" class="headerlink" title="视觉问答：数据集，算法和未来的挑战"></a>视觉问答：数据集，算法和未来的挑战</h2><!-- Ex: ## 强化学习在对话生成领域的应用 --><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="VQA的研究价值"><a href="#VQA的研究价值" class="headerlink" title="VQA的研究价值"></a>VQA的研究价值</h4><ol><li><p>大部分计算机视觉任务不能完整的理解图像<br>图像分类、物体检测、动作识别等任务很难获取到物体的<strong>空间位置信息</strong>并且根据它们的属性和关系进行<strong>推理</strong>。</p></li><li><p>人类对<strong>Grand Unified Theory</strong>的痴迷追求</p><ul><li>目标识别任务：图像里面有什么？[分类]</li><li>目标检测任务：图像里面有猫吗？[拉框]</li><li>属性分类任务：图像里面的猫是什么颜色的？</li><li>场景分类：图像是在室内吗？</li><li>计数任务：图像里面有多少猫？<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvah1uwsvij208c04ogn4.jpg" alt=""></li></ul></li><li><p>通过视觉图灵测试：</p><ul><li>基准问题测试</li><li>建立评价指标 </li></ul></li></ol><h3 id="VQA的数据集"><a href="#VQA的数据集" class="headerlink" title="VQA的数据集"></a>VQA的数据集</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvajh4e1wbj20tz09eacb.jpg" alt=""></p><h3 id="VQA的评价标准"><a href="#VQA的评价标准" class="headerlink" title="VQA的评价标准"></a>VQA的评价标准</h3><ul><li>Open-ended(OE): 开放式的</li><li>Multiple Choice(MC): 选择式的</li></ul><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvb6r1njlkj20kp0dmjuj.jpg" alt=""></p><h4 id="流行的评价标准"><a href="#流行的评价标准" class="headerlink" title="流行的评价标准"></a>流行的评价标准</h4><p>选择式任务的评价标准直接使用正确率即可。但是开放式任务的评价标准呢？</p><ol><li>Simple accuracy:<ol><li>Q: What animals are in the photo<br>若<code>dogs</code>是正确答案，那么<code>dog</code>和<code>zebra</code>的惩罚竟然是一样的</li><li>Q: What is in the tree<br>若<code>bald eagle</code>是正确答案，<code>eagle</code>或是<code>bird</code>  与  <code>yes</code>的惩罚竟然也是一样的</li></ol></li><li><p>Wu-Palmer Similarity</p><ol><li>语义相似度<br><code>Black</code>、<code>White</code>两个单词的<code>WUPS score</code>是0.91。所以这可能会给错误答案一个相当高的分数。</li><li>只可以评价单词，句子不可使用</li></ol></li><li><p>$Accuracy_{VQA}=min(\frac{n}{3}, 1)$<br>同样是语义相似度，大致正确就ok: 人为构造一个答案集合，$n$是算法和人类拥有的相同的答案数量。</p></li></ol><h3 id="VQA的算法"><a href="#VQA的算法" class="headerlink" title="VQA的算法"></a>VQA的算法</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbaa7e7d4j20qr098n45.jpg" alt=""><br>存在的算法大致结构均包括：</p><ol><li>提取图像特征</li><li>提取问题特征</li><li>利用特征产生结果的算法</li></ol><h4 id="Baseline和模型性能"><a href="#Baseline和模型性能" class="headerlink" title="Baseline和模型性能"></a>Baseline和模型性能</h4><ol><li>瞎猜最有可能的答案。“yes”/“no”</li><li>MLP(multi-layer percepton)</li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbapjz9zkj20ky0lradq.jpg" alt=""></p><h4 id="模型架构一览"><a href="#模型架构一览" class="headerlink" title="模型架构一览"></a>模型架构一览</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbasmehclj20qg0l278r.jpg" alt=""></p><ol><li>基于贝叶斯和问题导向的模型</li><li>基于注意力机制的模型<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbb0ttq7cj20r60dj7bs.jpg" alt=""></li><li>非线性池化方法</li></ol><ul><li>MULTI-WORLD: A multi-world approach to question answering about real- world scenes based on uncertain input, NIPS2014</li><li>ASK-NEURon: Ask your neurons: A neural-based ap- proach to answering questions about images, ICCV2015</li><li>ENSEMSBLE: Exploring models and data for image question answering, NIPS2015</li><li>LSTM Q+I: VQA: Visual question answering, ICCV2015</li><li>iBOWIMG: Simple baseline for visual question answering, arxiv</li><li>DPPNET: Image question answering using convolutional neural network with dynamic parameter prediction, CVPR2016</li><li>SMem: Ask, attend and answer: Exploring question-guided spatial attention for visual question answering, ECCV2016</li><li>SAN: Stacked attention networks for image question answering, CVPR2016</li><li>NMN: Deep compositional question answering with neural module networks, CVPR2016</li><li>FDA: A focused dynamic attention model for visual question answering, arxiv2016</li><li>HYBRID: Answer-type prediction for visual question answering, CVPR2016</li><li>DMN+: Dynamic memory networks for visual and textual question answering, ICML2016</li><li>MRN: Multimodal residual learning for visual qa, NIPS2016</li><li>HieCoAtten: Hierarchical question-image co-attention for visual question answering, NIPS2016</li><li>RAU_ResNet: Training recurrent answering units with joint loss minimization for VQA, arxiv2016</li><li>DAN: Dual attention networks for multimodal reasoning and matching, arxiv2016</li><li>MCB+Att: Multi-modal compact bilinear pooling for visual question answering and visual grounding, EMNLP2016</li><li>MLB: Hadamard product for low-rank bilinear pooling, arxiv2016</li><li>AMA: Ask me anything: Free-form visual question answering based on knowledge from external sources, CVPR2016</li><li>MCB-ensemble: Multi-modal compact bilinear pooling for visual question answering and visual grounding, EMNLP2016</li></ul><h3 id="VQA仍然存在很多问题"><a href="#VQA仍然存在很多问题" class="headerlink" title="VQA仍然存在很多问题"></a>VQA仍然存在很多问题</h3><p>虽然VQA已经取得了长足的进步，但是现有的算法仍然距离人类有巨大的差距。<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbbxjxinej20n10gvmzr.jpg" alt=""></p><p>现有问题有：</p><ol><li>现有的VQA系统太依赖于问题而不是图片内容，并且语言的偏差会严重影响VQA系统性能。<ol><li>只需要问题或者图片就能猜出来答案，甚至一个差的数据集(通常包含具有偏差的问题)会降低VQA系统的性能。也即越具体的问题越好！[do-&gt;play-&gt;sport play]</li></ol></li><li>算法性能的提升是否真的来自于注意力机制？<ol><li>通过多全局图片特征（预训练的VGG-19,ResNet-101）也能达到很好的效果。</li><li>注意力机制有时候会误导VQA系统。</li></ol></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>可以回答任意关于图片的问题的算法将会是人工智能的里程碑。</p><h4 id="研究方向潜力股"><a href="#研究方向潜力股" class="headerlink" title="研究方向潜力股"></a>研究方向潜力股</h4><ol><li>更<strong>大</strong>更<strong>无偏</strong>更<strong>丰富</strong>的数据集:每个问题权重不应该一样；问题的质量应该更高；答案不应该是二元的；多选题应当被淘汰</li><li>更加巧妙地模型评估方式</li><li>重点：可以对图片内容进行<strong>推理</strong>的算法！<ol><li>常识推理。</li><li>空间位置。</li><li>根据不同粒度回复问题。</li></ol></li></ol><!-- TODO: ### 批注版论文 > 1. 黄色表示研究领域的问题> 2. 紫色表示论文叙述内容的重点> 3. 绿色表示该论文的解决思路> 4. 蓝色表示该论文的公式以及定义 --><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><!--Ex:1. https://www.paperweekly.site/papers/notes/2212. https://scholar.google.com/-->]]></content>
      
      
      <categories>
          
          <category> VQA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summarize </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习模块文档备忘录</title>
      <link href="/2018/08/23/colab-tensorflow-usage/"/>
      <url>/2018/08/23/colab-tensorflow-usage/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Colab-study-notes"><a href="#Colab-study-notes" class="headerlink" title="Colab study notes"></a>Colab study notes</h2><h3 id="Install-commonly-used-packages"><a href="#Install-commonly-used-packages" class="headerlink" title="Install commonly used packages"></a>Install commonly used packages</h3><p>Although Colab has already installed some packages such as Tensorflow Matplotlib .etc, there are lots of commonly ised packages:</p><ul><li>Keras:<code>pip install keras</code></li><li>OpenCV:<code>!apt-get -qq install -y libsm6 libxext6 &amp;&amp; pip install -q -U opencv-python</code></li><li>Pytorch:<code>!pip install -q http://download.pytorch.org/whl/cu75/torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl torchvision</code></li><li>tqdm:<code>!pip install tqdm</code><h3 id="Authorized-to-log-in"><a href="#Authorized-to-log-in" class="headerlink" title="Authorized to log in"></a>Authorized to log in</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 PyDrive 操作库，该操作每个 notebook 只需要执行一次</span></span><br><span class="line">!pip install -U -q PyDrive</span><br><span class="line"><span class="keyword">from</span> pydrive.auth <span class="keyword">import</span> GoogleAuth</span><br><span class="line"><span class="keyword">from</span> pydrive.drive <span class="keyword">import</span> GoogleDrive</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> auth</span><br><span class="line"><span class="keyword">from</span> oauth2client.client <span class="keyword">import</span> GoogleCredentials</span><br><span class="line"></span><br><span class="line"><span class="comment"># 授权登录，仅第一次的时候会鉴权</span></span><br><span class="line">auth.authenticate_user()</span><br><span class="line">gauth = GoogleAuth()</span><br><span class="line">gauth.credentials = GoogleCredentials.get_application_default()</span><br><span class="line">drive = GoogleDrive(gauth)</span><br></pre></td></tr></table></figure><h3 id="File-IO"><a href="#File-IO" class="headerlink" title="File IO"></a>File IO</h3><h4 id="Read-file-from-Google-Drive"><a href="#Read-file-from-Google-Drive" class="headerlink" title="Read file from Google Drive"></a>Read file from Google Drive</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the file by id</span></span><br><span class="line">downloaded = drive.CreateFile(&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;yourfileID&#x27;</span>&#125;) <span class="comment"># replace the id with id of file you want to access</span></span><br><span class="line"><span class="comment"># Download file to colab</span></span><br><span class="line">downloaded.GetContentFile(<span class="string">&#x27;yourfileName&#x27;</span>)  </span><br><span class="line"><span class="comment"># Read file as panda dataframe</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">xyz = pd.read_csv(<span class="string">&#x27;yourfileName&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="Write-file-to-Google-Drive"><a href="#Write-file-to-Google-Drive" class="headerlink" title="Write file to Google Drive"></a>Write file to Google Drive</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Content file as Cache</span></span><br><span class="line">xyz.to_csv(<span class="string">&#x27;over.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># Create &amp; upload a text file.</span></span><br><span class="line">uploaded = drive.CreateFile(&#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;OK.csv&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># You will have a file named &#x27;OK.csv&#x27; which has content of &#x27;over.csv&#x27;</span></span><br><span class="line">uploaded.SetContentFile(<span class="string">&#x27;over.csv&#x27;</span>)</span><br><span class="line">uploaded.Upload()</span><br><span class="line"><span class="comment"># checkout your upload file&#x27;s ID</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Uploaded file with ID &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(uploaded.get(<span class="string">&#x27;id&#x27;</span>)))</span><br></pre></td></tr></table></figure></li></ul><h2 id="Tensorflow-commonly-used"><a href="#Tensorflow-commonly-used" class="headerlink" title="Tensorflow commonly used"></a>Tensorflow commonly used</h2><h3 id="tf"><a href="#tf" class="headerlink" title="tf"></a>tf</h3><h4 id="cast"><a href="#cast" class="headerlink" title="cast"></a>cast</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cast a tensor[x] to a new type[dtype]</span></span><br><span class="line">tf.cast(</span><br><span class="line">    x,</span><br><span class="line">    dtype,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="expand-dims"><a href="#expand-dims" class="headerlink" title="expand_dims"></a>expand_dims</h4><p>Inserts a dimension of 1 into a tensor’s shape.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(</span><br><span class="line">    <span class="built_in">input</span>,</span><br><span class="line">    axis=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># &#x27;t&#x27; is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, -<span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;t2&#x27; is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure></p><h4 id="read-file"><a href="#read-file" class="headerlink" title="read_file"></a>read_file</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.read_file(</span><br><span class="line">    filename,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="device"><a href="#device" class="headerlink" title="device"></a>device</h4><ol><li>manual mode<ul><li><code>with tf.device(&#39;/cpu:0&#39;)</code>: cpu</li><li><code>with tf.device(&#39;/gpu:0&#39;)</code>or<code>with tf.device(&#39;/device:GPU:0&#39;)</code>   </li></ul></li><li>GPU config<ul><li><code>import os</code></li><li><code>os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;]=&#39;0, 1&#39;</code></li></ul></li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.device(device_name_or_function)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&#x27;/gpu:0&#x27;</span>):</span><br></pre></td></tr></table></figure><h4 id="random-normal"><a href="#random-normal" class="headerlink" title="random_normal"></a>random_normal</h4><p>Outputs random values from a normal distribution.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal(</span><br><span class="line">    shape,</span><br><span class="line">    mean=<span class="number">0.0</span>,</span><br><span class="line">    stddev=<span class="number">1.0</span>,</span><br><span class="line">    dtype=tf.float32,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">tf.random_normal((<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></p><h4 id="ConfigProto"><a href="#ConfigProto" class="headerlink" title="ConfigProto"></a>ConfigProto</h4><p>allowing GPU memory growth by the process.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br></pre></td></tr></table></figure></p><h4 id="reduce-sum-reduce-mean"><a href="#reduce-sum-reduce-mean" class="headerlink" title="reduce_sum/reduce_mean"></a>reduce_sum/reduce_mean</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_sum(</span><br><span class="line">    input_tensor,</span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    keepdims=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reduction_indices=<span class="literal">None</span>,</span><br><span class="line">    keep_dims=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Returns: The reduced tensor</p><h3 id="tf-app"><a href="#tf-app" class="headerlink" title="tf.app"></a>tf.app</h3><p>Generic entry point</p><h4 id="flag-module"><a href="#flag-module" class="headerlink" title="flag module"></a><code>flag</code> module</h4><p>process command line parameters. Just like <code>argparse</code></p><h4 id="run"><a href="#run" class="headerlink" title="run(...)"></a><code>run(...)</code></h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run program with an optional &#x27;main&#x27; function and &#x27;argv&#x27; list</span></span><br><span class="line">tf.app.run(</span><br><span class="line">    main=<span class="literal">None</span>,</span><br><span class="line">    argv=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-contrib"><a href="#tf-contrib" class="headerlink" title="tf.contrib"></a>tf.contrib</h3><h4 id="eager"><a href="#eager" class="headerlink" title="eager"></a>eager</h4><ul><li>Saver: A tf.train.Saver adapter for use when eager execution is enabled.</li></ul><h3 id="tf-data"><a href="#tf-data" class="headerlink" title="tf.data"></a>tf.data</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># usage example</span></span><br><span class="line">tf.data.Dataset.from_tensor_slices(encode_train).<span class="built_in">map</span>(load_image).batch(<span class="number">16</span>)</span><br></pre></td></tr></table></figure><ul><li>from_tensor_slices(tensors): Creates a Dataset whose elements are slices of the given tensors. Returns: A dataset</li><li>map(map_func,num_parallel_calls=None) </li><li>batch(batch_size,drop_remainder=False)</li><li>prefetch(buffersize): Creates a Dataset that prefetches elements from this dataset.</li></ul><h3 id="tf-image"><a href="#tf-image" class="headerlink" title="tf.image"></a>tf.image</h3><h4 id="decode-jpeg"><a href="#decode-jpeg" class="headerlink" title="decode_jpeg"></a>decode_jpeg</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_jpeg(</span><br><span class="line">    contents,</span><br><span class="line">    channels=<span class="number">0</span>, <span class="comment"># 3: output an RGB image.</span></span><br><span class="line">    ratio=<span class="number">1</span>,</span><br><span class="line">    fancy_upscaling=<span class="literal">True</span>,</span><br><span class="line">    try_recover_truncated=<span class="literal">False</span>,</span><br><span class="line">    acceptable_fraction=<span class="number">1</span>,</span><br><span class="line">    dct_method=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="resize-images"><a href="#resize-images" class="headerlink" title="resize_images"></a>resize_images</h4><h3 id="tf-layers"><a href="#tf-layers" class="headerlink" title="tf.layers"></a>tf.layers</h3><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">    data_format=<span class="string">&#x27;channels_last&#x27;</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">random_image_gpu = tf.random_normal((<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">3</span>))</span><br><span class="line">net_gpu = tf.layers.conv2d(random_image_gpu, <span class="number">32</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><p>Returns: Output tensor.</p><h3 id="tf-test"><a href="#tf-test" class="headerlink" title="tf.test"></a>tf.test</h3><ul><li>gpu_device_name(): Check out GPU whether can be found.</li></ul><h3 id="tf-train"><a href="#tf-train" class="headerlink" title="tf.train"></a>tf.train</h3><ul><li>Saver</li></ul><h2 id="scikit-learn-sklearn"><a href="#scikit-learn-sklearn" class="headerlink" title="scikit-learn(sklearn)"></a>scikit-learn(sklearn)</h2><h3 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h3><ul><li>shuffle(*array):Shuffle arrays or sparse matrices in a consistent way<h3 id="model-selection"><a href="#model-selection" class="headerlink" title="model_selection"></a>model_selection</h3></li><li>train_test_split(*array): Split arrays or matrices into random train and test subsets<ul><li>Parameters<ul><li>arrays_data</li><li>arrays_label</li><li>test_size</li><li>random_state</li></ul></li></ul></li></ul><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>A high-API to build and train deep learning models.</p><h3 id="applications"><a href="#applications" class="headerlink" title="applications"></a>applications</h3><h4 id="inception-v3"><a href="#inception-v3" class="headerlink" title="inception_v3"></a>inception_v3</h4><ul><li>InceptionV3(…): Instantiates the Inception v3 architecture.  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.applications.InceptionV3(</span><br><span class="line">include_top=<span class="literal">True</span>, <span class="comment"># whether to include the fully-connected layer at the top of the network.</span></span><br><span class="line">weights=<span class="string">&#x27;imagenet&#x27;</span>,</span><br><span class="line">input_tensor=<span class="literal">None</span>,</span><br><span class="line">input_shape=<span class="literal">None</span>,</span><br><span class="line">pooling=<span class="literal">None</span>,</span><br><span class="line">classes=<span class="number">1000</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li>decode_predictions(…): Decodes the prediction of an ImageNet model.</li><li>preprocess_input(…): Preprocesses a numpy array encoding a batch of images.</li></ul><h3 id="backend"><a href="#backend" class="headerlink" title="backend"></a>backend</h3><h3 id="layers"><a href="#layers" class="headerlink" title="layers"></a>layers</h3><ul><li>Dense: regular densely-connected NN layer<ul><li>Arguments:<ul><li>units:</li><li>input_shape: </li></ul></li></ul></li><li>GRU/CuDNNGRU<ul><li>Arguments:<ul><li>units: Positive integer, dimensionality of the output space.</li><li>return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.</li><li>return_state: Boolean. Whether to return the last state in addition to the output.</li><li>recurrent_activation: Default is <code>hard sigmoid</code>.<code>sigmoid</code> is avaliable. <ul><li>hard sigmoid: a combination of sigmoid and relu</li></ul></li><li>recurrent_initializer: Defaul is <code>orthogonal</code>.</li></ul></li></ul></li></ul><h3 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h3><h4 id="image"><a href="#image" class="headerlink" title="image"></a>image</h4><h4 id="sequence"><a href="#sequence" class="headerlink" title="sequence"></a>sequence</h4><ul><li>pad_sequences:<ul><li>Arguments:<ul><li>sequences:  List of lists, where each element is a sequence.</li><li>padding: String, ‘pre’ or ‘post’: pad either before or after each sequence.<h4 id="text"><a href="#text" class="headerlink" title="text"></a>text</h4></li></ul></li></ul></li><li>hashing_trick</li><li>one_hot</li><li>text_to_word_sequence</li><li>Tokenizer(vetorize a text corpus)<ul><li>Arguments:<ul><li>num_words: the maximum number of words to keep, based on <strong>word frequency</strong>. </li><li>oov_token: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls</li><li>filters: a string where each element is a character that will be filtered from the texts. </li></ul></li><li>Methods:<ul><li>fit_on_texts: Updates internal vocabulary based on a list of texts.</li><li>texts_to_sequences: Transforms each text in texts in a sequence of integers.</li><li></li></ul></li></ul></li></ul><h3 id="utils-1"><a href="#utils-1" class="headerlink" title="utils"></a>utils</h3><ul><li>get_file: Downloads a file from a URL if it not already in the cache.</li></ul><blockquote><p>Reference:</p><ol><li><a href="https://segmentfault.com/a/1190000012731724">https://segmentfault.com/a/1190000012731724</a></li><li><a href="https://tensorflow.google.cn/api_docs/">https://tensorflow.google.cn/api_docs/</a></li><li><a href="https://www.jianshu.com/p/d7283bc427b1">https://www.jianshu.com/p/d7283bc427b1</a></li><li><a href="http://scikit-learn.org/stable/modules">http://scikit-learn.org/stable/modules</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> DeepLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> colab </tag>
            
            <tag> tensorflow </tag>
            
            <tag> sklearn </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zero-shot Learning学习笔记</title>
      <link href="/2018/08/22/zero-shot-learning/"/>
      <url>/2018/08/22/zero-shot-learning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Graph-Convolutional-Networks"><a href="#Graph-Convolutional-Networks" class="headerlink" title="Graph Convolutional Networks"></a>Graph Convolutional Networks</h2><p><img src="http://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png" alt="Multi-layer Graph Convolutional Network (GCN) with first-order filters."></p><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Generalizing well-stablished neural models like RNNs or CNNs to work on arbitrarily structured graphs is a challenging problem.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Zero-shot Learning is a concept from Transfer-Learning. In traditional machine learning method, Generalization is difficult since big data and time-consuming training are needed in general. Therefore more and more researchers pay attention to <strong>Zero-shot Learning</strong>/<strong>One-shot Learning</strong>/<strong>Few-shot Learning</strong></p><h3 id="types-of-Learning"><a href="#types-of-Learning" class="headerlink" title="types of Learning"></a>types of Learning</h3><h4 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h4><p>A model can create a map $X\rightarrowY$ automatically for the categories which have not appeared in a training set.</p><h4 id="One-shot-Learning"><a href="#One-shot-Learning" class="headerlink" title="One-shot Learning"></a>One-shot Learning</h4><p>One-shot learning is an object categorization problem in computer vision. Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training images.</p><h4 id="Few-shot-Leaning"><a href="#Few-shot-Leaning" class="headerlink" title="Few-shot Leaning"></a>Few-shot Leaning</h4><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><h3 id="DeVise-A-Deep-Visual-Semantic-Embedding-Model"><a href="#DeVise-A-Deep-Visual-Semantic-Embedding-Model" class="headerlink" title="DeVise: A Deep Visual-Semantic Embedding Model"></a>DeVise: A Deep Visual-Semantic Embedding Model</h3><h4 id="Core-idea"><a href="#Core-idea" class="headerlink" title="Core idea"></a>Core idea</h4><p>Combine <strong>feature vector</strong> from Computer Vision and <strong>semantic vector</strong> from NLP to realize zero-shot learning.</p><h3 id="Zero-shot-Learning-by-Convex-Combination-of-Semantic-Embeddings"><a href="#Zero-shot-Learning-by-Convex-Combination-of-Semantic-Embeddings" class="headerlink" title="Zero-shot Learning by Convex Combination of Semantic Embeddings"></a>Zero-shot Learning by Convex Combination of Semantic Embeddings</h3><h3 id="Objects2action-Classifying-and-localizing-actions-without-any-video-example"><a href="#Objects2action-Classifying-and-localizing-actions-without-any-video-example" class="headerlink" title="Objects2action: Classifying and localizing actions without any video example"></a>Objects2action: Classifying and localizing actions without any video example</h3><blockquote><p>Reference:</p><ol><li><a href="https://en.wikipedia.org/wiki/One-shot_learning">https://en.wikipedia.org/wiki/One-shot_learning</a></li><li><a href="https://blog.csdn.net/jningwei/article/details/79235019">https://blog.csdn.net/jningwei/article/details/79235019</a></li><li><a href="http://tkipf.github.io/graph-convolutional-networks/">http://tkipf.github.io/graph-convolutional-networks/</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Zero-shot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> graphConvolutionalNetwork </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记：Deep Reinforcement Learning for Dialogue Generation</title>
      <link href="/2018/08/18/paper-note-deep-reinfocement-learning-for-Dialogue-Generation/"/>
      <url>/2018/08/18/paper-note-deep-reinfocement-learning-for-Dialogue-Generation/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li>论文名：Deep Reinforcement Learning for Dialogue Generation</li><li>论文链接：<a href="https://arxiv.org/abs/1606.01541">https://arxiv.org/abs/1606.01541</a></li><li>论文源码：<ul><li><a href="https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow">https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow</a></li><li><a href="https://github.com/agsarthak/Goal-oriented-Dialogue-Systems">https://github.com/agsarthak/Goal-oriented-Dialogue-Systems</a></li><li><a href="https://github.com/jiweil/Neural-Dialogue-Generation">https://github.com/jiweil/Neural-Dialogue-Generation</a></li></ul></li><li>关于作者：<ul><li>Jiwei Li：斯坦福大学博士毕业生，截至发稿被引次数：2156</li><li>Will Monroe：斯坦福大学博士在读，截至发稿被引次数：562</li><li>Alan Ritter：俄亥俄州立大学教授，截至发稿被引次数：4608</li><li>Michel Galley：微软高级研究员，截至发稿被引次数：4529</li><li>Jianfeng Gao：雷德蒙德微软研究院（总部），截至发稿被引次数：11944</li><li>Dan Jurafsky：，斯坦福大学教授，截至发稿被引次数：32973</li></ul></li><li>关于笔记作者：<ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。</li></ul></li></ol><h2 id="论文推荐理由与摘要"><a href="#论文推荐理由与摘要" class="headerlink" title="论文推荐理由与摘要"></a>论文推荐理由与摘要</h2><p>最近对话生成的神经模型为会话Agent生成响应提供了很大的帮助，但其结果往往是短视的：一次预测一个话语会忽略它们对未来结果的影响。对未来的对话方向进行建模，这对于产生连贯，有趣的对话至关重要。这种对话需要在传统的NLP对话模式的技术上使用强化学习。在本文中，我们将展示如何整合这些目标，应用深度强化学习来模拟聊天机器人对话中的未来奖励。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性，连贯性和易于回答（与前瞻性功能相关）。我们在多样性，长度以及人类评判方面评估我们的模型，表明所提出的算法产生了更多的交互式响应，并设法在对话模拟中促进更持久的对话。这项工作标志着基于对话的长期成功学习神经对话模型的第一步。</p><h2 id="对话系统的缺点不再致命：深度强化学习带来的曙光"><a href="#对话系统的缺点不再致命：深度强化学习带来的曙光" class="headerlink" title="对话系统的缺点不再致命：深度强化学习带来的曙光"></a>对话系统的缺点不再致命：深度强化学习带来的曙光</h2><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="论文的写作动机"><a href="#论文的写作动机" class="headerlink" title="论文的写作动机"></a>论文的写作动机</h4><blockquote><p>Seq2Seq Model：将一个领域的序列(如英文句子)转换为另一个领域（如中文句子）的序列。在论文中是一种神经生成模型，它能最大限度地根据在前面的对话，生成回复的概率。</p></blockquote><p>Seq2Seq模型用于对话生成系统虽然已经取得一些成功，但是还存在两个问题：</p><ol><li><p>SEQ2SEQ模型是通过使用最大似然估计(MLE)目标函数,预测给定上下文中的下一个会话来训练的。SEQ2SEQ模型倾向于生成高度通用的响应，例如“我不知道”等。然而，“我不知道”显然不是一个好的回复。</p></li><li><p>基于最大似然估计的Seq2Seq模型无法结局重复的问题，因此对话系统通常会陷入重复性应答的无限循环之中。</p></li></ol><p>以上问题如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf3bujstcj20b509zgmm.jpg" alt=""></p><h4 id="论文思路的亮点"><a href="#论文思路的亮点" class="headerlink" title="论文思路的亮点"></a>论文思路的亮点</h4><p>首先提出对话系统应当具备的两种能力：</p><ol><li>结合开发人员定义的奖励函数，更好地模拟聊天机器人开发的真正目标。</li><li>在正在进行的对话中,对生成应答的长期影响进行建模。</li></ol><p>紧接着提出利用强化学习的生成方法来改进对话系统：</p><blockquote><p>encoder-decoder architecture:一种标准的神经机器翻译方法，用于解决seq2seq问题的递归神经网络。<br>Policy Gradient 策略梯度:</p></blockquote><p>该模型以encoder-decoder结构为骨干，模拟两个Agent之间的对话，在学习最大化预期回报的同时，探索可能的活动空间(回复的可能性)。Agent通过从正在进行的对话中优化长期Reward函数来学习策略。学习方式则使用策略梯度而非最大似然。</p><p>改进后的模型如下图所示：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf47elgdkj20af09yjse.jpg" alt=""></p><h3 id="论文模型的细节"><a href="#论文模型的细节" class="headerlink" title="论文模型的细节"></a>论文模型的细节</h3><h4 id="符号以及定义"><a href="#符号以及定义" class="headerlink" title="符号以及定义"></a>符号以及定义</h4><ol><li>$p$: 第一个Agent生成的句子</li><li>$q$: 第二个Agent生成的句子</li><li>$p_1,q_1,p_2,q_2,…,p_i,q_i$: 一段对话，或者称之为上下文.</li><li>$[p_i,q_i]$: Agent所处的状态，也即Agent的前两轮对话。</li><li>$p<em>{RL}(p</em>{i+1}|p_i,q_i)$: 策略(policy),论文中以LSTM encoder-decoder的形式出现。</li><li>$r$: 每个动作（每轮对话）的奖励函数。</li><li>$\mathbb{S}$: 人工构建的”迟钝回复”，例如“我不知道你在说什么”。</li><li>$N<em>{\mathbb{S}}$: 表示$N</em>{\mathbb{S}}$的基数</li><li>$N_{s}$: 表示“迟钝回复”$s$的符号数量。</li><li>$p_{seq2seq}$: 表示SEQ2SEQ模型的似然输出</li><li>$h_{p<em>i}$和$h</em>{p_{i+1}}$: 从encoder中获取的，代表Agent两轮连续对话$p<em>i$和$p</em>{i+1}$的表示。</li></ol><h4 id="Reward的定义和作用："><a href="#Reward的定义和作用：" class="headerlink" title="Reward的定义和作用："></a>Reward的定义和作用：</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fug2dvizhhj209o01wdfq.jpg" alt=""></p><blockquote><p>$N<em>{\mathbb{S}}$:表示$N</em>{\mathbb{S}}$的基数<br>$N<em>{s}$: 表示“迟钝回复”$s$的符号数量<br>$p</em>{seq2seq}$: 表示SEQ2SEQ模型的似然输出</p><ul><li>$r_1$是为了降低回复的困难程度。这个奖励函数的灵感来自于前瞻性函数：计算当模型产生的响应$a$作为输入时模型输出$s$的概率，在对$\mathbb{S}$集合中的每一句话进行求和。因为$p_seq2seq}可定小于1，所以log项大于零，则r1小于零。通过r1的奖励机制，模型最终产生的action会慢慢的远离dull response，而且也会一定程度上估计到下一个人的回复，让对方可以更容易回复。</li></ul></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fug5a6naokj20b302d747.jpg" alt=""></p><blockquote><p>$h_{p<em>i}$和$h</em>{p_{i+1}}$: 从encoder中获取的，代表Agent两轮连续对话$p<em>i$和$p</em>{i+1}$的表示。</p><ul><li>$r_2$是为了增加信息流的丰富程度，避免两次回复之间相似程度很高的情况。所以r2使用余弦相似度来计算两个句子之间的语义相似程度，很容易发现r2也是一个小于零的数，用来惩罚相似的句子。</li></ul></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fug5av9bkgj20bp028mx4.jpg" alt=""></p><blockquote><p>$p_{seq2seq}(a|p_i, q_i)$: 表示在给定对话上文$[p_i,q<em>i]$的情况下生成回复a的概率<br>$p^{backward}</em>{seq2seq}(q_i|a)$: 表示基于响应$a$来生成之前的对话$q_i$的概率。</p><ul><li>$r_3$是为了增强语义连贯性，避免模型只产生那些高reward的响应，而丧失回答的充分性和连贯性。为了解决这个问题模型采用互信息来实现。反向的seq2seq是使用source和target反过来训练的另外一个模型，这样做的目的是为了提高q和a之间的相互关系，让对话更具有可持续性。可以看出来，$r_3$的两项都是正值。</li></ul></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fug5beurspj209y01dt8k.jpg" alt=""></p><ul><li>最终的奖励函数式对$r_1，r_2，r_3$进行加权求和,论文中设定$\lambda_1=0.25, \lambda_2=0.25, \lambda3=0.5$。最后总模型在训练的时候也是先使用Seq2Seq模型先预训练一个基础模型，然后在其基础上在使用reward进行policy gradient的训练来优化模型的效果。</li></ul><h4 id="强化学习模型细节"><a href="#强化学习模型细节" class="headerlink" title="强化学习模型细节"></a>强化学习模型细节</h4><blockquote><p>完全监督环境（fully supervised setting）: 一个预先训练的SEQ2SEQ模型，用作初始化强化学习模型。<br>注意力模型（Attention）: 模型在产生输出的时候，还会产生一个“注意力范围”表示接下来输出的时候要重点关注输入序列中的哪些部分，然后根据关注的区域来产生下一个输出，如此往复。</p></blockquote><p>论文采用了AlphaGo风格的模型：通过一个完全监督的环境下的一般响应生成策略来初始化强化学习模型。其中，SEQ2SEQ模型加入了Attention机制并且该模型在<strong>OpenSubtitles dataset</strong>数据集上训练。</p><p>论文并未采用预训练的Seq2Seq模型来初始化强化学习策略模型，而是使用了第一作者本人在2016年提出的生成最大互信息响应的encoder-decoder模型: 使用$p_{SEQ2SEQ}(a|p_i, q<em>i)$来初始化$p</em>{RL}$。从生成的候选集$A={\hat{a}|\hat{a}~p_{RL}}$中的$\hat{a}$获取互信息的得分$m(\hat{a}, [p_i, q_i])$，那么对一个sequence的期望奖励函数为：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuhd5cu9doj208j01ddfo.jpg" alt=""></p><p>通过似然率估计的梯度为：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuhd6s09lyj20ax01awed.jpg" alt=""></p><p>通过随机梯度下降就可以更新encoder-decoder的参数。论文中通过借鉴curriculum learning strategy对梯度进行了改进。</p><p>最终的梯度为：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuhdbxz4kij20b401tjra.jpg" alt=""></p><p>优化模型过程中则使用策略梯度来寻找可以最大化奖励函数的参数：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuief5dqqxj209j01n0sn.jpg" alt=""></p><h3 id="仿真实验细节"><a href="#仿真实验细节" class="headerlink" title="仿真实验细节"></a>仿真实验细节</h3><h4 id="对话仿真流程："><a href="#对话仿真流程：" class="headerlink" title="对话仿真流程："></a>对话仿真流程：</h4><ol><li>从训练集中挑选一个message给Agent-A</li><li>Agent-A对message进行编码并解码出一个响应作为输出。</li><li>Agent-B以Agent-A的输出作为输入，并且通过encoder-decoder来</li></ol><p>而策略policy就是Seq2Seq模型生成的相应的概率分布。我们可以把这个问题看成是上下文的对话历史输入到神经网络中，然后输出是一个response的概率分布：$pRL(pi+1|pi,qi)$。所谓策略就是进行随机采样，选择要进行的回答。最后使用policy gradient进行网络参数的训练。</p><p>两个agent互相对话最终得到的reward来调整base model的参数。</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf4u98teyj20lv0aatav.jpg" alt=""></p><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><blockquote><p>BLEU: bilingual evaluation understudy，一个评估机器翻译准确度的算法。<br>论文并没有使用 广泛应用的BLEU作为评价标准。</p></blockquote><ol><li><p>对话的长度，作者认为当对话出现dull response的时候就算做对话结束，所以使用对话的轮次来作为了评价指标：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf0a67z3dj20fi051q3c.jpg" alt=""></p></li><li><p>不同unigrams、bigrams元组的数量和多样性，用于评测模型产生回答的丰富程度：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf0boign2j20e104a3z0.jpg" alt=""></p></li><li><p>人类评分：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf0gegq71j20gc03sgm9.jpg" alt=""></p></li><li><p>最终对话效果<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fuf0hq4jzaj20g203k3z5.jpg" alt=""></p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>作者使用深度强化学习的方法来改善多轮对话的效果，并提出了三种reward的定义方式。可以算是DRL与NLP结合的一个比较不错的例子。但是从最后的结果部分也可以看得出，作者无论是在reward的定义、还是最后的评价指标都没有采用使用比较广泛的BLUE指标。这种手工定义的reward函数不可能涵盖一段理想对话所具有特点的的方方面面。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ol><li><a href="https://www.paperweekly.site/papers/notes/221">https://www.paperweekly.site/papers/notes/221</a></li><li><a href="https://scholar.google.com/">https://scholar.google.com/</a></li><li><a href="https://blog.csdn.net/u014595019/article/details/52826423">https://blog.csdn.net/u014595019/article/details/52826423</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对话AI的论文列表</title>
      <link href="/2018/08/09/convAI-paper-list/"/>
      <url>/2018/08/09/convAI-paper-list/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>论文列表格式<br>&emsp;论文发表年份： 论文题目&amp;论文链接：第一作者（第一作者所属学校/机构），代码链接</p></blockquote><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="Existing-Models-of-Dialog-System"><a href="#Existing-Models-of-Dialog-System" class="headerlink" title="Existing Models of Dialog System"></a>Existing Models of Dialog System</h3><h4 id="Task-Oriented-Dialog"><a href="#Task-Oriented-Dialog" class="headerlink" title="Task-Oriented Dialog"></a>Task-Oriented Dialog</h4><ul><li>13: <a href="https://ieeexplore.ieee.org/document/6407655/"><strong>POMDP-Based Statistical Spoken Dialog Systems: A Review</strong></a>: Steve Young(Cambridge University)</li><li>11: <a href="https://www.wiley.com/en-us/Spoken+Language+Understanding:+Systems+for+Extracting+Semantic+Information+from+Speech-p-9780470688243"><strong>Spoken Language Understanding: Systems for Extracting Semantic Information from Speech</strong></a>: Book!</li><li>11:<a href="http://www.aclweb.org/anthology/D11-1054"><strong>Data-Driven Response Generation in Social Media</strong></a>: Alan Ritter(University of Washington Seattle)</li><li><p>15: <a href="https://www.aclweb.org/anthology/N/N15/N15-1020.pdf"><strong>A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</strong></a>: Alessandro Sordoni(Universite de Montreal)</p></li><li><p>15: <a href="https://arxiv.org/pdf/1506.05869.pdf"><strong>A Neural Conversational Model</strong></a>: Oriol Vinyals(Google), <a href="https://github.com/Conchylicultor/DeepQA"><strong>code</strong></a> via tensorflow</p></li><li>15: <a href="https://www.aclweb.org/anthology/P15-1152"><strong>Neural Responding Machine for Short-Text Conversation</strong></a>: Lifeng Shang(Noah’s Ark Lab), <a href="https://github.com/stamdlee/DeepLearningFramework"><strong>code</strong></a> via theano and tensorflow</li></ul><h3 id="Traditional-NLP-component-stack"><a href="#Traditional-NLP-component-stack" class="headerlink" title="Traditional NLP component stack"></a>Traditional NLP component stack</h3><h4 id="Challenge-of-NLP"><a href="#Challenge-of-NLP" class="headerlink" title="Challenge of NLP"></a>Challenge of NLP</h4><ul><li>09: <a href="https://www.cs.colorado.edu/~martin/slp.html"><strong>SPEECH and LANGUAGE PROCESSING An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition Second Edition</strong></a>: book </li></ul><h3 id="Deep-Semantic-Similarity-Model-DSSM"><a href="#Deep-Semantic-Similarity-Model-DSSM" class="headerlink" title="Deep Semantic Similarity Model(DSSM)"></a>Deep Semantic Similarity Model(DSSM)</h3><h4 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a>application scenarios</h4><ol><li>Web search<ul><li>13: <a href="http://dl.acm.org/citation.cfm?id=2505665"><strong>Learning deep structured semantic models for web search using clickthrough data</strong></a>: Po-Sen Huang(University of Illinois at Urbana-Champaign), <a href="https://github.com/wangtianqi1993/DL-WebSearch"><strong>code</strong></a> via tensorflow</li><li>14: <a href="http://dl.acm.org/citation.cfm?doid=2661829.2661935"><strong>A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</strong></a>: Yelong Shen(Microsoft Research)</li><li>16: <a href="https://arxiv.org/abs/1502.06922"><strong>Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</strong></a>: Hamid Palangi, <a href="https://github.com/zhaosm/dssm-lstm"><strong>code</strong></a></li></ul></li><li>Entity linking<ul><li>14: <a href="http://anthology.aclweb.org/D/D14/D14-1002.pdf"><strong>Modeling Interestingness with Deep Neural Networks</strong></a>: Jianfeng Gao(Microsoft Research)</li></ul></li><li>Image captioning<ul><li>15: <a href="https://arxiv.org/abs/1411.4952"><strong>From Captions to Visual Concepts and Back</strong></a>: Hao Fang&amp;Li Deng(Microsoft Research)</li></ul></li><li>Machine Translation<ul><li><a href="http://aclweb.org/anthology/P/P14/P14-1066.pdf"><strong>Learning Continuous Phrase Representations for Translation Modeling</strong></a>: Jianfeng Gao(Microsoft Research)</li></ul></li><li>Online recommendation<ul><li>[<strong>duplicate</strong>] 14: <a href="http://anthology.aclweb.org/D/D14/D14-1002.pdf"><strong>Modeling Interestingness with Deep Neural Networks</strong></a>: Jianfneg Gao(Microsoft Research)</li></ul></li></ol><h4 id="Framework-of-Model"><a href="#Framework-of-Model" class="headerlink" title="Framework of Model"></a>Framework of Model</h4><ul><li>[<strong>duplicate</strong>] 13: <a href="http://dl.acm.org/citation.cfm?id=2505665"><strong>Learning deep structured semantic models for web search using clickthrough data</strong></a>: Po-Sen Huang(University of Illinois at Urbana-Champaign), [<strong>code</strong>]</li><li>[<strong>duplicate</strong>] 14: <a href="http://dl.acm.org/citation.cfm?doid=2661829.2661935"><strong>A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</strong></a>: Yelong Shen(Microsoft Research)</li><li>16: <a href="https://arxiv.org/abs/1502.06922"><strong>Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</strong></a>: Hamid Palangi, <a href="https://github.com/zhaosm/dssm-lstm"><strong>code</strong></a></li><li><a href="http://aka.ms/sent2vec">Sent2Vec</a>: software by microsoft</li></ul><h4 id="Go-beyound-DSSM"><a href="#Go-beyound-DSSM" class="headerlink" title="Go beyound DSSM"></a>Go beyound DSSM</h4><ul><li>[<strong>duplicate</strong>] 15: <a href="https://arxiv.org/abs/1411.4952"><strong>From Captions to Visual Concepts and Back</strong></a>: Hao Fang&amp;Li Deng(Microsoft Research)</li></ul><hr><h2 id="Question-answeriing-QA-and-Machine-Readiing-Comprehension-MRC"><a href="#Question-answeriing-QA-and-Machine-Readiing-Comprehension-MRC" class="headerlink" title="Question answeriing(QA) and Machine Readiing Comprehension(MRC)"></a>Question answeriing(QA) and Machine Readiing Comprehension(MRC)</h2><h3 id="Open-Domain-Question-Answering"><a href="#Open-Domain-Question-Answering" class="headerlink" title="Open-Domain Question Answering"></a>Open-Domain Question Answering</h3><h4 id="Knowledge-Base-QA"><a href="#Knowledge-Base-QA" class="headerlink" title="Knowledge Base-QA"></a>Knowledge Base-QA</h4><ol><li>Symbolic approach via Large-scale knowledge graphs<ul><li>[<strong>oral</strong>] 98: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/COLING-98-richardson-dolan-vanderwende.pdf">MindNet: acquiring and structuring semantic information from text</a>: Stephen D.Richardson(Microsoft Research)</li><li>[<strong>oral</strong>] 13: <a href="http://www.aclweb.org/anthology/D13-1160">Semantic Parsing on Freebase from Question-Answer Pairs</a>: Jonathan Berant(Stanford University)</li><li>15: <a href="https://arxiv.org/pdf/1510.08565.pdf">Attention with Intention for a Neural Network Conversation Model</a>: Kaisheng Yao(Microsoft Research)</li><li>14: <a href="http://www.aclweb.org/anthology/P14-1091">Knowledge-Based Question Answering as Machine Translation</a>: Junwei Bao(Harbin Institute of Technology)</li><li>15: <a href="http://aclweb.org/anthology/P15-1128">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</a>:Wen-tau Yih(Microsoft Research)</li></ul></li><li><p><strong>ReasoNet</strong> with Shared Memory</p><ul><li>[<strong>oral</strong>][<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li><li>17: <a href="https://arxiv.org/pdf/1609.05284.pdf">ReasoNet: Learning to Stop Reading in Machine Comprehension</a>:Yelong Shen(Microsoft Research)</li></ul></li><li><p>Search Controller in <strong>ReasoNet</strong> </p><ul><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li></ul></li><li><strong>ReasoNet</strong> in symbolic vs neural space<ul><li>Symbolic is comprehensible but not robust<ul><li>11: <a href="http://www.cs.cmu.edu/~tom/pubs/lao-emnlp11.pdf">Random Walk Inference and Learning in A Large Scale Knowledge Base</a>:Ni Lao(Carnegie Mellon University)</li><li>98: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/COLING-98-richardson-dolan-vanderwende.pdf">MindNet: acquiring and structuring semantic information from text</a>:Stephen D.Richardson(Microsoft Research)</li></ul></li><li>Neural is robust but not comprehensible<ul><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li><li>[<strong>oral</strong>] 15: <a href="https://arxiv.org/abs/1412.6575">EMBEDDING ENTITIES AND RELATIONS FOR LEARNING AND INFERENCE IN KNOWLEDGE BASES</a>:Bishan Yang(Cornell University), <a href="https://github.com/thunlp/OpenKE/blob/master/models/DistMult.py">TensorFlow code</a>, <a href="https://github.com/thunlp/OpenKE/blob/OpenKE-PyTorch/models/DistMult.py">PyTorch code</a></li></ul></li><li>Hybrid is robust and  comprehensible<ul><li>18: <a href="https://arxiv.org/pdf/1802.04394.pdf">M-Walk: Learning to Walk in Graph with Monte Carlo Tree Search</a>:Yelong Shen(Microsoft Research&amp;Tecent AI Lab)</li><li>18: [<strong>oral</strong>] <a href="https://arxiv.org/abs/1707.06690">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</a>:Wenhan Xiong(University of California,Santa Barbara), <a href="https://github.com/xwhan/DeepPath">code1</a> <a href="https://github.com/arunarn2/DeepPathwithTensorforce">code2</a></li><li>18: <a href="https://arxiv.org/abs/1711.05851">GO FOR A WALK AND ARRIVE AT THE ANSWER: REASONING OVER PATHS IN KNOWLEDGE BASES USING REINFORCEMENT LEARNING</a>:Rajarshi Das(University of Massachusetts,Amherst), </li></ul></li></ul></li><li>Multi-turn KB-QA<ul><li><del>Programmed Dialogue policy</del><ul><li><del>15: <a href="https://arxiv.org/pdf/1504.07182.pdf">A Probabilistic Framework for Representing Dialog Systems and Entropy-Based Dialog Management through Dynamic Stochastic State Evolution</a>:Ji Wu(IEEE)</del></li></ul></li><li>Trained via RL Dialogue policy<ul><li>16: <a href="https://arxiv.org/abs/1512.01337">Neural Generative Question Answering </a>:Jun Yin(Noah’s Ark Lab, Huawe) <a href="https://github.com/jxfeb/Generative_QA">corpus</a></li><li>[<strong>oral</strong>] 16: <a href="https://arxiv.org/abs/1604.04562">A Network-based End-to-End Trainable Task-oriented Dialogue System</a>:Tsung-Hsien Wen(Cambridge University), <a href="https://github.com/shawnwun/NNDIAL">Theano code</a></li><li>[<strong>oral</strong>] 17: <a href="https://arxiv.org/abs/1609.00777">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>:Bhuwan Dhingra(Carnegie Mellon University), <a href="https://github.com/MiuLab/KB-InfoBot">Theano code</a></li></ul></li></ul></li></ol><h4 id="Text-QA"><a href="#Text-QA" class="headerlink" title="Text-QA"></a>Text-QA</h4><ol><li>MS MARCO<ul><li>16: <a href="https://arxiv.org/abs/1611.09268">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a>:Tri Nguyan(Microsoft AI&amp;Research)</li></ul></li><li>SQuAD<ul><li>16: <a href="https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>:Pranav Rajpurkar(Stanford University)</li></ul></li></ol><h3 id="Neural-MRC-Models"><a href="#Neural-MRC-Models" class="headerlink" title="Neural MRC Models"></a>Neural MRC Models</h3><h4 id="BiDAF"><a href="#BiDAF" class="headerlink" title="BiDAF"></a>BiDAF</h4><ul><li>16: <a href="https://arxiv.org/pdf/1611.01603.pdf">BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</a>:Minjoon Seo(University of Washington)<ul><li><a href="https://github.com/imraviagrawal/ReadingComprehension">code1</a></li><li><a href="https://github.com/bentrevett/bidaf">code2</a> </li><li><a href="https://github.com/akhil-vader/MachineComprehension_SQuAD">code3</a> </li><li><a href="https://github.com/RamkishanPanthena/Machine-Comprehension-using-SQuAD-Dataset">code4</a></li></ul></li></ul><h4 id="SAN"><a href="#SAN" class="headerlink" title="SAN"></a>SAN</h4><ul><li>18: <a href="https://arxiv.org/pdf/1712.03556.pdf">Stochastic Answer Networks for Machine Reading Comprehension</a>: Xiaodong Liu(Microsoft Research,Redmond), <a href="https://github.com/kevinduh/san_mrc">code</a></li></ul><h4 id="Neural-MRC-Models-on-SQuAD"><a href="#Neural-MRC-Models-on-SQuAD" class="headerlink" title="Neural MRC Models on SQuAD"></a><strong>Neural MRC Models on SQuAD</strong></h4><ol><li><p>Encoding: map each text span to a semantic vector</p><ul><li>Word Embedding<ul><li>14: <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>:Jeffrey Pennington(Stanford University)<ul><li><a href="https://github.com/brangerbriz/midi-glove">code:midi-glove</a></li><li><a href="https://github.com/fdurant/wiki_glove">code:wiki-glove</a></li></ul></li><li>13: <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>:Tomas Mikolov(Google Inc.)<ul><li><a href="https://github.com/brijml/mikolov_word2vec">code1</a></li><li><a href="https://github.com/shuuchen/keras_word2vec">code2</a></li></ul></li></ul></li><li><p>Context Embedding</p><ol><li><p>capture context info for each word</p><ul><li>16: <a href="http://aclweb.org/anthology/K16-1006">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>:Oren Melamud(Bar-Ilan University)</li><li>18: <a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>:Matthew E.Peters(Allen Institute for Artificial Intelligence), <a href="https://github.com/zqhZY/ner_elmo">code</a></li><li>18: <a href="https://arxiv.org/pdf/1804.09541.pdf">QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION</a>:Adams Wei Yu(CMU&amp;Google Brain)<ul><li><a href="https://github.com/ni9elf/QANet">code1</a></li><li><a href="https://github.com/BangLiu/QANet-PyTorch">code2</a></li></ul></li></ul></li><li><p>Context Embedding via BiLSTM/ELmo</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>:Matthew E.Peters(Allen Institute for Artificial Intelligence), <a href="https://github.com/zqhZY/ner_elmo">code</a></li><li>17: <a href="https://arxiv.org/abs/1708.00107">Learned in Translation: Contextualized Word Vectors</a>:Bryan McCann(SalesForce)</li><li>16: [duplicate]<a href="http://aclweb.org/anthology/K16-1006">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>:Oren Melamud(Bar-Ilan University)</li></ul></li><li><p>Context Embedding</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/pdf/1804.09541.pdf">QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION</a>:Adams Wei Yu(CMU&amp;Google Brain)<ul><li><a href="https://github.com/ni9elf/QANet">code1</a></li><li><a href="https://github.com/BangLiu/QANet-PyTorch">code2</a></li></ul></li></ul></li></ol></li></ul><ul><li>Query-context/Content-query attention</li></ul></li><li><p>Reasoning: rank and re-rank semantic vectors</p><ul><li><p>Multi-step reasoning for Text-QA</p><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/pdf/1609.05284.pdf">ReasoNet: Learning to Stop Reading in Machine Comprehension</a>:Yelong Shen(Microsoft Research)</li></ul></li><li><p>Stochastic Answer Net</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/pdf/1712.03556.pdf">Stochastic Answer Networks for Machine Reading Comprehension</a>: Xiaodong Liu(Microsoft Research,Redmond), <a href="https://github.com/kevinduh/san_mrc">code</a></li></ul></li></ul></li></ol><hr><h2 id="Task-oriented-dialogues"><a href="#Task-oriented-dialogues" class="headerlink" title="Task-oriented dialogues"></a>Task-oriented dialogues</h2><h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><h4 id="A-Example-Dialogue-with-Movie-Bot"><a href="#A-Example-Dialogue-with-Movie-Bot" class="headerlink" title="A Example Dialogue with Movie-Bot"></a>A Example Dialogue with Movie-Bot</h4><ul><li><a href="https://github.com/MiuLab/TC-Bot">source code</a></li></ul><h4 id="Conversation-as-Reinforcement-Learning"><a href="#Conversation-as-Reinforcement-Learning" class="headerlink" title="Conversation as Reinforcement Learning"></a>Conversation as Reinforcement Learning</h4><ul><li>00: <a href="http://www.thepieraccinis.com/publications/2000/IEEE_TSAP_00.pdf">A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies</a>: Esther Levin(IEEE)</li><li>00: <a href="https://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf">Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System</a>:Satinder Singh(AT&amp;T Labs)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul><h4 id="Dialogue-System-Evaluation-Simulated-Users"><a href="#Dialogue-System-Evaluation-Simulated-Users" class="headerlink" title="Dialogue System Evaluation(Simulated Users)"></a>Dialogue System Evaluation(Simulated Users)</h4><ol><li>Agenda based<ul><li>09: <a href="https://ieeexplore.ieee.org/document/4806280/">The Hidden Agenda User Simulation Model</a>:Jost Schatzmann(IEEE)</li><li><a href="https://github.com/MiuLab/TC-Bot">source code</a> </li></ul></li><li>Model based<ul><li>16: <a href="https://arxiv.org/abs/1607.00070">A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems</a>: Layla El Asri(Maluuba Research)</li><li>17: <a href="https://arxiv.org/pdf/1703.01008.pdf">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul></li></ol><h3 id="traditional-approache"><a href="#traditional-approache" class="headerlink" title="traditional approache"></a>traditional approache</h3><h4 id="Decison-theoretic-View-of-Dialogue-Management"><a href="#Decison-theoretic-View-of-Dialogue-Management" class="headerlink" title="Decison-theoretic View of Dialogue Management"></a>Decison-theoretic View of Dialogue Management</h4><ul><li>[<strong>duplicate</strong>] 00: <a href="https://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf">Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System</a>:Satinder Singh(AT&amp;T Labs)</li><li>00: <a href="http://www.thepieraccinis.com/publications/2000/IEEE_TSAP_00.pdf">A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies</a>: Esther Levin(IEEE)</li><li>00: <a href="http://www.aclweb.org/anthology/P98-2219">Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email</a>: Marilyn A.Walker(ATT Labs Research)</li><li>02: <a href="https://dl.acm.org/citation.cfm?id=1289246">Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning</a>:Konrad Scheffler(Cambridge University)</li></ul><h4 id="Language-Understanding-Uncertainty-POMDP-as-a-principled-framework"><a href="#Language-Understanding-Uncertainty-POMDP-as-a-principled-framework" class="headerlink" title="Language Understanding Uncertainty: POMDP as a principled framework"></a>Language Understanding Uncertainty: POMDP as a principled framework</h4><ul><li>00: <a href="http://www.mit.edu/~nickroy/papers/acl00.pdf">Spoken Dialogue Management Using Probabilistic Reasoning</a>: Nicholas Roy(Carnegie Mellon University)</li><li>01: <a href="http://www.wytsg.org:88/reslib/400/180/110/020/010/130/L000000000233767.pdf">Spoken Dialogue Management as Planning and Acting under Uncertainty</a>:Bo Zhang(Tech. of China)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul><h4 id="scaling-up-Dialogue-Optimization"><a href="#scaling-up-Dialogue-Optimization" class="headerlink" title="scaling up Dialogue Optimization"></a>scaling up Dialogue Optimization</h4><ol><li>Use approxmiate POMDP algorithms leveraging problem-specific structure<ul><li>00: <a href="http://www.mit.edu/~nickroy/papers/acl00.pdf">Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning</a>:Konrad Scheffler(Cambridge University)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul></li><li>Use Reinforcement Learning algorithms with function approximation<ul><li>08: <a href="http://www.aclweb.org/anthology/J08-4002">Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets</a>: James Henderson</li><li>09: <a href="https://pdfs.semanticscholar.org/a950/d7836e101e7d649791714d8383a804a6f671.pdf">Reinforcement Learning for Dialog Management using Least-Squares Policy Iteration and Fast Feature Selection</a>: Lihong Li(Rutgers University)</li><li>14: <a href="http://mi.eng.cam.ac.uk/~sjy/papers/gktb14.pdf">Incremental on-line adaptation of POMDP-based dialogue managers to extended domains</a>:M.Gasic[Cambridge University]</li></ul></li></ol><h3 id="Natural-language-understanding-and-dialogue-state-tracking"><a href="#Natural-language-understanding-and-dialogue-state-tracking" class="headerlink" title="Natural language understanding and dialogue state tracking"></a>Natural language understanding and dialogue state tracking</h3><h4 id="Language-Understanding"><a href="#Language-Understanding" class="headerlink" title="Language Understanding"></a>Language Understanding</h4><ol><li><p>DNN for Domain/Intent Classification</p><ul><li>15:  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/RNNLM_addressee.pdf">Recurrent Neural Network and LSTM Models for Lexical Utterance Classification</a>: Suman Raviuri(University of California,Berkeley)</li></ul></li><li><p>Slot filling</p><ul><li>16: <a href="https://www.csie.ntu.edu.tw/~yvchen/doc/IS16_MultiJoint.pdf">Multi-Domain Joint Semantic Frame Parsing using Bi-directional RNN-LSTM</a>: Dilek Hakkani-Tur(Microsoft Research)</li></ul></li><li><p>Further details on NLU</p><ul><li><a href="https://www.csie.ntu.edu.tw/~yvchen/doc/OpenDialogue_Tutorial_IJCNLP.pdf">ppt</a></li><li>E2E MemNN for Contectual LU: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_ContextualSLU.pdf">End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding</a>: Yun-Nung Chen(National Taiwan University )</li><li>[<strong>duplicate</strong>] LU Importance: 17: <a href="https://arxiv.org/pdf/1703.01008.pdf">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul></li></ol><h4 id="Dialogue-State-Tracking-DST"><a href="#Dialogue-State-Tracking-DST" class="headerlink" title="Dialogue State Tracking(DST)"></a>Dialogue State Tracking(DST)</h4><ol><li>DSTC(Dialog State Tracking Challenge)<ul><li><a href="https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/">DSTC1 official website</a></li><li><a href="http://camdial.org/~mh521/dstc/">DSTC2&amp;3 official website</a></li><li><a href="http://www.colips.org/workshop/dstc4/">DSTC4 official website</a></li><li><a href="http://workshop.colips.org/dstc5/">DSTC5 official website</a></li></ul></li><li><p>Neural Belief Tracker</p><ul><li>16: <a href="https://arxiv.org/abs/1606.03777">Neural Belief Tracker: Data-Driven Dialogue State Tracking</a>: Nikola Mrksic(University of Cambridge)</li></ul></li><li><p>NN-Based DST</p><ul><li>13: <a href="http://www.anthology.aclweb.org/W/W13/W13-4073.pdf">Deep Neural Network Approach for the Dialog State Tracking Challenge</a>: Matthew Henderson(University of Cambridge)</li><li>15: <a href="https://arxiv.org/abs/1506.07190">Multi-domain Dialog State Tracking using Recurrent Neural Networks</a>: Nikola Mrksic(University of Cambridge)</li><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/abs/1606.03777">Neural Belief Tracker: Data-Driven Dialogue State Tracking</a>: Nikola Mrksic(University of Cambridge)</li></ul></li></ol><h3 id="Deep-RL-for-dialogue-policy-learning"><a href="#Deep-RL-for-dialogue-policy-learning" class="headerlink" title="Deep RL for dialogue policy learning"></a>Deep RL for dialogue policy learning</h3><h4 id="Two-main-classed-of-RL-algorithms"><a href="#Two-main-classed-of-RL-algorithms" class="headerlink" title="Two main classed of RL algorithms"></a>Two main classed of RL algorithms</h4><ol><li>Value function based:<ul><li>15: <a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a>: Volodymyr Minh<ul><li><a href="https://github.com/devsisters/DQN-tensorflow">code1</a> by tensorflow</li><li><a href="https://github.com/pianomania/DQN-pytorch">code2</a> by pytorch</li></ul></li><li>16: <a href="https://arxiv.org/pdf/1606.02560.pdf">Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning</a>: Tiancheng Zhao(Carnegie Mellon University)</li></ul></li><li>Policy based:<ul><li>92: <a href="https://doi.org/10.1007/BF00992696">Simple statistical gradient-following algorithms for connectionist reinforcement learning</a>: Ronald J.Williams</li><li>17: <a href="http://www.aclweb.org/anthology/P/P16/P16-1230.pdf">On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems</a>: Pei-Hao Su(University of Cambridge)</li></ul></li></ol><h4 id="Domain-Extension-and-Exploration-BBQ-network"><a href="#Domain-Extension-and-Exploration-BBQ-network" class="headerlink" title="Domain Extension and Exploration(BBQ network)"></a>Domain Extension and Exploration(BBQ network)</h4><ul><li>18: <a href="https://arxiv.org/pdf/1608.05081.pdf">BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems</a>: Zachary Lipton(Carnegir Mellon University)</li></ul><h4 id="Composite-task-Dialogues"><a href="#Composite-task-Dialogues" class="headerlink" title="Composite-task Dialogues"></a>Composite-task Dialogues</h4><ol><li>A Hierarchical Policy Learner<ul><li>98: <a href="http://papers.nips.cc/paper/1384-reinforcement-learning-with-hierarchies-of-machines.pdf">Reinforcement Learning with Hierarchies of Machines</a>: Ronald Parr(UC Berkeley)</li><li>17: <a href="https://arxiv.org/abs/1704.03084">Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning</a>: Baolin Peng(Microsoft Research)</li></ul></li><li>Integrating Planning for Dialogue Policy Learning<ul><li>18: <a href="https://arxiv.org/abs/1801.06176">Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning</a>: Baolin Peng(Microsoft Research) , <a href="https://github.com/MiuLab/DDQ">code</a></li></ul></li></ol><h3 id="Decision-theoretic-View-of-Dialogue-Management"><a href="#Decision-theoretic-View-of-Dialogue-Management" class="headerlink" title="Decision-theoretic View of Dialogue Management"></a>Decision-theoretic View of Dialogue Management</h3><h4 id="Hybrid-Code-Networks"><a href="#Hybrid-Code-Networks" class="headerlink" title="Hybrid Code Networks"></a>Hybrid Code Networks</h4><ul><li>17: <a href="https://arxiv.org/abs/1702.03274">Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</a>: Jason D. Williams(Microsoft Research)</li></ul><h4 id="Differentiating-KB-Accesses"><a href="#Differentiating-KB-Accesses" class="headerlink" title="Differentiating KB Accesses"></a>Differentiating KB Accesses</h4><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/abs/1609.00777">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>:Bhuwan Dhingra(Carnegie Mellon University)</li></ul><h4 id="An-E2E-Neural-Dialogue-System"><a href="#An-E2E-Neural-Dialogue-System" class="headerlink" title="An E2E Neural Dialogue System"></a>An E2E Neural Dialogue System</h4><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/pdf/1703.01008.pdf">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul><hr><h2 id="Fully-data-driven-conversation-models-and-chatbots"><a href="#Fully-data-driven-conversation-models-and-chatbots" class="headerlink" title="Fully data-driven conversation models and chatbots"></a>Fully data-driven conversation models and chatbots</h2><h3 id="Historical-overview"><a href="#Historical-overview" class="headerlink" title="Historical overview"></a>Historical overview</h3><h4 id="Response-retrival-system"><a href="#Response-retrival-system" class="headerlink" title="Response retrival system"></a>Response retrival system</h4><ul><li>10: <a href="https://aritter.github.io/chat.pdf">Filter, Rank, and Transfer the Knowledge: Learning to Chat</a>:<br>Alan Ritter(University of Washington)</li></ul><h4 id="Response-generation-using-Statistical-Machine-Translation"><a href="#Response-generation-using-Statistical-Machine-Translation" class="headerlink" title="Response generation using Statistical Machine Translation"></a>Response generation using Statistical Machine Translation</h4><ul><li>11:  <a href="http://www.aclweb.org/anthology/D11-1054">Data-Driven Response Generation in Social Media</a>: Alan Ritter(University of Washington)</li></ul><h4 id="First-neural-response-generation-systems"><a href="#First-neural-response-generation-systems" class="headerlink" title="First neural response generation systems"></a>First neural response generation systems</h4><ol><li>Neural Models for Response Generation<ul><li>15: <a href="https://www.aclweb.org/anthology/N/N15/N15-1020.pdf">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a>: Alessandro Sordoni(University de Montreal)</li><li>15: <a href="https://arxiv.org/pdf/1506.05869.pdf">A Neural Conversational Model</a>: Oriol Vinyals(Google .Inc)</li><li>15: <a href="https://www.aclweb.org/anthology/P15-1152">Neural Responding Machine for Short-Text Conversation</a>: Lifeng Shang(Noah’s Ark Lab), <a href="https://github.com/stamdlee/DeepLearningFramework">code</a></li></ul></li><li>Neural conversation engine: <ul><li>16: <a href="http://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a>: Jiwei Li(Stanford University)</li></ul></li></ol><h3 id="challenges-and-remedies"><a href="#challenges-and-remedies" class="headerlink" title="challenges and remedies"></a>challenges and remedies</h3><h4 id="Challenge-The-blandness-problem"><a href="#Challenge-The-blandness-problem" class="headerlink" title="Challenge: The blandness problem"></a>Challenge: The blandness problem</h4><ul><li>[<strong>duplicate</strong>] 16: <a href="http://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a>: Jiwei Li(Stanford University)</li></ul><h4 id="Challenge-The-consistency-problem"><a href="#Challenge-The-consistency-problem" class="headerlink" title="Challenge: The consistency problem"></a>Challenge: The consistency problem</h4><ol><li>Solution: Personalized Response Generation<ul><li>Microsoft Personality chat:speaker embedding LSTM: <a href="https://arxiv.org/abs/1603.06155">A Persona-Based Neural Conversation Model</a>: Jiwei Li(Stanford University), <a href="https://github.com/fionn-mac/A-Persona-Based-Neural-Conversation-Model">code</a> via Pytorch</li></ul></li><li>Personal modeling as multi-task learning<ul><li>17: <a href="https://arxiv.org/abs/1710.07388">Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models</a>: Yi Luan(University of Washington)</li></ul></li><li>Improving personalization with multiple losses<ul><li>16: <a href="https://arxiv.org/pdf/1606.00372.pdf">Conversational Contextual Cues: The Case of Personalization and History for Response Ranking</a>: Rami Al-Rfou(Google .Inc)</li></ul></li></ol><h4 id="Challenge-Long-conversational-context"><a href="#Challenge-Long-conversational-context" class="headerlink" title="Challenge: Long conversational context"></a>Challenge: Long conversational context</h4><ol><li>It can be challenging for LSTM/GRU to encode very long context<ul><li>18: <a href="https://arxiv.org/abs/1805.04623">Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</a>: Urvashi Khadelwal(Stanford University)</li></ul></li><li>Hierarchical Encoder-Decoder(HRED), <a href="https://github.com/urvashik/lm-context-analysis">code</a><ul><li>16: <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160">Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a>: Iulian V.Serban(University de Montreal), <a href="https://github.com/hsgodhia/hred">code</a></li></ul></li><li>Hierarchical Latent Variable Encoder-Decoder(VHRED)<ul><li>17: <a href="http://www.cs.toronto.edu/~lcharlin/papers/vhred_aaai17.pdf">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a>: Iulian V. Serban</li></ul></li></ol><h3 id="Grounded-conversation-models"><a href="#Grounded-conversation-models" class="headerlink" title="Grounded conversation models"></a>Grounded conversation models</h3><h4 id="A-Knowledge-Grounded-Neural-Conversation-Model"><a href="#A-Knowledge-Grounded-Neural-Conversation-Model" class="headerlink" title="A Knowledge-Grounded Neural Conversation Model"></a>A Knowledge-Grounded Neural Conversation Model</h4><ul><li>15: <a href="https://arxiv.org/pdf/1503.08895.pdf">End-To-End Memory Networks</a>: Sainbayar Sukhbaatar(New York University)<ul><li><a href="https://github.com/carpedm20/MemN2N-tensorflow">code1</a> via Tensorflow</li><li><a href="https://github.com/domluna/memn2n">code2</a> via Tensorflow</li><li><a href="https://github.com/vinhkhuc/MemN2N-babi-python">code3</a> for bAbI QA tasks</li></ul></li><li>17: <a href="https://arxiv.org/abs/1702.01932">A Knowledge-Grounded Neural Conversation Model</a>: Marjan Gahzvininejad(USC)</li></ul><h4 id="Grounded-E2E-Dialogue-Systems"><a href="#Grounded-E2E-Dialogue-Systems" class="headerlink" title="Grounded E2E Dialogue Systems"></a>Grounded E2E Dialogue Systems</h4><ul><li>16: <a href="https://arxiv.org/abs/1611.08669">Visual Dialog</a>: Abhishek Das(Georgia Institute of Tehhnology)<ul><li><a href="https://github.com/batra-mlp-lab/visdial">code1</a> via Lua</li><li><a href="https://github.com/jiasenlu/visDial.pytorch">code2</a> via Pytorch</li></ul></li><li>17: <a href="https://arxiv.org/abs/1701.08251">Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation</a>: Nasrin Mostafazadeh(University of Rochster)</li><li>18: <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/04/huber2018chi.small_.pdf">Emotional Dialogue Generation using Image-Grounded Language Models</a>:Bernd Huber(Harvard University)</li></ul><h3 id="Beyond-supervised-learning-Deep-Reinforcement-Learning-for-E2E-Dialogue"><a href="#Beyond-supervised-learning-Deep-Reinforcement-Learning-for-E2E-Dialogue" class="headerlink" title="Beyond supervised learning(Deep Reinforcement Learning for E2E Dialogue)"></a>Beyond supervised learning(Deep Reinforcement Learning for E2E Dialogue)</h3><ul><li>16: <a href="https://arxiv.org/abs/1606.01541">Deep Reinforcement Learning for Dialogue Generation</a>:Jiwei Li(Stanford University)<ul><li><a href="https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow">code1</a> via Tensorflow</li><li><a href="https://github.com/agsarthak/Goal-oriented-Dialogue-Systems">code2</a> via keras</li><li><a href="https://github.com/jiweil/Neural-Dialogue-Generation">code3</a> by Jiwei Li</li></ul></li></ul><h3 id="Data-and-evaluation"><a href="#Data-and-evaluation" class="headerlink" title="Data and evaluation"></a>Data and evaluation</h3><h4 id="Conversational-datasets-for-social-bots-E2E-dialogue-research"><a href="#Conversational-datasets-for-social-bots-E2E-dialogue-research" class="headerlink" title="Conversational datasets(for social bots, E2E dialogue research)"></a>Conversational datasets(for social bots, E2E dialogue research)</h4><ul><li>15: <a href="https://arxiv.org/abs/1512.05742">A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a>: Iulian Vlad Serban(Universite de Montreal)</li></ul><h4 id="Evaluating-E2E-Dialogue-Systems-via-Autumatic-evaluation"><a href="#Evaluating-E2E-Dialogue-Systems-via-Autumatic-evaluation" class="headerlink" title="Evaluating E2E Dialogue Systems via Autumatic evaluation"></a>Evaluating E2E Dialogue Systems via Autumatic evaluation</h4><ol><li>Machine-Translation-Based Metric<ul><li>02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4">code</a></li><li>02: <a href="http://www.mt-archive.info/HLT-2002-Doddington.pdf">Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics</a>: George Doddington</li></ul></li><li>Sentence-level correlation of MT metrics:<ul><li>16: <a href="https://aclweb.org/anthology/D16-1230">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a>: Chia-Wei Liu(McGill University)</li><li>15: <a href="http://www.aclweb.org/anthology/N15-1124">Accurate Evaluation of Segment-level Machine Translation Metrics</a>: Yvette Graham(The University of Melbourne)</li></ul></li></ol><h4 id="The-importance-of-sample-size"><a href="#The-importance-of-sample-size" class="headerlink" title="The importance of sample size"></a>The importance of sample size</h4><ul><li>[<strong>duplicate</strong>] 02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4">code</a></li><li>06: <a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/bootstrap2004.pdf">Statistical Significance Tests for Machine Translation Evaluation</a>: Philipp Kowehn(MIT)</li></ul><h4 id="Corpus-level-Correlation"><a href="#Corpus-level-Correlation" class="headerlink" title="Corpus-level Correlation"></a>Corpus-level Correlation</h4><ul><li>[<strong>duplicate</strong>] 02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4">code</a></li><li>[<strong>duplicate</strong>] 06: <a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/bootstrap2004.pdf">Statistical Significance Tests for Machine Translation Evaluation</a>: </li></ul><h3 id="Chatbot-in-public"><a href="#Chatbot-in-public" class="headerlink" title="Chatbot in public"></a>Chatbot in public</h3><h4 id="Social-Bots-commercial-systems"><a href="#Social-Bots-commercial-systems" class="headerlink" title="Social Bots: commercial systems"></a>Social Bots: commercial systems</h4><ol><li>For end users<ul><li>Replika.ai system description: <a href="https://github.com/lukalabs/replika-research/blob/master/scai2017/replika_ai.pdf">replika_ai</a>: Slides</li><li>XiaoIce:<br>15:<a href="https://www.nytimes.com/interactive/2015/07/27/science/chatting-with-xiaoice.html">Chatting With Xiaoice</a>: News</li></ul></li><li>For bot developers<ul><li>[<strong>duplicate</strong>] Microsoft Personality chat:speaker embedding LSTM: <a href="https://arxiv.org/abs/1603.06155">A Persona-Based Neural Conversation Model</a>: Jiwei Li(Stanford University), <a href="https://github.com/fionn-mac/A-Persona-Based-Neural-Conversation-Model">code</a> via Pytorch</li><li>Microsoft Personality chat’s API: <a href="https://labs.cognitive.microsoft.com/en-us/project-personality-chat">Project Personality Chat’s url</a> </li></ul></li></ol><h4 id="Open-Benchmarks"><a href="#Open-Benchmarks" class="headerlink" title="Open Benchmarks"></a>Open Benchmarks</h4><ol><li><p>Alexa Challenge</p><ul><li>website: <a href="https://developer.amazon.com/alexaprize/proceedings">Alexa Prize Proceedings</a></li></ul></li><li><p>Dialogue System Technology Challenge(DSTC)</p><ul><li><a href="http://workshop.colips.org/dstc7">DSTC7</a></li><li>Visual-Scene: <a href="https://github.com/hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge">DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge 2018</a></li><li>background article:<br><a href="https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling">DSTC7-End-to-End-Conversation-Modeling 2018</a></li><li>Registration Link:<br><a href="http://workshop.colips.org/dstc7/call.html">DSTC7 Registration</a></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> conversationalAI </tag>
            
            <tag> nlp </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对话AI的术语和学习地图</title>
      <link href="/2018/08/08/convAI-map-and-term/"/>
      <url>/2018/08/08/convAI-map-and-term/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="chatbot基本篇"><a href="#chatbot基本篇" class="headerlink" title="chatbot基本篇"></a>chatbot基本篇</h2><ul><li><p>Natural language processing(自然语言处理/NLP)<br>自然语言处理是人工智能的一个子集领域。自然语言处理是一项包罗万象且相当复杂的技术，它包含许多子集，如自然语言理解。<br>NLP指的是机器理解人类输入的所有东西。为此，NLP引擎将使用许多工具，如NLU，总结算法，情绪分析，标记化等等。</p></li><li><p>Natural language understanding (自然语言理解/NLU)<br>自然语言理解是自然语言处理的一个子集。NLU和NLP经常被混淆，因为它们的意思非常接近。<br>NLU是NLP引擎中非常具体的部分，它检查话语并提取其实体和意图。用更通俗的话说，NLU允许机器理解用户在说什么。<br>说到聊天机器人，可以把NLU想象成阅读人类语言并识别文本不同部分的过程，把它分解成正确的意图和实体</p></li><li><p>Chatbot(聊天机器人)<br><code>chatbot</code>是一个可对话的计算机程序。但是<strong>对话agent</strong>可能是形容这个程序更好的词汇。</p></li><li><p>Utterance(表达)<br>用户对chatbot说的任何话，也可以看做是用户输入。例如，如果用户输入“给我看昨天的财经新闻”，整个句子就是<code>Utterance</code>。</p></li><li><p>Intent(意图))<br><code>Intent</code>代表了用户<code>Utterance</code>的意义。Chatbot将会根据用户一系列的<code>Intent</code>和对<code>Intent</code>的理解来回应用户。例如，如果用户输入“show me yesterday’s financial news”，用户的意图是检索金融标题列表。<code>Intent</code>通常是一个动词和一个名词，如“showNews”。</p></li><li><p>Entity(实体)<br><code>Entity</code>通常修饰<code>Intent</code>。例如，如果用户输入“show me yesterday’s financial news”，那么<code>Entity</code>是“yesterday”和“financial”。<code>Entity</code>会被赋予一个名称，例如“dateTime”和“newsType”。<code>Entity</code>有时也被称为<code>Slots</code>。</p></li></ul><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fu2otnnx3oj21r60x246f.jpg" alt=""></p><ul><li><p>Broadcast(广播)<br><code>Broadcast</code>是预先发送给用户的消息。它不是对用户输入的响应。<code>Broadcast</code>也被称为“订阅消息”，它相当于聊天机器人中的移动应用程序中的推送消息。</p></li><li><p>Ambiguity</p></li><li>Paraphrase</li><li>metric</li></ul><h2 id="术语进阶篇"><a href="#术语进阶篇" class="headerlink" title="术语进阶篇"></a>术语进阶篇</h2><h3 id="NLP常用术语"><a href="#NLP常用术语" class="headerlink" title="NLP常用术语"></a>NLP常用术语</h3><h4 id="词级别"><a href="#词级别" class="headerlink" title="词级别"></a>词级别</h4><ul><li>分词（Seg）</li><li>词性标注（POS）</li><li>命名实体识别（NER）</li><li>未登录词识别</li><li>词向量（word2vec）</li><li>词义消歧</li></ul><h4 id="句子级别"><a href="#句子级别" class="headerlink" title="句子级别"></a>句子级别</h4><ul><li>情感分析</li><li>关系提取</li><li>意图识别</li><li>依存句法分析（parser）</li><li>角色标注，</li><li>浅层语义分析，</li><li>指代消解</li></ul><h4 id="篇章级别"><a href="#篇章级别" class="headerlink" title="篇章级别"></a>篇章级别</h4><ul><li>信息抽取：</li><li>本体提取：</li><li>事件抽取：</li><li>主题提取：</li><li>文档聚类：</li><li>舆情分析：</li><li>篇章理解：</li><li>自动文摘：</li></ul><h4 id="常用基础算法："><a href="#常用基础算法：" class="headerlink" title="常用基础算法："></a>常用基础算法：</h4><ol><li><p>机器学习：</p><ul><li>隐马尔科夫（HMM）</li><li>条件随机场（CRF）</li><li>支持向量机（SVM）</li><li>语言模型</li><li>主题模型（LDA）</li><li>TF-IDF</li><li>互信息（PMI）</li><li>贝叶斯模型</li><li>概率图模型   </li></ul></li><li><p>深度学习:</p></li></ol><h3 id="Qustion-Answering-QA"><a href="#Qustion-Answering-QA" class="headerlink" title="Qustion Answering(QA)"></a>Qustion Answering(QA)</h3><h3 id="Reinfoecement-Learning-强化学习-RL"><a href="#Reinfoecement-Learning-强化学习-RL" class="headerlink" title="Reinfoecement Learning(强化学习/RL)"></a>Reinfoecement Learning(强化学习/RL)</h3><h3 id="Markov-Decision-Process-马尔科夫决策过程-MDP"><a href="#Markov-Decision-Process-马尔科夫决策过程-MDP" class="headerlink" title="Markov Decision Process(马尔科夫决策过程/MDP)"></a>Markov Decision Process(马尔科夫决策过程/MDP)</h3><h3 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP"></a>POMDP</h3><h3 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h3><h3 id="Phonology"><a href="#Phonology" class="headerlink" title="Phonology"></a>Phonology</h3><h3 id="分词（Segment）"><a href="#分词（Segment）" class="headerlink" title="分词（Segment）"></a>分词（Segment）</h3><p>中英文都存在分词的问题，不过相对来说，英文单词与单词之间本来就有空格进行分割，所以处理起来相对方便。但是中文书写是没有分隔符的，所以分词的问题就比较突出。分词常用的手段可以是基于字典的最长串匹配，据说可以解决85%的问题，但是歧义分词很难。另外就是当下主流的统计机器学习的办法。</p><h3 id="词性标注（Label）"><a href="#词性标注（Label）" class="headerlink" title="词性标注（Label）"></a>词性标注（Label）</h3><p>基于机器学习的方法里，往往需要对词的词性进行标注。标注的目的是用来表示，词的一种隐状态，隐藏状态构成的转移就构成了状态转移序列。例如：苏宁易购/n 投资/v 了/u 国际米兰/n。其中，n代表名词，v代表动词，n,v都是标注。以此类推。</p><h3 id="命名实体识别（Named-Entity-Recognition）"><a href="#命名实体识别（Named-Entity-Recognition）" class="headerlink" title="命名实体识别（Named Entity Recognition）"></a>命名实体识别（Named Entity Recognition）</h3><p>本质上还是标注问题的一种。只不过把标注细化了。比如，苏宁/cmp_s 易购/cmp_e 是/v B2C/n 电商/n。我们把苏宁易购 标注成cmp_s和cmp_e,分别表征公司名的起始和结束。这样，当遇上苏宁/云商/易购这种场景时，也可以完整得识别出它是一个公司名称。如果，按照传统的标注方式，苏宁/cmp 易购/cmp这样笼统地标注可能会有问题。</p><h3 id="句法分析（Syntax-Parsing）"><a href="#句法分析（Syntax-Parsing）" class="headerlink" title="句法分析（Syntax Parsing）"></a>句法分析（Syntax Parsing）</h3><p>句法分析往往是一种基于规则的专家系统。当然也不是说它不能用统计学的方法进行构建，不过最初的时候，还是利用语言学专家的知识来构建的。句法分析的目的是解析句子的中各个成分的依赖关系。所以，往往最终生成的结果，是一棵句法分析树。句法分析可以解决传统词袋模型不考虑上下文的问题。比如，张三是李四的领导；李四是张三的领导。这两句话，用词袋模型是完全相同的，但是句法分析可以分析出其中的主从关系，真正理清句子的关系。</p><h3 id="指代消解-Anaphora-Resolution"><a href="#指代消解-Anaphora-Resolution" class="headerlink" title="指代消解(Anaphora Resolution)"></a>指代消解(Anaphora Resolution)</h3><p>中文中代词出现的频率很高，它的作用的是用来表征前文出现过的人名、地名等词。例如，苏宁易购坐落在南京，这家公司目前位于中国B2C市场前三。在这句话中，其实“苏宁易购”这个词出现了2次，“这家公司”指代的就是苏宁易购。但是出于中文的习惯，我们不会把“苏宁易购”再重复一遍。</p><h2 id="AI模型篇"><a href="#AI模型篇" class="headerlink" title="AI模型篇"></a>AI模型篇</h2><h3 id="Deep-Semantic-Similarity-Model-DSSM"><a href="#Deep-Semantic-Similarity-Model-DSSM" class="headerlink" title="Deep Semantic Similarity Model(DSSM)"></a>Deep Semantic Similarity Model(DSSM)</h3><h3 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h3><h3 id="Machine-Reading-Comprehension-MRC"><a href="#Machine-Reading-Comprehension-MRC" class="headerlink" title="Machine Reading Comprehension(MRC)"></a>Machine Reading Comprehension(MRC)</h3><h3 id="Knowledge-Base-QA-KBQA"><a href="#Knowledge-Base-QA-KBQA" class="headerlink" title="Knowledge Base-QA(KBQA)"></a>Knowledge Base-QA(KBQA)</h3><ul><li>WordNet(1998)</li><li>Freebase(2008)</li><li>Yago(2007)</li></ul><h3 id="Knowledge-base-completion-KBC"><a href="#Knowledge-base-completion-KBC" class="headerlink" title="Knowledge base completion(KBC)"></a>Knowledge base completion(KBC)</h3><h2 id="chatbot领域学习地图："><a href="#chatbot领域学习地图：" class="headerlink" title="chatbot领域学习地图："></a>chatbot领域学习地图：</h2><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fue50ug6o9j217m88pb2c.jpg" alt=""></p><blockquote><p>参考与引用</p><ol><li><a href="https://www.microsoft.com/en-us/research/publication/neural-approaches-to-conversational-ai/">https://www.microsoft.com/en-us/research/publication/neural-approaches-to-conversational-ai/</a></li><li><a href="https://chatbotsmagazine.com/chatbot-vocabulary-10-chatbot-terms-you-need-to-know-3911b1ef31b4">https://chatbotsmagazine.com/chatbot-vocabulary-10-chatbot-terms-you-need-to-know-3911b1ef31b4</a></li><li><a href="https://blog.ubisend.com/discover-chatbots/chatbot-glossary">https://blog.ubisend.com/discover-chatbots/chatbot-glossary</a></li><li><a href="https://blog.csdn.net/wangongxi/article/details/52662177">https://blog.csdn.net/wangongxi/article/details/52662177</a></li><li><a href="https://www.jianshu.com/p/d7ec29abbcb8">https://www.jianshu.com/p/d7ec29abbcb8</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> conversationalAI </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习手写笔记</title>
      <link href="/2018/08/03/Deeplearning-note/"/>
      <url>/2018/08/03/Deeplearning-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><div class="row"><iframe src="https://drive.google.com/file/d/14hBEl8WUtbBdDHcIE4uAMledytupRDzP/preview" style="width:100%; height:550px"></iframe></div>]]></content>
      
      
      <categories>
          
          <category> HandWriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客主题配置</title>
      <link href="/2018/07/26/blog-config/"/>
      <url>/2018/07/26/blog-config/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="hexo基本使用"><a href="#hexo基本使用" class="headerlink" title="hexo基本使用"></a>hexo基本使用</h2><h3 id="创建新页面"><a href="#创建新页面" class="headerlink" title="创建新页面"></a>创建新页面</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page your_page_name</span><br></pre></td></tr></table></figure><p>执行命令后会生成<code>/source/your_page_name/index.md</code>。</p><p>接着在<code>theme/next/_condig.yml</code>找到<code>menu</code>属性设置路由即可。</p><p>如有必要，需要在<code>theme/next/languages</code>找到<code>menu</code>进行设置。</p><h3 id="创建新博客文章"><a href="#创建新博客文章" class="headerlink" title="创建新博客文章"></a>创建新博客文章</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建新的文章</span></span><br><span class="line">hexo new post your_post_name</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建新的草稿</span></span><br><span class="line">hexo new draft your_draft_name</span><br></pre></td></tr></table></figure><h2 id="博客主题的基本配置"><a href="#博客主题的基本配置" class="headerlink" title="博客主题的基本配置"></a>博客主题的基本配置</h2><p>博客基本信息修改位置为：<code>/_config.yml</code></p><h3 id="博客logo修改位置为："><a href="#博客logo修改位置为：" class="headerlink" title="博客logo修改位置为："></a>博客logo修改位置为：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/themes/next/source/images/apple-touch-icon-next.png;/themes/next/source/images/favicon-16*16-next.png;/themes/next/source/images/favicon-32*32-next.png;</span><br></pre></td></tr></table></figure><p>三个文件需要修改，缩放图片建议使用百度搜索的<strong>改图宝</strong>。</p><h3 id="博客大背景图片修改位置为："><a href="#博客大背景图片修改位置为：" class="headerlink" title="博客大背景图片修改位置为："></a>博客大背景图片修改位置为：</h3><p><code>/theme/next/source/css/_custom/custom.styl</code></p><h3 id="在右上角实现fork-me-on-github"><a href="#在右上角实现fork-me-on-github" class="headerlink" title="在右上角实现fork me on github:"></a>在右上角实现fork me on github:</h3><p>从<a href="https://github.com/blog/273-github-ribbons">这里</a>复制代码到<code>themes/next/layout/_layout.swig</code>文件中的<code>&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;</code>一句下面并修改<code>href</code>。</p><h3 id="点击动画特效："><a href="#点击动画特效：" class="headerlink" title="点击动画特效："></a>点击动画特效：</h3><p>修改位置为<code>themes/next/source/js/src/click-effect.js</code>，动画特效可以自己DIY,默认设置的特效为社会主义核心价值观。</p><h3 id="修改文章底部的那个带-号的标签为更漂亮的样式："><a href="#修改文章底部的那个带-号的标签为更漂亮的样式：" class="headerlink" title="修改文章底部的那个带#号的标签为更漂亮的样式："></a>修改文章底部的那个带#号的标签为更漂亮的样式：</h3><p>修改模板<code>/themes/next/layout/_macro/post.swig</code>，搜索<code>rel=&quot;tag&quot;&gt;#</code>，将<code>#</code>换成<code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code></p><h3 id="修改作者头像："><a href="#修改作者头像：" class="headerlink" title="修改作者头像："></a>修改作者头像：</h3><p><code>/themes/next/source/images/avater.jpg</code></p><h3 id="语言设置bug"><a href="#语言设置bug" class="headerlink" title="语言设置bug:"></a>语言设置bug:</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.在hexo 文件夹里主题 next文件夹里的language 文件夹 找到 zh-Hans.yml 重命名 zh-CN.yml</span><br><span class="line">2.在hexo 文件夹里的_config.yml 修改 language: zh-CN</span><br></pre></td></tr></table></figure><h3 id="文章加密访问："><a href="#文章加密访问：" class="headerlink" title="文章加密访问："></a>文章加密访问：</h3><p>打开<code>themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig</code>文件，在<code>rel=&quot;stylesheet&quot;</code>的下一行添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    (<span class="keyword">function</span> (<span class="params"></span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&#x27;&#123;&#123; page.password &#125;&#125;&#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="title function_">prompt</span>(<span class="string">&#x27;请输入文章密码&#x27;</span>) !== <span class="string">&#x27;&#123;&#123; page.password &#125;&#125;&#x27;</span>) &#123;</span><br><span class="line">                <span class="title function_">alert</span>(<span class="string">&#x27;密码错误！&#x27;</span>);</span><br><span class="line">                <span class="keyword">if</span> (history.<span class="property">length</span> === <span class="number">1</span>) &#123;</span><br><span class="line">                    location.<span class="title function_">replace</span>(<span class="string">&quot;http://xxxxxxx.xxx&quot;</span>); <span class="comment">// 这里替换成你的首页</span></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    history.<span class="title function_">back</span>();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><br>使用方法如下：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1ftorxsujudj20k00c6dgz.jpg" alt=""></p><h3 id="友情链接设置：在-themes-next-config-yml的Blog-rolls中这只链接即可"><a href="#友情链接设置：在-themes-next-config-yml的Blog-rolls中这只链接即可" class="headerlink" title="友情链接设置：在/themes/next/_config.yml的Blog rolls中这只链接即可"></a>友情链接设置：在<code>/themes/next/_config.yml</code>的Blog rolls中这只链接即可</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># <span class="title class_">Blog</span> rolls</span><br><span class="line"><span class="attr">links_icon</span>: link</span><br><span class="line"><span class="attr">links_title</span>: 友情链接</span><br><span class="line"># <span class="attr">links_layout</span>: block</span><br><span class="line"><span class="attr">links_layout</span>: inline</span><br><span class="line"><span class="attr">links</span>:</span><br><span class="line">  your links</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="自定义鼠标样式："><a href="#自定义鼠标样式：" class="headerlink" title="自定义鼠标样式："></a>自定义鼠标样式：</h3><p>在<code>/themes/next/source/css/_custom/custom.styl</code>中添加如下代码</p><p>鼠标样式的<code>ico</code>文件有需要的话可以<a href="http://www.rw-designer.com/cursor-library">来这里</a>自己DIY。</p><h3 id="博客添加打赏功能："><a href="#博客添加打赏功能：" class="headerlink" title="博客添加打赏功能："></a>博客添加打赏功能：</h3><p>在<code>/themes/next/_config.yml</code>中搜索reward并对收款码进行修改。</p><h2 id="第三方服务配置"><a href="#第三方服务配置" class="headerlink" title="第三方服务配置"></a>第三方服务配置</h2><h3 id="添加RSS"><a href="#添加RSS" class="headerlink" title="添加RSS:"></a>添加RSS:</h3><p>配置位置分别为<code>/_config.yml</code>与<code>/themes/next/_config.yml</code></p><h3 id="访客统计："><a href="#访客统计：" class="headerlink" title="访客统计："></a>访客统计：</h3><p>根据<a href="https://blog.csdn.net/ganzhilin520/article/details/79048021">教程</a>，在leancloud注册并新建应用，获取id和key后填写到<code>_config.yml</code>里<code>leancloud_visitors</code>的属性中。</p><h3 id="为博客添加宠物："><a href="#为博客添加宠物：" class="headerlink" title="为博客添加宠物："></a>为博客添加宠物：</h3><p>在终端输入<code>npm install -save hexo-helper-live2d</code>在<code>/themes/next/_config.yml</code>中添加代码,详情请见<a href="https://github.com/EYHN/hexo-helper-live2d/blob/61b581634a916cf4ce035c1685174cb2755264f3/README.zh-CN.md">这里</a></p><h3 id="博客在线显示pdf功能"><a href="#博客在线显示pdf功能" class="headerlink" title="博客在线显示pdf功能"></a>博客在线显示pdf功能</h3><p>首先安装插件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-pdf</span><br></pre></td></tr></table></figure><br>使用方法为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf http://your_pdf_link.pdf %&#125;</span><br><span class="line">&#123;% pdf ./your_pdf_**relative**_path %&#125;</span><br></pre></td></tr></table></figure></p><h3 id="修改博客的搜索功能："><a href="#修改博客的搜索功能：" class="headerlink" title="修改博客的搜索功能："></a>修改博客的搜索功能：</h3><p>建议使用algolia,因为swiftype已收费。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-algolia --save</span><br><span class="line">```    </span><br><span class="line">在`/themes/next/_config.yml`中搜索algolia,添加代码：</span><br><span class="line">``` js</span><br><span class="line">algolia:</span><br><span class="line">  applicationID: your_algolia_ud</span><br><span class="line">  apiKey: your_apiKey</span><br><span class="line">  indexName: your_index</span><br></pre></td></tr></table></figure></p><h3 id="文章置顶功能"><a href="#文章置顶功能" class="headerlink" title="文章置顶功能"></a>文章置顶功能</h3><p>安装第三方插件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br></pre></td></tr></table></figure><br>对需要置顶的文章添加属性即可<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo+GitHub博客搭建实战</span><br><span class="line">date: 2017-09-08 12:00:25</span><br><span class="line">categories: 博客搭建系列</span><br><span class="line">**top: true**</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><blockquote><p>参考与引用</p><ol><li><a href="https://blog.csdn.net/qq_33232071/article/details/51108062">https://blog.csdn.net/qq_33232071/article/details/51108062</a></li><li><a href="https://hexo.io/zh-cn/docs/setup.html">https://hexo.io/zh-cn/docs/setup.html</a></li><li><a href="http://mashirosorata.vicp.io/HEXO-NEXT%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE.html">http://mashirosorata.vicp.io/HEXO-NEXT%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE.html</a></li><li><a href="https://segmentfault.com/a/1190000009544924#articleHeader21">https://segmentfault.com/a/1190000009544924#articleHeader21</a></li><li><a href="https://github.com/shenzekun/shenzekun.github.io/blob/hexo/themes/next/layout/_layout.swig">https://github.com/shenzekun/shenzekun.github.io/blob/hexo/themes/next/layout/_layout.swig</a></li><li><a href="https://baike.baidu.com/item/%E7%A4%BE%E4%BC%9A%E4%B8%BB%E4%B9%89%E6%A0%B8%E5%BF%83%E4%BB%B7%E5%80%BC%E8%A7%82/3271832?fr=aladdin">https://baike.baidu.com/item/%E7%A4%BE%E4%BC%9A%E4%B8%BB%E4%B9%89%E6%A0%B8%E5%BF%83%E4%BB%B7%E5%80%BC%E8%A7%82/3271832?fr=aladdin</a></li><li><a href="https://blog.csdn.net/ganzhilin520/article/details/79048021">https://blog.csdn.net/ganzhilin520/article/details/79048021</a></li><li><a href="https://www.zhihu.com/question/41625825">https://www.zhihu.com/question/41625825</a></li><li><a href="https://www.jianshu.com/p/f5c184047e72">https://www.jianshu.com/p/f5c184047e72</a></li><li><a href="https://github.com/xiaweizi/BackupBlog/commit/b97173da33837604a31f2e5f78b17ba819306f74">https://github.com/xiaweizi/BackupBlog/commit/b97173da33837604a31f2e5f78b17ba819306f74</a></li><li><a href="https://blog.csdn.net/luzheqi/article/details/52798557">https://blog.csdn.net/luzheqi/article/details/52798557</a></li><li><a href="https://blog.csdn.net/qwerty200696/article/details/79010629">https://blog.csdn.net/qwerty200696/article/details/79010629</a></li><li><a href="https://blog.csdn.net/pop1586082213/article/details/54576131">https://blog.csdn.net/pop1586082213/article/details/54576131</a></li><li><a href="https://github.com/nodejs/node-v0.x-archive/issues/3911">https://github.com/nodejs/node-v0.x-archive/issues/3911</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo-next </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息科学原理手写笔记</title>
      <link href="/2018/07/18/infomation-science-handwriting/"/>
      <url>/2018/07/18/infomation-science-handwriting/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h2><div class="row"><iframe src="https://drive.google.com/file/d/1lli8ZJgdkfyIBUT6-CmVPeujpqpfpJGu/preview" style="width:100%; height:550px"></iframe></div><h2 id="题目参考"><a href="#题目参考" class="headerlink" title="题目参考"></a>题目参考</h2><div class="row"><iframe src="https://drive.google.com/file/d/1Gsc5yuhNsulK-0hRGNEEZ_0QumcebPQJ/preview" style="width:100%; height:550px"></iframe></div><h2 id="题目索引"><a href="#题目索引" class="headerlink" title="题目索引"></a>题目索引</h2><div class="row"><iframe src="https://drive.google.com/file/d/1HaiyMuEi_VjEyKDH3KEchJMT1TEusPio/preview" style="width:100%; height:550px"></iframe></div>]]></content>
      
      
      <categories>
          
          <category> HandWriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> informationScience </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模式识别与机器学习手写笔记</title>
      <link href="/2018/07/10/PRML-handwriting/"/>
      <url>/2018/07/10/PRML-handwriting/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h2><div class="row"><iframe src="https://drive.google.com/file/d/145ag3TfrL6bY222mopcvCnCedipimPdu/preview" style="width:100%; height:550px"></iframe></div><p>##考试大抄<br><div class="row"><iframe src="https://drive.google.com/file/d/1N8rNwzZp8jq2IvkrDh4LjS0E8S_dpCdM/preview" style="width:100%; height:550px"></iframe></div></p>]]></content>
      
      
      <categories>
          
          <category> HandWriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习-DavidSilver(Lecture1 to Lecture3)</title>
      <link href="/2018/06/30/RL-DavidSilver-1-3/"/>
      <url>/2018/06/30/RL-DavidSilver-1-3/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>本科毕业设计的强化学习笔记。许多强化学习的术语使用中文表达容易产生歧义，因此本笔记使用英文。 <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">David Silver强化学习课程视频网址</a><br><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver强化学习课程课件</a>。</p></blockquote><h2 id="Lecture-One"><a href="#Lecture-One" class="headerlink" title="Lecture One"></a>Lecture One</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>all is about decision</li><li>no supervisor,only <strong>reward</strong> signal(not supervisor learning)</li><li>feedback is delayed,not instantaneous</li><li>the key role:<strong>agent</strong>(brain)</li></ul><h3 id="The-process"><a href="#The-process" class="headerlink" title="The process"></a>The process</h3><ul><li>background:environment(earth) input: observation &amp; reward –&gt; output:action</li><li>history and state:<br>History:  $$H_t=A_1,O_1,A_2,O_2…A_t,O_t,R_t$$<br>State of agent :    $$S_t^a=f(H_t)$$<br>State of environment:    $$S_t=f(H_t)$$</li></ul><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstg5bizsrj20xg0ictf8.jpg" alt=""></p><h3 id="keys-of-fundamental-assumptions"><a href="#keys-of-fundamental-assumptions" class="headerlink" title="keys of fundamental assumptions"></a>keys of fundamental assumptions</h3><ol><li>Markov state</li><li>agent may conclude</li></ol><ul><li>policy:to decide agent’s function towards current state</li><li>value:predictive reward of agent’s action</li><li>model:a metaphysical regulation of “world”(<strong>state</strong>&amp;<strong>reward</strong>)</li></ul><ol><li>exploitation(choose old) and exploration(choose new)</li></ol><h3 id="The-classification-of-agent"><a href="#The-classification-of-agent" class="headerlink" title="The classification of agent"></a>The classification of agent</h3><ul><li>Value Based: only need $max(V)$</li><li>Policy Based: get action directly by state</li><li>AC(Actor Critic):act(i.e. policy), critic(i.e. value function)</li></ul><h3 id="let-us-review-immediately"><a href="#let-us-review-immediately" class="headerlink" title="let us review immediately"></a>let us review immediately</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstg3rvl2wj21yu2kz7wh.jpg" alt=""></p><hr><h2 id="Lecture-Two"><a href="#Lecture-Two" class="headerlink" title="Lecture Two"></a>Lecture Two</h2><h3 id="introduction-to-MDP-markov-decision-process"><a href="#introduction-to-MDP-markov-decision-process" class="headerlink" title="introduction to MDP(markov decision process)"></a>introduction to MDP(markov decision process)</h3><blockquote><p>almost all RL problems can be formalised as MDPs</p></blockquote><h3 id="Markov-Property-and-Markov-Process"><a href="#Markov-Property-and-Markov-Process" class="headerlink" title="Markov Property and Markov Process"></a>Markov Property and Markov Process</h3><blockquote><p>the future is independent of the past given the present</p></blockquote><h4 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstg8lkc95j20x607mq3j.jpg" alt=""></p><h4 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstg9h4fybj20ww08gjst.jpg" alt=""></p><h3 id="State-Transition-Matrix"><a href="#State-Transition-Matrix" class="headerlink" title="State Transition Matrix"></a>State Transition Matrix</h3><blockquote><p>the state transition probability for a Markov state and successor state</p></blockquote><h3 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h3><blockquote><p>A Markov reward process is a Markov chain with values.(State,Probability,Reward,Discount factor-gamma)</p></blockquote><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fse949pfxej21410f1di6.jpg" alt=""></p><h4 id="return-G"><a href="#return-G" class="headerlink" title="return(G)"></a>return(<strong>G</strong>)</h4><blockquote><p>the return G is the total discounted reward from time-step t. </p></blockquote><p>$$G<em>t = R</em>{t+1}+{\gamma}R<em>{t+2}+ {\gamma} ^2R</em>{t+3}…+{\gamma}^kR_{t+k+1}$$<br>　we could figure out from equation above if discount factor closes to 0 the return will be “myopic”, if discount factor nevertheless closes to 1 then the return will be “far-sighted”</p><h4 id="Value-Function-Expectation"><a href="#Value-Function-Expectation" class="headerlink" title="Value Function:Expectation"></a>Value Function:<strong>Expectation</strong></h4><blockquote><p>the value function v(s) gives the long-term value of state s.</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgan5dxdj20xa08qjsh.jpg" alt=""></p><p>then we could deduce MRP Bellman Equation below:</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgbhlcxoj211s0a4t9x.jpg" alt=""></p><p>Reward only relates to state,therefore Bellman Equation can be decomposition said by:<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgc2frlqj211i0hc3zu.jpg" alt=""></p><h4 id="Bellman-Equation-for-MRPs-in-matrix"><a href="#Bellman-Equation-for-MRPs-in-matrix" class="headerlink" title="Bellman Equation for MRPs in matrix:"></a>Bellman Equation for MRPs in matrix:</h4><p>The bellman equation can be expressed concisely using matrics:</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgoq6yy1j211k0j0wg3.jpg" alt=""></p><p>The bellman equation is a kind of linear equation so we could solve it directly:</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgfys2wej212w0aimxx.jpg" alt=""></p><p>But time complexity is close to $O(n^3)$,we could solve it by iteration methods:</p><ul><li>Dynamic Programming</li><li>Monto Carlo evaluation</li><li>Temporal-Difference(TD) Learning</li></ul><p>Let us see some other concepts for preparing!</p><h3 id="Markov-Decision-Processes"><a href="#Markov-Decision-Processes" class="headerlink" title="Markov Decision Processes"></a>Markov Decision Processes</h3><blockquote><p>Markov Decison Processes (S,A,P,R,$\gamma$) is a Markov Reward Processed with decisons.It is an environment in which all states are Markov</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgr8o1hsj20q40b6gng.jpg" alt=""></p><h4 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h4><blockquote><p>A policy $\pi$ is a distribution over actions given states.</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgruhsrtj20bs028q2u.jpg" alt=""></p><p>for MDP, we must calculate state-value function and action-value function, we have definition below:<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgsgz1gaj20pu0esdhw.jpg" alt=""></p><p>just like bellman equation, we could deduce bell expectation equation from state-value function and action-value function:</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fstgtf593bj20r60di0tx.jpg" alt=""></p><p>we finally have different Bellman Expectation Equation for $V^{\pi}$,$Q^{\pi}$ and the Matrix Form is：</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fsthdw3ihrj20ow0diaav.jpg" alt=""></p><h4 id="Optimal-Value-Functions"><a href="#Optimal-Value-Functions" class="headerlink" title="Optimal Value Functions"></a>Optimal Value Functions</h4><p>Coming question, how could we judge the performance of our policy?</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fseb9v3uaxj20rm0dl3zx.jpg" alt=""></p><h4 id="Optimal-Bellman-Equation"><a href="#Optimal-Bellman-Equation" class="headerlink" title="Optimal Bellman Equation"></a>Optimal Bellman Equation</h4><p>we could deduce the Optimal Bellman Eqution from above first:</p><p align="center"><br>    <img with="400px" height="400px"    src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsthiix1lcj20h00dmdgm.jpg"><br></p><p align="center"><br>    <img with="300px" height="300px"    src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsthirugdzj20lo0coq3t.jpg"><br></p><p>we could solve the equation by:</p><ul><li>Value iteration</li><li>Police iteration</li><li>Q-Learning</li><li>Sarsa</li></ul><h3 id="extension-of-MDP"><a href="#extension-of-MDP" class="headerlink" title="extension of MDP"></a>extension of MDP</h3><ul><li>infinite MDPS</li><li>Reductions of POMDP’s</li></ul><h4 id="infinite-MDPs"><a href="#infinite-MDPs" class="headerlink" title="infinite MDPs"></a>infinite MDPs</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fsthjqcbhoj20p80gq767.jpg" alt=""></p><h4 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP:"></a>POMDP:</h4><p>It could be seen as a Hidden Markov Process adding actions!</p><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsebmimapgj21180lejux.jpg" alt=""></p><h3 id="Let-us-review-immediately"><a href="#Let-us-review-immediately" class="headerlink" title="Let us review immediately!"></a>Let us review immediately!</h3><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsthrducjcj21yu2kz4r0.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsthqqzx45j21yu2kz1l8.jpg" alt=""></p><hr><h2 id="Lecture-3-Planning-by-Dynamic-Programming"><a href="#Lecture-3-Planning-by-Dynamic-Programming" class="headerlink" title="Lecture 3 : Planning by Dynamic Programming"></a>Lecture 3 : Planning by Dynamic Programming</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><strong>Dynamic</strong> sequential or temporal component to the problem</li><li><strong>Programming</strong> is a optimision of question!<ul><li>optimal substructure</li><li>overlapping subproblems</li></ul></li></ul><p>&emsp;&emsp;There are two applications of DP</p><ol><li><p>for prediction:</p><ul><li>Input：MDP &amp; policy $\pi$</li><li>Output: value function $v_\pi$</li></ul></li><li><p>for control</p><ul><li>Input: MDP</li><li>Output: optimal value function $v<em>*$ &amp; optimal policy $\pi</em>*$</li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fsthz8kmyjj21yu2kzb29.jpg" alt=""></p><h3 id="Iterative-Policy-Evaluation"><a href="#Iterative-Policy-Evaluation" class="headerlink" title="Iterative Policy Evaluation"></a>Iterative Policy Evaluation</h3><p>How to evaluate $\pi$?</p><ul><li>solition:iterative application of bellman expectation backup to get the true value function($V<em>0-&gt;V</em>\pi$)</li><li>from end to start by iteration.</li></ul><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst51czfmaj20pm0ewmy8.jpg" alt=""></p><p>&emsp;&emsp;It is just like a weighted average of every probability of each action.</p><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst8a8p85fj20om0kutb7.jpg" alt=""></p><p>the detail you could see the manuscript!</p><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RL5.jpg" alt=""></p><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration:"></a>Policy Iteration:</h3><ol><li>evaluate the policy $\pi$：fill the maze with number to get $v<em>\pi$<br>$$V</em>\pi(s) = E[R<em>{t+1}+\gamma R</em>{t+2}+…|S_t = s]$$</li><li>improve the policy by acting greedy with respect to $v<em>\pi$<br>$$\pi^{‘} = greedy(v</em>\pi) $$</li></ol><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst59a5cd0j20vr0fn42a.jpg" alt=""></p><ol><li>Policy improvement:</li></ol><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst9t2rt9bj20vi0kmtc8.jpg" alt=""></p><p>the details you could see the manuscript!<br><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RL5.jpg" alt=""></p><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p>The solution v∗(s) can be found by one-step lookahead, and it start with final rewards and work backwards:<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst5i9k9fzj20vr04ljrr.jpg" alt=""></p><p>There is an example:<br><img src="leanote://file/getImage?fileId=5b373da1afc5ce605c000001" alt=""></p><p>the details you could see the manuscript!</p><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fst5j0ochmj20vo0fcmyf.jpg" alt=""></p><h3 id="Synchronous-Dynamic-Programming-Algorithms"><a href="#Synchronous-Dynamic-Programming-Algorithms" class="headerlink" title="Synchronous Dynamic Programming Algorithms"></a>Synchronous Dynamic Programming Algorithms</h3><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fstbt3m1pkj20wg0aiq4p.jpg" alt=""></p><p>we could see Iterative Policy Evaluation and Policy Iteration as a whole knowledge. The knowledge is all in consideration of policy.They as the same in essence.</p><h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><ul><li>In-Place Dynamic Programming</li></ul><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fstet5a8koj20w60iaacj.jpg" alt=""></p><p>the main difference between two methods above is the number of copy for reducing storage.</p><ul><li>Prioritised sweeping</li></ul><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fstevcscvuj20w00eowgs.jpg" alt=""></p><ul><li>Real-time dynamic programming</li></ul><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fsteyfhrxoj20rm0cswg6.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reinforcementLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2018/06/29/CNN-note/"/>
      <url>/2018/06/29/CNN-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="视觉感知"><a href="#视觉感知" class="headerlink" title="视觉感知"></a>视觉感知</h2><blockquote><p>相同的图片经过不同的视觉系统，会得到适合自己生存环境的感知。</p></blockquote><p>不同的观察角度，决定了图片的识别结果。</p><h2 id="图像表达"><a href="#图像表达" class="headerlink" title="图像表达"></a>图像表达</h2><p>画面识别的输入$x$是形状为（width, height, depth）的三维张量。其中每一个（width，height）的矩阵称为一个channel。</p><blockquote><p>画面不变性：一个物体在channel中的位置不应该影响对该物体的识别结果。</p></blockquote><h2 id="为什么前馈神经网络不能完成任务？"><a href="#为什么前馈神经网络不能完成任务？" class="headerlink" title="为什么前馈神经网络不能完成任务？"></a>为什么前馈神经网络不能完成任务？</h2><p>输入图片是一个三维张量，但是很明显前馈神经网络很难识别不同位置的“相同”样本。<br>也即：应当使前馈神经网络在识别图片中的物体时，即使物体在不同位置也能顺利识别出来。<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsrxt7affrj20k00ce0z1.jpg" alt=""></p><h2 id="卷积神经网络：让权重在不同位置共享的神经网络"><a href="#卷积神经网络：让权重在不同位置共享的神经网络" class="headerlink" title="卷积神经网络：让权重在不同位置共享的神经网络"></a>卷积神经网络：让权重在不同位置共享的神经网络</h2><p>卷积神经网络的最基本的操作：</p><ol><li>卷积</li><li>非线性（ReLu）</li><li>Pooling池化</li><li>分类（全连接层）</li></ol><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><ul><li><p>使用局部区域去扫描整张图片<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsrzmkrrypj20e90ea43u.jpg" alt=""><br>其中：红框表示<strong>filter</strong>或者<strong>kernel</strong>,隐藏层节点为kernel的线性组合。<br>那么，隐藏层节点$y_0$的表达式即为：<br>$$ y_0=x_0<em>w_1+x_1</em>w_2+x4<em>w_3+x_5</em>w_4+b_0 $$</p></li><li><p>空间共享<br>不同的区域共享同一个“权重矩阵”和偏移量$b_0$。</p></li><li><p>矩阵形式的输出表达：<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fss08p95qjj20e90eawjr.jpg" alt=""><br>经过一次feature detector后的隐藏层就可以看做是“卷积”的特征。</p></li><li><p>处理Depth这一个维度:将三个channel看成三组不同的权重矩阵<br>特别的，针对$$ 2\cdot2\cdot3 $$(RGB)的kernel有：<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fss0q4vy7kj207t01ajr8.jpg" alt=""><br>也就是说，<strong>depth维是以被贯穿的方式处理的</strong>。<br>在实践中，Depth的数值与filter的个数相同。</p></li><li><p>步长（Stride）：<br>这个参数决定了filter滑动一次所跨越的像素数量。本文的例子中步长均为1。</p></li><li><p>Zero padding<br>为了保证卷积后的图片尺寸不变，则需要在最外一层填充0，</p></li><li><p>多filters<br>不同的filters对同一个图像抓取到的Feature Maps也会不同。<br>每一个不同的filter代表了每一个不同的操作，下图就展示了filter对用一张图片不同的处理结果。</p></li></ul><p align="center"><br>  <img width="60%" height="60%" src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fss2yuhw9dj20ia0u2485.jpg"><br></p><h3 id="非线性（ReLu）"><a href="#非线性（ReLu）" class="headerlink" title="非线性（ReLu）"></a>非线性（ReLu）</h3><p>回顾一下老朋友ReLU:Rectified Linear Unit<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fss3hexqnoj20tr09c3zw.jpg" alt=""><br>这个环节的主要作用就是把一些值为负数的像素点全部设置为0，</p><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fss42edfvbj215k0fkjzp.jpg" alt=""></p><h3 id="Pooling池化"><a href="#Pooling池化" class="headerlink" title="Pooling池化"></a>Pooling池化</h3><ul><li>Max pooling<br>整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出output。那么为什么要有这个环节？<blockquote><p>卷积后的Feature Map中有对于识别物体不必要的冗余信息。 </p></blockquote></li></ul><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fss45xe26bj20rg0nedlu.jpg" alt=""></p><p>&emsp;&emsp;注：上图的步长设置为1</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。</p><h2 id="小结：卷积神经网络的训练过程"><a href="#小结：卷积神经网络的训练过程" class="headerlink" title="小结：卷积神经网络的训练过程"></a>小结：卷积神经网络的训练过程</h2><ol><li>随机初始化所有的filter、参数、权重</li><li>将训练图像作为输入，通过卷积-&gt;ReLu-&gt;池化（pooling）-&gt;全连接层的过程找到每个分类的概率。</li><li>在输出层计算总误差。</li><li>使用梯度下降反向传播更新所有filter、参数、权重的值。其中，权重按它们所占误差的比例更新；</li></ol><hr><h2 id="再用一个例子查漏补缺"><a href="#再用一个例子查漏补缺" class="headerlink" title="再用一个例子查漏补缺"></a>再用一个例子查漏补缺</h2><p><a href="http://scs.ryerson.ca/~aharley/vis/conv/flat.html">可视化CNN游戏，强烈建议尝试一下！</a></p><p>游戏截图如下：<br>一副图片由1024（$32*32$）个像素组成。<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fss7xncql9j21q0172x6p.jpg" alt=""></p><p>第一个卷积层由6个$5*5$并且步长设置为1的filter生成，我们可以形象的理解为它的depth维度为6。注：下图将ReLu环节与卷积环节合二为一了，请读者心中有数。</p><p align="center"><br>  <img width="200px" height="400px" src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fss7zpo0cxj20go0vmdne.jpg"><br><br></p><p>紧接着针对每一个feature map使用$2*2$并且步长为2的max pooling。可以看到Pooling层的每一个像素对应Conv层的四个像素。</p><p align="center"><br>    <img with="200" height="400px"    src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssbu4uyuwj20ek0gwn0t.jpg"><br></p><p>接下来是最难理解的第二次的卷积层和max pooling层的环节：<br>首先，观察第一层max pooling到第二层卷积的个数，为什么8个feature map经过filter的f卷积之后变成了16个feature map？<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssctu1m8lj219y0620yl.jpg" alt=""></p><p>为了寻找答案，我们来观察下第二层feature maps的filter都是长成什么样的：<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssd06wljtj210w0k67fd.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssd0qrdugj21aq0kek7e.jpg" alt=""></p><p>请大家仔细观察上图：我们会发现，第二层卷积层的filter的形状和所选择的第一层max pooling的feature map息息相关！也就是说，我们在做第二层的卷积时，只考察了第一层的局部特征！</p><p>第二层的max pooling与第一层同理，我就不再赘述了。</p><p>之后是全连接层，全连接层利用了所有的特征：</p><p><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssd8ndkubj21cg02yn25.jpg" alt=""></p><p>全连接层的全景图如下：<br><img src="https://ws1.sinaimg.cn/mw690/ca26ff18ly1fssdbxdtm0j21oq0c2gzj.jpg" alt=""></p><blockquote><p>参考与引用</p><ol><li><a href="https://zhuanlan.zhihu.com/p/27642620">https://zhuanlan.zhihu.com/p/27642620</a></li><li><a href="https://cs231n.github.io/convolutional-networks/">https://cs231n.github.io/convolutional-networks/</a></li><li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></li><li><a href="http://scs.ryerson.ca/~aharley/vis/conv/flat.html">http://scs.ryerson.ca/~aharley/vis/conv/flat.html</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> DeepLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人类简史与未来简史</title>
      <link href="/2018/06/27/some-great-books-note/"/>
      <url>/2018/06/27/some-great-books-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="人类简史"><a href="#人类简史" class="headerlink" title="人类简史"></a>人类简史</h2><h3 id="以问题为导向阅读"><a href="#以问题为导向阅读" class="headerlink" title="以问题为导向阅读"></a>以问题为导向阅读</h3><h4 id="问题1：人类如何从不相关的物种变成了地球的主宰者？"><a href="#问题1：人类如何从不相关的物种变成了地球的主宰者？" class="headerlink" title="问题1：人类如何从不相关的物种变成了地球的主宰者？"></a>问题1：人类如何从不相关的物种变成了地球的主宰者？</h4><p>&emsp;答案：集体层面上，人类可以<em>灵活</em>地进行<em>大型合作</em>。其中，灵活相对于地球上的其他物种，例如：蚁群可以高效的合作，但是它们不会策划去推翻蚁后的统治。大型合作代表一种在信任机制，即陌生人“绑定”之间的契约。</p><p>&emsp;列举一些只会出现在人类这个物种上的合作机制：</p><ul><li>屠宰场</li><li>监狱</li><li>集中营</li></ul><p>&emsp;启发：人类之所以可以进行灵活地大型合作，我认为一部分要归功于人类的自我意识。而人工意识真的是当前人工智能的发展未来吗？人类进化成为“有意识”的物种花费了数百万年的时间。目前计算机接口并不具备像人类一样与现实世界进行交互，通过自然选择筛选基因的外部条件。因此人工智能也许会跳过意识这一环节，成为无意识的超级智能。再或者，计算机通过其特有的接口“互联网”，直接进化出超级意识。</p><h4 id="问题2：人类为何能建立前所未有的合作机制？"><a href="#问题2：人类为何能建立前所未有的合作机制？" class="headerlink" title="问题2：人类为何能建立前所未有的合作机制？"></a>问题2：人类为何能建立前所未有的合作机制？</h4><p>&emsp;答案：想象力：人类可以创造虚幻或者虚幻的故事。只要人们广泛“相信”/“信任”这些故事，大家便会遵循相同的规定、准则、价值观。</p><ul><li>动物仅仅用语言描述现实世界，而人类使用语言创造新的（虚构的）现实。例如：上帝、人权、国家、民族、公司、货币。</li></ul><p>&emsp;启发：正在飞速发展的AI恰恰会有相同形式机制，比如医疗领域，人类医生有数千万，但是他们参差不齐的水平和文化背景经常会导致误诊。而共享单一医疗网络的AI医生将会共享同一个“准则”，第一时间可以获取到最新具有抗药性的病毒，这无疑对医疗来说是巨大的进步。</p><h3 id="2-按照时间顺序阅读"><a href="#2-按照时间顺序阅读" class="headerlink" title="2. 按照时间顺序阅读"></a>2. 按照时间顺序阅读</h3><ol><li>认知革命：语言沟通进行合作。</li><li>农业革命：驯化地球上的其他物种；使用文字进行记录。</li><li>科学革命：人类发现自己的“无知”，进而不断探索未知的领域。</li><li>？？革命：？？</li></ol><p>&emsp;启发：人类会无意识的进入一个全新的时代。在时代的转换过程中阶层流动将会变得十分迅速，而每次转换都会让资源集中在更少的人手中。越来越多的权力由人类涌向算法，最后十分有可能是一个无意识的智能体控制这个星球。</p><h2 id="未来简史"><a href="#未来简史" class="headerlink" title="未来简史"></a>未来简史</h2><h3 id="观点导向阅读"><a href="#观点导向阅读" class="headerlink" title="观点导向阅读"></a>观点导向阅读</h3><ol><li><p>人类一直在追求“永生”，而计算机（硅基生命）天生“永生”。<br><br>启发：按照进化论的观点，通过不断地“死亡”来筛选适合生存的突变基因，碳基生命完成了进化。相反，硅基生命则不需要通过这种方式完成进化。那么我们需不需要编码一个“原始”的硅基生命体（含有基本代码片的单片机）使其模拟人类进化的过程呢？</p></li><li><p>科技革命产生了人文主义宗教。<br><br>科学，本身成为了一种宗教。“圣经“们打下了有神论宗教的基础。”牛顿与苹果“，”马尔科夫假设“，”神经网络“，成为了计算机乃至科学的基石。我常常恐惧于：如果有一天计算机科学的基石崩塌，这门学科将会向</p></li><li><p>人文主义将会面临彻底的失败。<br><br>人文主义的基本假设是：人是有自我意识的。但是神经科学家告诉我们：人的意识仅仅是神经信号，并不比地球上的其他物种高明多少，也没有21克的”灵魂“存在。未来数据主义十分有可能成为新的主流信仰，即全世界的低级碳基生命伴随着高级硅基生命一起进化，最终变成一个巨大的综合生命体。</p></li></ol><h3 id="章节顺序阅读"><a href="#章节顺序阅读" class="headerlink" title="章节顺序阅读"></a>章节顺序阅读</h3><ul><li>幸福：幸福感的本质是神经元之间传递的电化学反应信号，或许真的有一串代码可以传递一个”幸福矩阵“于神经网络之中呢？</li><li>直觉：生物自有的算法（适者生存）已经保证了，地球上的生物最起码是局部最优的。而人类决策算法中最重要的”直觉“或者说”无意识部分“，需要进行巧妙地编码。</li><li>意识：意识本质上是个体的退化与集体的进化。自由意志本身也只是一个算法。那么首先，如何让冷冰冰的代码具有主观能动性呢？是需要给他们编码一个最基本的生存动机？还是其他方式呢？</li><li>左右脑的启发：当前最为火爆的GAN(对抗神经网络)模型，可以类比人类的左右脑。因为脑科学实验表明，左脑和右脑本来就是两套不同的决策体系，它们通过神经纤维连接了起来，最终作出一致的决策。因此，从神经元到深度神经网络到对抗式神经网络，人类不经意间走着仿生学的”正确“道路。一步一步实现人工智能。（此处可以添加GAN相关论文成果）</li></ul><h3 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h3><ul><li>当前世界的时间节点，无论是中国倡导的“经济全球化”，还是美国倡导的“美国优先”，都是在讲一个故事，创造一种幻觉。</li><li>pytorch, tensorflow最终会变成一个超级平台，规范一切关于人工智能的进化过程。C语言就像是现实世界的物理定律，底层硬件就像是现实世界中的实体。</li><li>人类只是简单地算法，人工智能需要一个本领域的”质能方程“。 </li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aBriefHistoryOfHumankind </tag>
            
            <tag> homoDeus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习设计理念手写笔记</title>
      <link href="/2018/06/16/YJango-Deep-Learning/"/>
      <url>/2018/06/16/YJango-Deep-Learning/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><div class="row"><iframe src="https://drive.google.com/file/d/1cski1XD1gfZauuDb-TOw8HqxmGixbkoO/preview" style="width:100%; height:550px"></iframe></div>]]></content>
      
      
      <categories>
          
          <category> HandWriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超智能体读书笔记</title>
      <link href="/2018/06/15/super-organism-YJango/"/>
      <url>/2018/06/15/super-organism-YJango/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>版权说明：<br>本文仅记录在阅读 于建国（YJango）博士的《超智能体》一书过程中的笔记。本文作者已通过邮件联系作者，获得授权。</p></blockquote><h2 id="智能的本质"><a href="#智能的本质" class="headerlink" title="智能的本质"></a>智能的本质</h2><p>　智能起源于随机性（熵）：随着时间的推移，孤立系统会自发朝向最大熵状态演化[不去刻意整理的宿舍会越来越乱]。</p><blockquote><p>智能：根据环境变化做出相应变化的能力，即熵减的能力[减少“不确定性”]</p></blockquote><p>　想要探究智能，我们必须具备正确描述世界状态和不同时间下的状态变化。线性代数则给了我们答案。</p><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><blockquote><p>线性代数：有关任意维度空间下事物<strong>状态</strong>和<strong>变化状态</strong>的规则</p></blockquote><h4 id="矩阵的本质：存储状态（静态）或变化（动态）的信息"><a href="#矩阵的本质：存储状态（静态）或变化（动态）的信息" class="headerlink" title="矩阵的本质：存储状态（静态）或变化（动态）的信息"></a>矩阵的本质：存储状态（静态）<strong>或</strong>变化（动态）的信息</h4><ol><li>矩阵的静态信息：<br>　向量可以描述一个事物的状态，许多具有相同维度的向量的有序排列构成了矩阵。<ul><li>关于张量：多个标量有序排列后形成向量，多个向量有序排列后形成矩阵，多个矩阵有序排列后形成三维张量（3D tensor）。</li></ul></li><li><p>矩阵的动态信息：<br>　此时矩阵可以看做是多个维度相同的权重的有序排列，并且可以对另一个矩阵的静态信息进行批量变化。这便是矩阵乘法的本质。</p><blockquote><p>两个矩阵相乘，一个矩阵提供状态信息，另个矩阵提供变化信息</p></blockquote></li><li><p>向量空间：能够容纳所有线性组合的状态集合。</p><ul><li>向量空间一定在各个维度可以无限延伸（因为实数域无限）</li><li>子空间：子空间内的向量空间<ul><li>最小的子空间：0</li><li>空集不可以是向量空间</li></ul></li></ul></li><li><p>线性变换：<br> 矩阵乘以矩阵可以视作一个矩阵内部向量的批量线性变换（lineartransformation）。方便理解起见可以仅讨论由矩阵乘以向量所形成的一次线性变换。直接上图：<br> <img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/superorganism-1.png" alt=""></p></li><li><p>线性变化：不同维度空间下的向量组的投影。例如$y<em>{2*1}=A</em>{2<em>3}x_{3</em>1}$便是将三维向量$x$经过线性变换后变成二维空间的向量$y$。<br>注：神经网络的核心 $$ y=a(Ax+b)$$ </p></li></ol><h4 id="维度的扩展"><a href="#维度的扩展" class="headerlink" title="维度的扩展"></a>维度的扩展</h4><p><strong>思维空间</strong>：人们认为自己拥有自由的意识和思维。然而这种自由也是有限的。它好比线性空间里的张成,能张成多大的意识空间<strong>取决于脑中有多少互不相关的因素</strong>,也就是维度（秩）。</p><p>　维度的作用：</p><ul><li>复数的理解:进一步扩展的数的域。</li><li>傅里叶变换的理解:在x-y坐标系上增加1维时,一切豁然开朗。</li><li><p>弦理论的理解:尝试融合相对论和量子力学的理论,但只有当扩充到10维空间+1维时间时,数学公式才合理。</p></li><li><p><strong>弦理论</strong>：</p><ul><li>1.问题的起源：<ul><li>宇宙也许存在高维度的空间</li><li>构成世界最基础的成分是什么？</li></ul></li><li>2.物质的构成：分子-&gt;原子-&gt;质子、中子-&gt;？？？<br>  可能是不断<strong>跳动</strong>的能量<strong>线条</strong>：宇宙万物的一切皆源于此。</li><li>3.上述理论的数学证明之后在十维空间和一维时间的情况下才成立。那么我们的宇宙的确可能存在高纬度的空间</li><li>4.新的问题：当我们观测一个宇宙的状态时，我们确定了20个数值（粒子的质量，重力场的强度，…），并且如果这20个数中的任何一个数有所变化，我们的宇宙将不复存在。那么这20个数值是因为什么而确定的呢？<br>  也许是更高纬度的空间</li><li><ol><li>通过实验证明高纬度空间的存在：<br>欧洲大型强子对撞机通过将质子加速对撞，观测：如果对撞后的能量有所损失，则可能是因为对撞的一部分“残骸”进入到了高纬度空间！</li></ol></li></ul></li></ul><p>小结：<strong>当问题无法被理解时，往往是因为找错的地方，不妨尝试扩展维度，增加搜索空间。</strong>然而由于信息量的限制，很多事物无法确定变化后的状态，因此需要概率为我们提供依据。</p><h3 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h3><blockquote><p>概率是用来衡量事物在跨时间后的不同状态的确信度</p></blockquote><h3 id="熵与生命"><a href="#熵与生命" class="headerlink" title="熵与生命"></a>熵与生命</h3><blockquote><p>生命活着就在减熵：利用信息压缩（或者说抽象）后形成的知识，对抗熵增！</p></blockquote><p>　智能的条件</p><ul><li>智能LV1:从环境到行动的关联能力[生存]（植物&amp;微生物）</li><li>智能LV2:利用过去到未来的关联能力[预测]（动物）</li><li>智能的实现：通过存储关联的材料（遗传物质）</li></ul><hr><h2 id="自然智能"><a href="#自然智能" class="headerlink" title="自然智能"></a>自然智能</h2><h3 id="RNA与DNA（智能LV1）"><a href="#RNA与DNA（智能LV1）" class="headerlink" title="RNA与DNA（智能LV1）"></a>RNA与DNA（智能LV1）</h3><ul><li>识别：DNA上的信息：蛋白质合成。</li><li>学习：繁衍，变异，筛选。</li><li>进化：<strong>进化是以种群为单位的被动过程</strong><br>　问题：在<strong>《未来简史》</strong>中，作者描述人类将打破自然选择的进化理论，转而通过基因改造技术“主动”进化。</li><li>永生的缺陷：永生者失去了为种群提供差异性的筛选功能。</li></ul><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><ol><li>神经元的本质行为：<br>$$y=a(Wx+b),其中x是输入信号，y是输出信号$$</li></ol><hr><h2 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h2><h3 id="梯度下降的问题"><a href="#梯度下降的问题" class="headerlink" title="梯度下降的问题"></a>梯度下降的问题</h3><ul><li>局部最小值（鞍点）的解决方案：<ul><li>随机梯度下降：每次只更新一个样本所计算的梯度</li><li>小批量梯度下降：每次更新若干样本所计算梯度的平均值</li><li>.etc</li></ul></li></ul><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><blockquote><p>神经网络不缺少新结构，但缺少一个该领域的$E=mc^2$</p></blockquote><ol><li>为什么神经网络高效：并行的先验知识使得模型可用线性级数量的样本学习指数级数量<br>的变体：</li><li>学习的本质是什么：将变体拆分成因素和知识（Disentangle Factors of Variation）</li><li>为什么深层神经网络比浅层神经网络更高效：</li><li>神经网络在什么问题上不具备优势：<ul><li>非函数问题：需要想办法将问题转化为函数问题</li><li>非迭代：该层的状态不是由上层状态构成的任务</li></ul></li></ol><h3 id="深度学习计算机实现平台：tensorflow"><a href="#深度学习计算机实现平台：tensorflow" class="headerlink" title="深度学习计算机实现平台：tensorflow"></a>深度学习计算机实现平台：tensorflow</h3><p>Tensorflow基本用法</p><ol><li>准备阶段：组装计算图<ul><li>计算图<code>想象成一个管道</code>：需要组装的结构，由许多操作组成</li><li>操作<code>想象成不通管道分支的连接处</code>：对零个或多个数据进行输入与输出</li><li>数据类型：1.张量（tensor）2.变量（variable）3.常量（constant）<ul><li>张量<code>想象成管道中的液体</code>：多维度array或者list<br>  tener_name = tf.placeholder(type, shape, name)</li><li>变量：在同一时刻对图中所有其他操作都保持静态的数据（管道中的阀门）<br>  name_variable = tf.Variable(value, name)</li><li>常量：无需初始化的变量<br>  name_constant = tf.constant(value)</li></ul></li></ul></li><li>执行阶段：使用计算图<ul><li>执行语句: sess.run(op)</li><li>送值（feed）: 输入操作的输入值（输入液体）<br>  sess.run([output], feed_dict={input1:value1, input2:value2})</li><li>取值（fetch）: 获取操作的输出值（得到液体）<br>  sess.run(one op)<br>  sess.run([a list of op])</li></ul></li></ol><h3 id="DEMO部分："><a href="#DEMO部分：" class="headerlink" title="DEMO部分："></a>DEMO部分：</h3><p>YJango博士的部分demo由于tensorflow版本问题可能无法运行，我将部分代码已经“修正”并已上传至github，本地环境测试均可通过，请大家放心使用。<br><a href="https://github.com/824zzy/TutorialTensorflow">代码部分的笔记</a></p><blockquote><p>参考与引用: 1. 《超智能体》 于建国（Yjango）博士</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> readingNote </tag>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRE阅读方法总结</title>
      <link href="/2018/06/10/GRE-reasoning-method/"/>
      <url>/2018/06/10/GRE-reasoning-method/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Reading的正确打开方式"><a href="#Reading的正确打开方式" class="headerlink" title="Reading的正确打开方式"></a>Reading的正确打开方式</h2><blockquote><p>Reading is <strong>not</strong> translation! 方向错了，就算再努力，也会在错误的方向越走越远</p></blockquote><ol><li><p>先读文章：<br> Key words =&gt; Important information =&gt; Function of sentences</p></li><li><p>答题原则</p></li></ol><ul><li>忠于文本</li><li>区分题型</li><li>读懂选项</li></ul><hr><h2 id="短文章题目类型（前四类）"><a href="#短文章题目类型（前四类）" class="headerlink" title="短文章题目类型（前四类）"></a>短文章题目类型（前四类）</h2><ul><li>although/though/even tho ugh/even if：表示让步（句内逻辑）</li><li>任何英语句子：1.表示thing 2.表示state</li></ul><h3 id="全文主旨题："><a href="#全文主旨题：" class="headerlink" title="全文主旨题："></a><strong>全文主旨题</strong>：</h3><ul><li>The passage is <strong>primarily concerned</strong> with</li><li>Which of the following best states <strong>the central idea</strong> of the passage</li><li>The <strong>primary purpose</strong> of the passage is to<br>  思路：文章结构+文章开头</li></ul><h3 id="推理题（基于原文）："><a href="#推理题（基于原文）：" class="headerlink" title="推理题（基于原文）："></a><strong>推理题</strong>（基于原文）：</h3><ul><li>标志词：<code>inferred</code>/<code>suggest</code>/<code>implies</code></li><li>It can be inferred from the passage that …?</li><li>The author suggests/implies which of the following about …?<br>  思路：<ol><li>定位（scanning 问啥找啥 找第一次）</li><li>读定位词所在句（OR 其前后1-2句）</li><li>匹配选项（定位词所在句OR 其前后1-2句 的同意替换）</li></ol></li></ul><h3 id="作者意图题"><a href="#作者意图题" class="headerlink" title="作者意图题"></a><strong>作者意图题</strong></h3><ul><li>标志词：The author of the passage mentions … primarily <code>in order to</code><br>  思路：<ol><li>定位…所在句</li><li>句内/句间 逻辑</li></ol></li></ul><h3 id="事实内容题"><a href="#事实内容题" class="headerlink" title="事实内容题"></a><strong>事实内容题</strong></h3><ul><li>标志词：<code>which of the following is true of sth</code><br>  思路：<ol><li>定位包含信息点的句子：</li><li>直接看选项找同意替换的选项</li><li>如果没有答案，就接着看下一句</li></ol></li></ul><hr><h2 id="其他题型"><a href="#其他题型" class="headerlink" title="其他题型"></a>其他题型</h2><h3 id="不定项选择题"><a href="#不定项选择题" class="headerlink" title="不定项选择题"></a><strong>不定项选择题</strong></h3><p>依题目要求作答</p><h3 id="观点论证题"><a href="#观点论证题" class="headerlink" title="观点论证题"></a><strong>观点论证题</strong></h3><p>观点标志词</p><ul><li><code>notion</code>/<code>belief</code>/<code>view</code>/<code>point</code>/<code>perception</code>…</li><li>…<code>argue</code>…：认为 </li></ul><h3 id="补充知识点-qualify"><a href="#补充知识点-qualify" class="headerlink" title="补充知识点: qualify"></a>补充知识点: qualify</h3><blockquote><p>If you qualify a statement, you make it less strong or less general by adding a detail or explanation to it.</p></blockquote><h3 id="高亮句作用题"><a href="#高亮句作用题" class="headerlink" title="高亮句作用题"></a><strong>高亮句作用题</strong></h3><ul><li>读懂<code>高亮句</code></li><li>读懂<code>前后句</code></li><li>理解<code>句间关系</code></li></ul><h3 id="补充知识点：句间关系"><a href="#补充知识点：句间关系" class="headerlink" title="补充知识点：句间关系"></a>补充知识点：句间关系</h3><blockquote><p>英文常用顶真方式连接文章逻辑。<code>上句末尾，本句开头</code></p></blockquote><h3 id="文学评论"><a href="#文学评论" class="headerlink" title="文学评论"></a><strong>文学评论</strong></h3><h4 id="补充知识点：wry"><a href="#补充知识点：wry" class="headerlink" title="补充知识点：wry"></a>补充知识点：wry</h4><blockquote><p>a wry expression or wry humor shows that you know a situation is bad, but you alse think it is slightly amusing</p></blockquote><hr><h2 id="精读速度训练"><a href="#精读速度训练" class="headerlink" title="精读速度训练"></a>精读速度训练</h2><h3 id="反应速度训练"><a href="#反应速度训练" class="headerlink" title="反应速度训练"></a>反应速度训练</h3><ol><li>中文（sentence by sentence） 每天2-3篇 练5天；卡壳就查/录音查漏补缺</li><li>英文（sentence by sentence） 每天2-3篇 练5天；卡壳就查/录音查漏补缺</li><li>英文（不出声 停顿两秒想这句话的意思<code>keyword and meaning</code>） 每天2-3篇 练5天</li></ol><h3 id="专注度"><a href="#专注度" class="headerlink" title="专注度"></a>专注度</h3><h3 id="记忆力"><a href="#记忆力" class="headerlink" title="记忆力"></a>记忆力</h3><hr><h2 id="意群划分"><a href="#意群划分" class="headerlink" title="意群划分"></a>意群划分</h2><ol><li>介词短语：prepositional phrase(prep)<ul><li>of individual folktales</li><li>in the place</li></ul></li><li>分词短语：participial phrase（part）<ul><li>concerning the exact point</li><li>told by aliens</li></ul></li><li>形容词短语：adjective pharse(adj)<ul><li>full of books</li><li>useful for me</li></ul></li><li>不定式：infinitive phrase(inf)<ul><li>to do</li></ul></li><li>从句：subordinate clause(sub)<ul><li>that/which/whose</li></ul></li></ol><h2 id="长文章"><a href="#长文章" class="headerlink" title="长文章"></a>长文章</h2><h3 id="类型-思路"><a href="#类型-思路" class="headerlink" title="类型/思路"></a>类型/思路</h3><ol><li>详略结合<br>详读<br>（主题句 + 段末总结 + 逻辑关系）<br>略读<br>（举例 + 展开描述）</li><li>分清题型</li></ol><h3 id="段内关系"><a href="#段内关系" class="headerlink" title="段内关系"></a>段内关系</h3><h3 id="段间关系"><a href="#段间关系" class="headerlink" title="段间关系"></a>段间关系</h3><hr><h2 id="逻辑单题的类型-思路-解题步骤"><a href="#逻辑单题的类型-思路-解题步骤" class="headerlink" title="逻辑单题的类型/思路/解题步骤"></a>逻辑单题的类型/思路/解题步骤</h2><h3 id="逻辑单题的类型"><a href="#逻辑单题的类型" class="headerlink" title="逻辑单题的类型"></a>逻辑单题的类型</h3><ul><li>假设<ol><li>提问方式<ul><li>Which of the following is an <strong>assumption</strong> on which the argument depends?</li><li>Which of the following is an <strong>assumption</strong> on which the argument relies?</li></ul></li><li>检查选项的逻辑成立的条件</li></ol></li><li>推理<ol><li>提问方式<ul><li>The information given, if accurate,most strongly supports which of the following <strong>hypotheses</strong>? </li><li>Which of the following <strong>conclusions</strong> is the best supported by the information? </li></ul></li></ol></li><li>加强<ol><li>提问方式<ul><li>Which of the following,if true,<strong>most strengthens</strong> the argument given?</li><li>Which is the following provide <strong>the most support</strong> for the argument</li></ul></li><li>选项提高结论的可信度</li></ol></li><li>句子功能<ol><li>提问方式<ul><li>In the argument given, the <strong>two highlighted portions</strong> play which of the following roles?</li></ul></li></ol></li><li>削弱<ol><li>提问方式<ul><li>Which of the following,if true,<strong>most weakens</strong> the argument given?</li><li>Which of the following,if true,<strong>most seriously undermines</strong>…?</li></ul></li></ol></li><li>填空<ol><li>提问方式<ul><li>Which of the following,if true,most logically complete the argument?</li></ul></li></ol></li></ul><h3 id="逻辑单题的解题步骤"><a href="#逻辑单题的解题步骤" class="headerlink" title="逻辑单题的解题步骤"></a>逻辑单题的解题步骤</h3><ol><li><p>看题目（确定类型）{5秒钟}</p></li><li><p>看文章（读懂逻辑）{45秒钟}</p></li><li>想原理（预设提问）[用中文想]{20秒钟}</li><li>看选项（排除错选）{60秒钟}</li></ol>]]></content>
      
      
      <categories>
          
          <category> GRE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRE阅读导学</title>
      <link href="/2018/06/10/GRE-reasoning-overall/"/>
      <url>/2018/06/10/GRE-reasoning-overall/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="语言-amp-题目"><a href="#语言-amp-题目" class="headerlink" title="语言&amp;题目"></a>语言&amp;题目</h2><ul><li>Specify your problems</li><li>Simplify your solutions</li></ul><h2 id="GRE-General-Test"><a href="#GRE-General-Test" class="headerlink" title="GRE General Test"></a>GRE General Test</h2><ul><li>Analytical Writing</li><li>Verbal Reasoning</li><li>Quantitative Reasoning</li></ul><h3 id="Analytical-Writing-2-tasks"><a href="#Analytical-Writing-2-tasks" class="headerlink" title="Analytical Writing(2 tasks)"></a><strong>Analytical Writing</strong>(2 tasks)</h3><h3 id="Verbal-Reasoning-2-sections"><a href="#Verbal-Reasoning-2-sections" class="headerlink" title="Verbal Reasoning(2 sections)"></a><strong>Verbal Reasoning</strong>(2 sections)</h3><p>30 min &amp; 20 proplems</p><h3 id="Quantitative-Reasoning-2-sections"><a href="#Quantitative-Reasoning-2-sections" class="headerlink" title="Quantitative Reasoning(2 sections)"></a><strong>Quantitative Reasoning</strong>(2 sections)</h3><h3 id="Unscored-positon-varies"><a href="#Unscored-positon-varies" class="headerlink" title="Unscored(positon varies)"></a>Unscored(positon varies)</h3><h3 id="score-standard"><a href="#score-standard" class="headerlink" title="score standard"></a>score standard</h3><p>section-level(根据第一部分的分数决定第二部分的难度)</p><hr><h2 id="GRE-Verbal-Reasoning"><a href="#GRE-Verbal-Reasoning" class="headerlink" title="GRE Verbal Reasoning"></a>GRE Verbal Reasoning</h2><ul><li>Reading Comprehension(20 min)</li><li>Text Completion</li><li>Sentence Equivalence</li></ul><h3 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h3><ul><li>长文章 4</li><li>短文章 2</li><li>逻辑单题 1</li></ul><h3 id="文章初体验"><a href="#文章初体验" class="headerlink" title="文章初体验"></a>文章初体验</h3><ul><li>plant <strong>communities</strong>:群落</li><li>more <strong>susceptible</strong> than:易受影响的</li><li><strong>inconsistent</strong> theroy:前后不一致的</li><li>resource-release <strong>mechanisms</strong>:方式方法</li><li><strong>ecological</strong> correlates: 生态学的</li><li>no <strong>necessary</strong> relationship:必然的</li><li>this may <strong>arise from</strong> sth:由于</li></ul>]]></content>
      
      
      <categories>
          
          <category> GRE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRE填空逻辑方法总结</title>
      <link href="/2018/06/10/GRE-verbal-overall/"/>
      <url>/2018/06/10/GRE-verbal-overall/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="GRE填空简介"><a href="#GRE填空简介" class="headerlink" title="GRE填空简介"></a>GRE填空简介</h2><ol><li>题目数量：每个section10个题目，一共两个section共20题</li><li>题目类型：</li></ol><ul><li>单空题：五选一</li><li>双空题：三选一</li><li>三空题：三选一</li><li>句子等价：六选二（选同义词）</li></ul><hr><h2 id="如何取得高分？"><a href="#如何取得高分？" class="headerlink" title="如何取得高分？"></a>如何取得高分？</h2><ol><li><p>词汇：不能低于12000，最好在15000以上</p><blockquote><p>五遍以下纯裸考，十遍以下比基尼考</p></blockquote><ul><li>relying on the <strong>well-to-do</strong> for <strong>commissions</strong>：有钱人&amp;佣金</li><li>hypocrite：虚伪</li><li>egotist：以自我为中心这</li><li>sycophant：奉承者</li><li>adulator：奉承者</li><li>braggart：自夸者</li><li>coward：懦夫</li><li>if sth at all：即使真的发生某事</li></ul></li><li>句子结构（主要是语法问题）<ul><li>后置定语（四种）：<ul><li>The boy <strong>who is reading a book</strong>：定语从句做后置定语</li><li>The boy <strong>reading a book</strong>：分词短语做后置定语</li><li>The boy <strong>in a red sweater</strong>:介词短语做后置定语</li><li>The boy angry at his teacher:形容词短语做后置定语</li><li>真题例句：The writer so <strong>wary of extravagance</strong> was <strong>profligate</strong> with paper：对奢侈浪费十分谨慎;浪荡 行为不检点</li></ul></li><li>倒装句<ul><li>1.部分倒装：Only by ignoring decades of mismanagement and inefficiency could investors conclude that a fresh <strong>infusion of cash</strong> of cash would provide anything other than a <strong>fleeting solution</strong> to the company’s <strong>financial woes</strong>.：注入现金；短暂的方案；金融危机<ul><li>主句正常结构是：Investors could conclude that … only by ignoring decades of …</li><li>部分倒装（助动词，Be动词，情态动词提前）<ul><li>only + 状语提前句子部分倒装</li><li>否定副词提前句子部分倒装</li></ul></li></ul></li><li>2.完全倒装：Burke is often on slippery ground when it comes to her primary sources;<strong>especially duious is the mode</strong> by which she gathered her oral evidence<ul><li>正常语序：Burke is often on slippery ground when it comes to her primary sources;<strong>the mode is especially duious</strong> by which she gathered her oral evidence</li><li>完全倒装知识讲解：<ul><li>1.“主谓句+地点状语/时间状语”中状语可以提前</li><li>2.“主语+be动词+表语（形容词短语，介词短语，分词短语）”</li></ul></li></ul></li></ul></li><li>插入语：<ul><li>常见插入语例子：To be frank；The teacher,along with the headmaster,is doing sth</li><li>如何处理插入语：先看插入语之外的句子结构，再带入插入语进行完整的理解。</li></ul></li><li>That引导主语从句（that的从句用作主语）</li><li>As引导的让步状语从句<ul><li>1.作为：As a student, I must study hard</li><li>2.当：It was raining as I came back to my car</li><li>3.因为：As he is a child, we should not blame him</li><li>4.如此：I am as tall as my father</li><li>5.与…比起来：I am as tall as my father</li><li>6.尽管：<ul><li>Child as he is, he know everything = Although he is a child, he knows everything</li><li>Object as you may, I will go = Although you may object, I will go</li><li>Much as you like it, I will not buy it. = Although you like it, I will not buy it for you.</li></ul></li></ul></li><li>语法补充知识点（自学）<ul><li>六大从句</li><li>强调句</li><li>虚拟语气</li><li>非谓语动词</li></ul></li></ul></li></ol><ol><li><p>逻辑推理</p><ul><li>逻辑要收敛：任何推理都要基于题目本身！<ul><li>erudite：博学的</li><li>insular：与世隔绝的</li><li>cosmopolitan：四海为家的</li><li>imperturbable：泰然自若的</li></ul></li><li>很多时候我们只需要掌握句子的逻辑主线，不必过于纠结于汉语的翻译。<ul><li>savor：品尝 欣赏</li><li>rejoice:庆祝</li><li>prevarication：搪塞</li><li>flattery：奉承</li><li>affectat：感动</li><li>narcissism：自恋；水仙花</li><li>indolence：懒散</li><li>fecundit：肥沃富饶</li><li>economy：节约；理财</li></ul></li></ul></li><li><p>对应技巧</p><ul><li>填空核心方法总结<strong>：读懂句意-简化句意-梳理核心逻辑-找到空格对应点-选出答案</strong></li><li>练习：<ul><li>disdain：蔑视</li><li>egalitarian：平等主义</li><li>maverick：特立独行</li><li>dilettante：一知半解者</li><li>iconoclast：偶像破坏者</li><li>purveyor：承办商</li><li>prose:散文</li><li>general：将军</li><li>belligerence:好战的</li><li>indigence：贫穷</li><li>perfidy：背信弃义</li><li>betrayal：揭露</li><li>haughty：高傲的</li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> GRE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> verbal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRE填空逻辑方法总结</title>
      <link href="/2018/06/10/GRE-verbal-method/"/>
      <url>/2018/06/10/GRE-verbal-method/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="同意重复"><a href="#同意重复" class="headerlink" title="同意重复"></a>同意重复</h2><blockquote><p>同意重复定义：句中通过一些明显的指示代词和指代性的名词对前文或者后文的内容进行重复的题目考点</p></blockquote><ol><li>同意重复常见标志代词：<code>this,that,such,this very（正是这个）,the,the same,the former/later</code>等</li><li>同意重复扩展标志词：<code>necessarily, surely, definitely, equally, parallel</code></li></ol><p>广义同/反义词</p><blockquote><p>在评价正负性方向相同/相反的词汇和短语（善良、勇敢 VS 无知 吝啬）</p></blockquote><p>补充知识点：if的用法</p><ol><li>如果：</li><li>是否：</li><li>即使：if not all（即使不是全部），“大部分”的意思</li><li>其他短语：<ul><li>If any:即便有</li><li>If ever:如果真有</li><li>If at all:即使某事真的 发生</li></ul></li></ol><hr><h2 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h2><h3 id="因果关系逻辑词"><a href="#因果关系逻辑词" class="headerlink" title="因果关系逻辑词"></a>因果关系逻辑词</h3><ul><li><p>表原因：because, since, for, in that(GRE常用！), now that（既然）,  as, because of, due to, given（GRE常用！）, on the ground that, on ground of, as long as </p><h4 id="例句："><a href="#例句：" class="headerlink" title="例句："></a>例句：</h4><ul><li>Dramatic literature often recapitulates the history of a culture in that <strong>it takes as its subject matter the important event</strong> that have shaped and guided the culture.</li><li>=Dramatic literature often recapitalates the history of a culture in that it takes the important events that have shaped and guided the culture as its subject matter.<br>补充知识点：宾语后置<blockquote><p>宾语后置：在“主语+谓语+宾语+状语”的句型中。有时候宾语如果有修饰成分，则可以把宾语后置 </p></blockquote></li></ul></li><li><p>表结果：therefore, thereby, thus, so, whereby, as a result, consequently, so(such)…that…, so(such)…as to, too…to…等</p></li><li>其他因果逻辑词：result in(左边导致右边), result from(右边导致左边), lead to, give rise to, bring about</li></ul><h3 id="解题要点"><a href="#解题要点" class="headerlink" title="解题要点"></a>解题要点</h3><p>根据已知的因果关系判断空格语义，再选取同义或者广义同义。</p><p>补充知识点：as long as用法</p><pre><code>- 和...一样长：This stick is as long as that one- 只要...,如果：We can do this directly as long as we have certain types of information- 既然，因为：As long as you have been here, you could wait for a moment.</code></pre><hr><h2 id="目的手段"><a href="#目的手段" class="headerlink" title="目的手段"></a>目的手段</h2><h3 id="表示目的的词或者短语："><a href="#表示目的的词或者短语：" class="headerlink" title="表示目的的词或者短语："></a>表示目的的词或者短语：</h3><p>to do, for doing, in order to, so that, for fear that, lest, in case, for 等</p><h3 id="表示手段的词或者短语："><a href="#表示手段的词或者短语：" class="headerlink" title="表示手段的词或者短语："></a>表示手段的词或者短语：</h3><p>by doing, whereby, by means of, be way of, depend on, rely on, by virtur of 等</p><h3 id="目的手段题目解题要点"><a href="#目的手段题目解题要点" class="headerlink" title="目的手段题目解题要点"></a>目的手段题目解题要点</h3><p>根据目的反推手段或者方法，或者根据手段推导目的或者结果</p><hr><h2 id="解释说明"><a href="#解释说明" class="headerlink" title="解释说明"></a>解释说明</h2><h3 id="解释说明标志词："><a href="#解释说明标志词：" class="headerlink" title="解释说明标志词："></a>解释说明标志词：</h3><p>冒号（：），破折号（-），定语从句，分词短语做状语，同位语，形容词短语，主系表结构等 </p><h3 id="解题要点-1"><a href="#解题要点-1" class="headerlink" title="解题要点"></a>解题要点</h3><p>对解释说明的信息进行同义改写</p><hr><h2 id="类比关系"><a href="#类比关系" class="headerlink" title="类比关系"></a>类比关系</h2><h3 id="类比关系标志词"><a href="#类比关系标志词" class="headerlink" title="类比关系标志词"></a>类比关系标志词</h3><p><code>just as, like, as...as, as if/as though, resemble, similarly, in the same way, like wise 等</code></p><h3 id="解题要点-2"><a href="#解题要点-2" class="headerlink" title="解题要点"></a>解题要点</h3><p>找类比关系进行同义改写</p><hr><h2 id="并列关系"><a href="#并列关系" class="headerlink" title="并列关系"></a>并列关系</h2><h3 id="并列关系标志词"><a href="#并列关系标志词" class="headerlink" title="并列关系标志词"></a>并列关系标志词</h3><p><code>and, or, not only...but alse, not just...but also, as well as, at once...and等</code></p><h3 id="并列分为顺承并列和转折并列"><a href="#并列分为顺承并列和转折并列" class="headerlink" title="并列分为顺承并列和转折并列"></a>并列分为顺承并列和转折并列</h3><ul><li>我欣赏她的美丽和善良</li><li>我既喜欢这本书又介意它的一些细节</li></ul><h3 id="解题要点-3"><a href="#解题要点-3" class="headerlink" title="解题要点"></a>解题要点</h3><p>　找同义广义词（很少找直接同义词）</p><hr><h2 id="递进关系"><a href="#递进关系" class="headerlink" title="递进关系"></a>递进关系</h2><h3 id="递进关系逻辑词"><a href="#递进关系逻辑词" class="headerlink" title="递进关系逻辑词"></a>递进关系逻辑词</h3><p><code>even, not only ... but also, indeed, especially, particularly, in addition, moreover, futher more, all the more</code></p><h3 id="注意even的停发"><a href="#注意even的停发" class="headerlink" title="注意even的停发"></a>注意even的停发</h3><ul><li>even表示让步转折：多放在句首，<strong>即使</strong></li><li>even表示递进关系：多放在句中，<strong>更加</strong></li></ul><h3 id="解题要点-4"><a href="#解题要点-4" class="headerlink" title="解题要点"></a>解题要点</h3><p>　加强递进/减弱递进</p><hr><h2 id="让步转折"><a href="#让步转折" class="headerlink" title="让步转折"></a>让步转折</h2><h3 id="让步转折逻辑词"><a href="#让步转折逻辑词" class="headerlink" title="让步转折逻辑词"></a>让步转折逻辑词</h3><p><code>even, although, though, despite, however, nevertheless, but, in spite of, for all=despite, even as, even when, as</code></p><h3 id="解题要点-5"><a href="#解题要点-5" class="headerlink" title="解题要点"></a>解题要点</h3><p>　取反义或广义反义</p><h2 id=""><a href="#" class="headerlink" title="　"></a>　</h2><h2 id="对比关系"><a href="#对比关系" class="headerlink" title="对比关系"></a>对比关系</h2><h3 id="对比关系逻辑词"><a href="#对比关系逻辑词" class="headerlink" title="对比关系逻辑词"></a>对比关系逻辑词</h3><ul><li>连接性关系词：<code>rather than, far from, on the contratary, in contrast to, compare with, unlike, different from, not but</code></li><li>对比关系的其他逻辑词：<code>ironic, surprising, stunning, strange, mask, belie, veil, seem, appear, paradox, contradiction, dichtomy</code></li></ul><h3 id="解题要点-6"><a href="#解题要点-6" class="headerlink" title="解题要点"></a>解题要点</h3><p>　取反义或广义反义词</p>]]></content>
      
      
      <categories>
          
          <category> GRE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> verbal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB进阶设计模式</title>
      <link href="/2018/06/09/mongodb-design-mode/"/>
      <url>/2018/06/09/mongodb-design-mode/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>使用了mongodb这么长时间，似乎遇到了瓶颈。本文通过几篇博客再次学习mongo，希望有新的认识。温故而知新~</p></blockquote><h2 id="MongoDB的主体认识："><a href="#MongoDB的主体认识：" class="headerlink" title="MongoDB的主体认识："></a>MongoDB的主体认识：</h2><ul><li>高可用</li><li>分布式</li><li>灵活模式</li><li>文档数据库</li></ul><h3 id="传统关系模型（SQL）和文档模型的区别"><a href="#传统关系模型（SQL）和文档模型的区别" class="headerlink" title="传统关系模型（SQL）和文档模型的区别"></a>传统关系模型（SQL）和文档模型的区别</h3><p><img src="http://www.mongoing.com/wp-content/uploads/2016/01/MongoDB-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E8%BF%9B%E9%98%B6%E6%A1%88%E4%BE%8B_%E9%A1%B5%E9%9D%A2_04-1024x791.png" alt=""></p><h3 id="文档模型的优点"><a href="#文档模型的优点" class="headerlink" title="文档模型的优点"></a>文档模型的优点</h3><ul><li>读写效率高：Data Locality</li><li>可扩展能力强: 无关联易分库</li><li>动态模式：灵活应付不同的数据模式</li><li>模型自然：最接近于对象模型</li></ul><hr><h2 id="MongoDB文档模式设计的基本策略"><a href="#MongoDB文档模式设计的基本策略" class="headerlink" title="MongoDB文档模式设计的基本策略"></a>MongoDB文档模式设计的基本策略</h2><h3 id="优先考虑内嵌，再去考虑引用"><a href="#优先考虑内嵌，再去考虑引用" class="headerlink" title="优先考虑内嵌，再去考虑引用"></a>优先考虑内嵌，再去考虑引用</h3><p><img src="http://www.mongoing.com/wp-content/uploads/2016/01/MongoDB-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E8%BF%9B%E9%98%B6%E6%A1%88%E4%BE%8B_%E9%A1%B5%E9%9D%A2_06.png" alt=""></p><h3 id="MongoDB设计模式原则"><a href="#MongoDB设计模式原则" class="headerlink" title="MongoDB设计模式原则"></a>MongoDB设计模式原则</h3><ul><li>为应用程序服务，而不是为了存储优化</li><li>为实现最佳性能而设计</li></ul><hr><h2 id="经典的模式设计案例"><a href="#经典的模式设计案例" class="headerlink" title="经典的模式设计案例"></a>经典的模式设计案例</h2><h3 id="电商（ECommerce）"><a href="#电商（ECommerce）" class="headerlink" title="电商（ECommerce）"></a>电商（ECommerce）</h3><h4 id="设计考量"><a href="#设计考量" class="headerlink" title="设计考量"></a>设计考量</h4><ul><li>一个购物车数据项不会太大，一般项数少于100</li><li>数据自动过期（15-30分钟无交互）</li><li>用冗余的方式来提供读取性能</li></ul><h4 id="参考数据模型"><a href="#参考数据模型" class="headerlink" title="参考数据模型"></a>参考数据模型</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;_id&quot;</span>: ObjectId(<span class="string">&quot;*&quot;</span>)</span><br><span class="line">    <span class="string">&quot;userid&quot;</span>: <span class="number">1234</span>,</span><br><span class="line">    <span class="string">&quot;last_activity&quot;</span>: ISODate(...), // 使用TTL来自动删除未付款的购物车</span><br><span class="line">    <span class="string">&quot;status&quot;</span>: <span class="string">&quot;active&quot;</span>,</span><br><span class="line">    <span class="string">&quot;items&quot;</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;itemid&quot;</span>: <span class="number">1234</span>,</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: <span class="string">&quot;Milk&quot;</span>,</span><br><span class="line">            <span class="string">&quot;price&quot;</span>: <span class="number">5.00</span>,</span><br><span class="line">            <span class="string">&quot;quantity&quot;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&quot;img_url&quot;</span>: <span class="string">&quot;milk,jpg&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;itemid&quot;</span>: <span class="number">4567</span>,</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: <span class="string">&quot;Eggs&quot;</span>,</span><br><span class="line">            <span class="string">&quot;price&quot;</span>: <span class="number">3.00</span>,</span><br><span class="line">            <span class="string">&quot;quantity&quot;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&quot;img_url&quot;</span>: <span class="string">&quot;eggs.jpg&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="对模型的操作"><a href="#对模型的操作" class="headerlink" title="对模型的操作"></a>对模型的操作</h4><ul><li><p>添加商品到购物车</p>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">db.cart.update(&#123;</span><br><span class="line">    <span class="string">&quot;_id&quot;</span>: ObjectId(<span class="string">&quot;*&quot;</span>)</span><br><span class="line">&#125;, &#123;</span><br><span class="line">    $push:&#123;</span><br><span class="line">        <span class="string">&quot;items&quot;</span>:&#123;</span><br><span class="line">            <span class="string">&quot;itemid&quot;</span>: <span class="number">2345</span>,</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: <span class="string">&quot;Bread&quot;</span>,</span><br><span class="line">            <span class="string">&quot;price&quot;</span>: <span class="number">2.00</span>,</span><br><span class="line">            <span class="string">&quot;quantity&quot;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&quot;img_url&quot;</span>: <span class="string">&quot;bread.jpg&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    $<span class="built_in">set</span>:&#123;</span><br><span class="line">        <span class="string">&quot;last_activity&quot;</span>: ISODate()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></li><li><p>更新某个商品的数量</p>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">db.carts.update(&#123;</span><br><span class="line">    <span class="string">&quot;id&quot;</span>: ObjectId(<span class="string">&quot;*&quot;</span>)</span><br><span class="line">    <span class="string">&quot;items.itemid&quot;</span>: <span class="number">4567</span></span><br><span class="line">&#125;,&#123;</span><br><span class="line">    $<span class="built_in">set</span>:&#123;</span><br><span class="line">        <span class="string">&quot;item.$.quantity&quot;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&quot;last_activity&quot;</span>: ISODate()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>统计商品总数（<strong>聚合运算</strong>）</p>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">db.cart.createIndex(&#123;<span class="string">&quot;items.itemid&quot;</span>: <span class="number">1</span>&#125;)</span><br><span class="line">db.cart.aggregate(</span><br><span class="line">    &#123; $match: &#123;<span class="string">&quot;items.itemid&quot;</span>: <span class="number">8910</span>&#125;&#125;, <span class="comment"># 筛选出购物车里id为8910的商品</span></span><br><span class="line">    &#123; $unwind: <span class="string">&quot;$items&quot;</span> &#125;, <span class="comment">#　展开items数组，每个数组的元素变成一个文档</span></span><br><span class="line">    &#123; $group: &#123;</span><br><span class="line">        <span class="string">&quot;_id&quot;</span>: <span class="string">&quot;$items.itemid&quot;</span>,</span><br><span class="line">        <span class="string">&quot;amount&quot;</span>: &#123; <span class="string">&quot;$sum&quot;</span> : <span class="string">&quot;$items.quantity&quot;</span> &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;<span class="comment">#使用聚合运算$sum吧每一件商品的数量求和</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h3 id="社交（Social）"><a href="#社交（Social）" class="headerlink" title="社交（Social）"></a>社交（Social）</h3><h4 id="设计考量-1"><a href="#设计考量-1" class="headerlink" title="设计考量"></a>设计考量</h4><ul><li>维护朋友关系-关注、被关注</li><li>朋友圈设计、名人效应</li></ul><h4 id="经典文档设计的问题"><a href="#经典文档设计的问题" class="headerlink" title="经典文档设计的问题"></a>经典文档设计的问题</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;id&quot;</span>: <span class="string">&quot;Yuan&quot;</span>,</span><br><span class="line">    <span class="string">&quot;fullname&quot;</span>: <span class="string">&quot;Zhu Zheng Yuan&quot;</span>,</span><br><span class="line">    <span class="string">&quot;followers&quot;</span>: [<span class="string">&quot;Oscar&quot;</span>, <span class="string">&quot;Mandy&quot;</span>],</span><br><span class="line">    <span class="string">&quot;following&quot;</span>: [<span class="string">&quot;Mandy&quot;</span>, <span class="string">&quot;Bert&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>　如果TJ我是一个明星，他们关注我的人可能有千万。一个千万级的数组会有两个问题：<br>1.有可能超出一个文档最大16M的硬性限制； 2. MongoDB数组太大会严重影响性能。<br>　解决方案是建立一个专门的集合来描述关注关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.follower.find()</span><br><span class="line">&#123;<span class="string">&quot;_id&quot;</span>: ObjectId(), <span class="string">&quot;user&quot;</span>: <span class="string">&quot;Yuan&quot;</span>, <span class="string">&quot;following&quot;</span>: <span class="string">&quot;Mandy&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;_id&quot;</span>: ObjectId(), <span class="string">&quot;user&quot;</span>: <span class="string">&quot;Yuan&quot;</span>, <span class="string">&quot;following&quot;</span>: <span class="string">&quot;Bert&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;_id&quot;</span>: ObjectId(), <span class="string">&quot;user&quot;</span>: <span class="string">&quot;Oscar&quot;</span>, <span class="string">&quot;following&quot;</span> : <span class="string">&quot;Yuan&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;_id&quot;</span>: ObjectId(), <span class="string">&quot;user&quot;</span>: <span class="string">&quot;Mandy&quot;</span>, <span class="string">&quot;following&quot;</span>: <span class="string">&quot;Yuan&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="微博墙的实现"><a href="#微博墙的实现" class="headerlink" title="微博墙的实现"></a>微博墙的实现</h4><blockquote><p>微博墙：列表显示所有<strong>关注用户</strong>的<strong>最新状态</strong></p></blockquote><p>解决方案：</p><ul><li>扇出读（常规玩法）：当你需要去获得所有你关注用户的最新更新的时候，你就去到每一个你关注用户的数据区，把最新的一些数据取回来。[最慢服务器响应时间决定了总体的响应时间]</li><li>扇出写（土豪玩法）：当被关注用户发布微博的时候，一条数据会写多次：直接写到每一个关注你的粉丝的墙上。[写入需求会被放大]</li></ul><blockquote><p>参考资料：</p><ol><li>MongoDB 进阶模式设计：<a href="http://www.mongoing.com/mongodb-advanced-pattern-design">http://www.mongoing.com/mongodb-advanced-pattern-design</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mongoDB </tag>
            
            <tag> designMode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习中的梯度下降法</title>
      <link href="/2018/05/18/gradientDescent/"/>
      <url>/2018/05/18/gradientDescent/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="引言："><a href="#引言：" class="headerlink" title="引言："></a>引言：</h2><p>梯度下降法的在机器学习中的地位：当神经网络使用反向传播算法之后，需要使用最优化算法来减小误差。而在各种最优化算法中，梯度下降法是最简单、最常见的一种，在深度学习的训练中被广泛使用。</p><h2 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h2><blockquote><p>求解函数极值的问题，包括极大值和极小值</p></blockquote><ol><li>只要函数是可导的，极值点的导数必定为0。</li><li><strong>其中x称为优化变量，f称为目标函数。极大值问题可以转换成极小值问题来求解，只需要将目标函数加上负号即可</strong>：<br>$$max_xf(x)\equiv{min_x{-f(x)}}$$<!-- ![](http://ww1.sinaimg.cn/large/ca26ff18ly1fsqyceiqsdj20hs02n74p.jpg){:height="50px" width="200px"} --></li></ol><h2 id="导数与梯度"><a href="#导数与梯度" class="headerlink" title="导数与梯度"></a>导数与梯度</h2><ol><li>多元函数的梯度定义为：<br>$$\nabla{f(x)=(\frac{\partial{f}}{\partial{x_1}},…,\frac{\partial{f}}{\partial{x_n}})^{T}}$$</li></ol><p>&emsp;其中$\nabla$称为梯度算子，它作用于一个多元函数，得到一个向量。下面是计算函数梯度的一个例子：$\nabla{(x^2+xy-y^2)=(2x+y,x-2y)}$</p><ol><li>梯度为0只是函数取极值的必要条件而不是充分条件</li><li><p>如何确定驻点是极大值还是极小值？<br> 要看二阶导数/Hessian矩阵：</p><ul><li>如果Hessian矩阵正定，函数有极小值</li><li>如果Hessian矩阵负定，函数有极大值</li><li>如果Hessian矩阵不定，则需要进一步讨论</li></ul></li><li><p>为什么不可以直接求函数的梯度，来解方程？<br>答：方程可能很难解：对于有指数函数，对数函数，三角函数的方程，我们称为超越方程。比如$3x^2e^{xy}+xcos(xy)=0$。其求解的难度并不比求极值本身小。</p></li></ol><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fsr7fgqxxyj21yu2kzhe3.jpg" alt=""></p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fsr7h1sclgj21yu2kze8a.jpg" alt=""></p><h2 id="面临的问题"><a href="#面临的问题" class="headerlink" title="面临的问题"></a>面临的问题</h2><blockquote><p>补充：驻点要求一阶导数必须存在，而极值点对导数没有要求</p><ol><li>局部极小值点</li><li>鞍点问题<h2 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h2></li><li>AdaGrad(自适应梯度)</li><li>AdaDelta</li><li>Adam(adaptive moment estimation)</li><li>NAG<h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2>随机梯度下降法在数学期望的意义下收敛，但并不能保证每次迭代时函数值一定下降。</li></ol><p>参考与引用</p></blockquote><ol><li><a href="https://mp.weixin.qq.com/s/lqwUkimO4irkIZmAnp0bcg">https://mp.weixin.qq.com/s/lqwUkimO4irkIZmAnp0bcg</a></li><li><a href="https://blog.csdn.net/lanchunhui/article/details/52504859">https://blog.csdn.net/lanchunhui/article/details/52504859</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gradientDescent </tag>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN原理介绍以及二进制加法器</title>
      <link href="/2018/04/23/RNN-introduction/"/>
      <url>/2018/04/23/RNN-introduction/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="前言：神经网络拓扑学-amp-convJS"><a href="#前言：神经网络拓扑学-amp-convJS" class="headerlink" title="前言：神经网络拓扑学&amp;convJS"></a>前言：神经网络拓扑学&amp;convJS</h2><p>两个博客的动图形象的展示了神经网络内部的工作过程。</p><blockquote><p><a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a><br><a href="https://cs.stanford.edu/people/karpathy/convnetjs/">https://cs.stanford.edu/people/karpathy/convnetjs/</a></p></blockquote><h2 id="RNN（Recurrent-Neural-Network）"><a href="#RNN（Recurrent-Neural-Network）" class="headerlink" title="RNN（Recurrent Neural Network）"></a>RNN（Recurrent Neural Network）</h2><p><strong>为了简易教学过程，我会从“简易”的RNN模型逐渐过渡到“真实”的RNN模型</strong></p><h3 id="为什么会有RNN？"><a href="#为什么会有RNN？" class="headerlink" title="为什么会有RNN？"></a>为什么会有RNN？</h3><ul><li>类比人类的思考过程。人们对新事物的思考总是包含着先验知识（记忆）的</li><li>尝试从后向前背字母表的十分困难的，因为人类总是以序列为单位记忆。（就像链表）</li></ul><h3 id="RNN长成什么样？"><a href="#RNN长成什么样？" class="headerlink" title="RNN长成什么样？"></a>RNN长成什么样？</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_1.png" alt=""></p><ul><li>由上图可见，RNN的信息流是：</li></ul><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_2.png" alt=""></p><ul><li>RNN中的记忆代表prev_hidden（之前的隐层“记忆”）也被当做了输入用来训练神经网络！</li></ul><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_3.png" alt=""></p><blockquote><p>注意区分上图：一个RNN可以看作数多个相同神经网络的复制版本！</p></blockquote><ul><li>假设我们有一个时间步长为4的RNN，那么它的信息流就是<br><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_4.png" alt=""></li></ul><ul><li>让我们形象的看看“记忆”是如何影响RNN的</li></ul><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_5.png" alt=""></p><h3 id="RNN可以解决什么问题？"><a href="#RNN可以解决什么问题？" class="headerlink" title="RNN可以解决什么问题？"></a>RNN可以解决什么问题？</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_6.png" alt=""></p><blockquote><p>时间序列和列表模型：语音识别，语言模型，翻译，图像捕捉等</p></blockquote><h3 id="RNN有什么缺点吗？"><a href="#RNN有什么缺点吗？" class="headerlink" title="RNN有什么缺点吗？"></a>RNN有什么缺点吗？</h3><ul><li>the clouds are in the sky</li><li>I grew up in France… I speak fluent French.</li></ul><h4 id="长期依赖：当gap越来越大，RNN就不太可能学习有用的信息了"><a href="#长期依赖：当gap越来越大，RNN就不太可能学习有用的信息了" class="headerlink" title="长期依赖：当gap越来越大，RNN就不太可能学习有用的信息了"></a>长期依赖：当gap越来越大，RNN就不太可能学习有用的信息了</h4><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_7.png" alt=""></p><h3 id="如何解决长期依赖？"><a href="#如何解决长期依赖？" class="headerlink" title="如何解决长期依赖？"></a>如何解决长期依赖？</h3><p>看看我们大脑是如何工作的：我们的大脑有遗忘的功能，即只记住记忆中关键的信息点，而不去存储完整的记忆。这种功能让我们大脑的计算负荷大大减少。—-于是LSTM诞生了</p><h2 id="LSTM-（Long-Short-Term-Memory）"><a href="#LSTM-（Long-Short-Term-Memory）" class="headerlink" title="LSTM （Long Short Term Memory）"></a>LSTM （Long Short Term Memory）</h2><h3 id="LSTM与RNN的结构差异"><a href="#LSTM与RNN的结构差异" class="headerlink" title="LSTM与RNN的结构差异"></a>LSTM与RNN的结构差异</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_8.png" alt=""></p><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_9.png" alt=""></p><p>LSTM将RNN的单层神经网络变成了四层神经网络</p><h3 id="wtf-赶快讲讲细节"><a href="#wtf-赶快讲讲细节" class="headerlink" title="wtf?赶快讲讲细节"></a>wtf?赶快讲讲细节</h3><h4 id="我们在图中有以下约定："><a href="#我们在图中有以下约定：" class="headerlink" title="我们在图中有以下约定："></a>我们在图中有以下约定：</h4><ul><li>图中的线条代表一个完整的向量</li><li>粉色圆圈代表一个向量操作</li></ul><h3 id="LSTM的核心思想"><a href="#LSTM的核心思想" class="headerlink" title="LSTM的核心思想"></a>LSTM的核心思想</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_10.png" alt=""></p><ul><li><p>上图表示LSTM有能力给Cell state添加或者删除信息（这些能力被一种叫做<strong>gates</strong>的结构控制）</p></li><li><p>gates:一条信息经过的路径由运算函数和sigmoid函数构成</p></li></ul><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_11.png" alt=""></p><h4 id="补充：重新认识sigmoid函数："><a href="#补充：重新认识sigmoid函数：" class="headerlink" title="补充：重新认识sigmoid函数："></a>补充：重新认识sigmoid函数：</h4><p>$$S(x)= \frac{1}{1+e^{-x}}$$</p><p>$$S^{‘}{(x)}=\frac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x))$$</p><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_12.png" alt=""></p><p>　很明显，其作用是把x映射到$[0,1]$：0表示忘记，1表示记住</p><h3 id="LSTM的具体工作流程"><a href="#LSTM的具体工作流程" class="headerlink" title="LSTM的具体工作流程"></a>LSTM的具体工作流程</h3><ol><li>LSTM第一步便是决定从cell state中丢弃的信息(由sigmoid函数完成)</li></ol><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_13.png" alt=""></p><p>$$f_t=\sigma(W<em>f·[h</em>{t-1},x_t] + b_f)$$</p><ol><li><p>LSTM第二步是决定将那些新信息保存至cell state</p><ul><li>sigmoid层决定更新哪些值</li><li><p>tanh层则负责创建一个新的候选值</p><p>$$i_t = \sigma(W<em>i·[h</em>{t-1},x_t]+b_i)$$</p><p>$$\hat{C_t}=tanh(W<em>C·[h</em>{t-1},x_t]+b_C)$$</p></li></ul></li></ol><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_14.png" alt=""></p><ol><li>更新$C_{t-1}$为新的$C_t$</li></ol><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_15.png" alt=""></p><p>$$C_t=f<em>t * C</em>{t-1}+i_t * \hat{C_t}$$</p><ol><li>LSTM决定输出值<ul><li>使用sigmoid层决定那一部分用作输出</li><li>通过tanh和部分输出决定总输出</li></ul></li></ol><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_16.png" alt=""></p><p>$$o_t=\sigma(W<em>o[h</em>{t-1},x_t]+b_o)$$</p><p>$$h_t = o_t *tanh(C_t)$$</p><h2 id="回归RNN，我们来简单的描述下RNN梯度下降的过程"><a href="#回归RNN，我们来简单的描述下RNN梯度下降的过程" class="headerlink" title="回归RNN，我们来简单的描述下RNN梯度下降的过程"></a>回归RNN，我们来简单的描述下RNN梯度下降的过程</h2><h3 id="简单介绍传统神经网络的反向传播算法"><a href="#简单介绍传统神经网络的反向传播算法" class="headerlink" title="简单介绍传统神经网络的反向传播算法"></a>简单介绍传统神经网络的反向传播算法</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_17.png" alt=""></p><h3 id="再来看看真实的RNN到底是什么样子的"><a href="#再来看看真实的RNN到底是什么样子的" class="headerlink" title="再来看看真实的RNN到底是什么样子的"></a>再来看看真实的RNN到底是什么样子的</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_18.png" alt=""></p><h4 id="我们约定图中符号为："><a href="#我们约定图中符号为：" class="headerlink" title="我们约定图中符号为："></a>我们约定图中符号为：</h4><ul><li>t时刻的输入：$x^t\in R^{xdim}$</li><li>隐层节点的输出：$h^t\in R^{hdim}$</li><li>输出层的预测值:$y^t\in R^{ydim}$</li><li>从输入到隐层的权重矩阵：$V\in R^{xdim·ydim}$</li><li>隐层的自循环矩阵:$U\in R^{hdim·hdim}$</li><li>隐层到输出层的权重矩阵:$W\in R^{hdim·ydim}$</li><li>各层对应的偏置向量: $b_h\in R^{hdim},b_y\in R^{ydim}$</li><li>输入层、隐层、输出层的节点为标识为$i、j、k$</li><li>真实的输出：$d^t\in R^{ydim}$</li></ul><h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h4><ol><li>$h^t=activate_1(x^tV+h^{t-1}U+b_h)$，其中令$net_h^t=x^tV+h^{t-1}U+b_h$</li><li>$y^t=activate_2(h^tW+b_y)$,其中令$net_y^t=h^tW+b_y$</li><li>定义单个时间节点$p$的误差为<br> $$E^t=\sum_p\frac{1}{2}||d^t-y^t||^2$$</li><li><p>则有总误差为</p><p> $$E=\sum_tE^t=\frac{1}{2}\sum<em>p{\sum^T</em>{t=1}||y^t-d^t||^2}$$</p></li></ol><h4 id="反向传播：Backpropation-Through-Time-BPTT"><a href="#反向传播：Backpropation-Through-Time-BPTT" class="headerlink" title="反向传播：Backpropation Through Time(BPTT)"></a>反向传播：Backpropation Through Time(BPTT)</h4><ol><li>计算RNN内参数的梯度<br>$$\delta^t<em>{yk}=\frac{\partial{E}}{\partial{net^t</em>{yk}}}，\delta^t<em>{hj}=\frac{\partial{E}}{\partial^t</em>{hj}}$$</li></ol><blockquote><p>这两个偏导数是为：总误差分别对第t个时间节点的输出层的第k个节点&amp;隐藏层的第j个节点的偏导数</p></blockquote><p>展开则有：</p><p>$$\delta^t_{yk}={\frac{\partial{E}}{\partial{y^t_k}}}{\frac{\partial{y^t<em>k}}{\partial{net^t</em>{yk}}}}=(y^t_k-d^t<em>k){g^{‘}(net^t</em>{yk})}$$</p><p>将上式向量化表示有：<br>$$\delta_y^t=(y^t-d^t)\circ{g^{‘}(net^t_y)}, \circ表示对应元素相乘而非矩阵乘法$$<br>$$\delta_h^t=(W(\delta_y^t)^T+U(\delta^{t+1}_h)^T)^T\circ{f^{‘}(net^t_h)}$$<br>由此可得各个参数的梯度为：<br>$$\Delta{W}=\sum_t{(h^{t})^T\delta^t_y}$$</p><p>$$\Delta{U}=\sum_t{(h^{t-1})^T\delta^t_h}$$</p><p>$$\Delta{V}=\sum_t{(x^t)^T\delta^t_h}$$</p><p>$$\Delta{by}=\sum_t{\delta^t_y}$$</p><p>$$\Delta{bh}=\sum_t{\delta^t_h}$$</p><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_19.png" alt=""></p><blockquote><p>注意：从上图中可以很明显的观察到计算t时刻的梯度时，需要用到$t-1，t-2,…$时刻计算得到的梯度。</p></blockquote><h2 id="来用python写一个小demo（二进制加法器）试试？"><a href="#来用python写一个小demo（二进制加法器）试试？" class="headerlink" title="来用python写一个小demo（二进制加法器）试试？"></a>来用python写一个小demo（二进制加法器）试试？</h2><h3 id="我们的目标："><a href="#我们的目标：" class="headerlink" title="我们的目标："></a>我们的目标：</h3><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/RNN_t_20.png" alt=""></p><p>我们想要完成一个八位二进制加法器：进位从第三位开始！这个加法器可以直接预测两个八位二进制数的结果，并且我们想要RNN学会是否进位这个“记忆”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author  : zzy824</span></span><br><span class="line"><span class="comment"># @File    : traskRNNTutorial.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute sigmoid nonlinearity</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    output = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert output of sigmoid function to its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_output_to_derivative</span>(<span class="params">output</span>):</span><br><span class="line">    <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/4/24 下午2:43</span></span><br><span class="line"><span class="comment"># @Author  : zzy824</span></span><br><span class="line"><span class="comment"># @File    : traskRNNTutorial.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute sigmoid nonlinearity</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    output = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert output of sigmoid function to its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_output_to_derivative</span>(<span class="params">output</span>):</span><br><span class="line">    <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training dataset generation</span></span><br><span class="line">int2binary = &#123;&#125;</span><br><span class="line">binary_dim = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">largest_number = <span class="built_in">pow</span>(<span class="number">2</span>, binary_dim)</span><br><span class="line">binary = np.unpackbits(np.array([<span class="built_in">range</span>(largest_number)], dtype=np.uint8).T, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(largest_number):</span><br><span class="line">    int2binary[i] = binary[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># input variables</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">input_dim = <span class="number">2</span> <span class="comment"># because we add two number </span></span><br><span class="line">hidden_dim = <span class="number">16</span> </span><br><span class="line">output_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize neural network weights</span></span><br><span class="line">synapse_0 = <span class="number">2</span> * np.random.random((input_dim, hidden_dim)) - <span class="number">1</span> <span class="comment"># input&amp;hidden</span></span><br><span class="line">synapse_1 = <span class="number">2</span> * np.random.random((hidden_dim, output_dim)) - <span class="number">1</span> <span class="comment"># hidden&amp;output</span></span><br><span class="line">synapse_h = <span class="number">2</span> * np.random.random((hidden_dim, hidden_dim)) - <span class="number">1</span> <span class="comment"># hidden&amp;hidden</span></span><br><span class="line"></span><br><span class="line">synapse_0_update = np.zeros_like(synapse_0)</span><br><span class="line">synapse_1_update = np.zeros_like(synapse_1)</span><br><span class="line">synapse_h_update = np.zeros_like(synapse_h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training logic</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate a simple addition problem (a + b = c)</span></span><br><span class="line">    a_int = np.random.randint(largest_number / <span class="number">2</span>)  <span class="comment"># int version</span></span><br><span class="line">    a = int2binary[a_int]  <span class="comment"># binary encoding</span></span><br><span class="line">    b_int = np.random.randint(largest_number / <span class="number">2</span>)  <span class="comment"># int version</span></span><br><span class="line">    b = int2binary[b_int]  <span class="comment"># binary encoding</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># true answer</span></span><br><span class="line">    c_int = a_int + b_int</span><br><span class="line">    c = int2binary[c_int]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># where we&#x27;ll store our best guess (binary encoded)</span></span><br><span class="line">    d = np.zeros_like(c)</span><br><span class="line"></span><br><span class="line">    overallError = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    layer_2_deltas = <span class="built_in">list</span>()</span><br><span class="line">    layer_1_values = <span class="built_in">list</span>()</span><br><span class="line">    layer_1_values.append(np.zeros(hidden_dim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># moving along the positions in the binary encoding</span></span><br><span class="line">    <span class="keyword">for</span> position <span class="keyword">in</span> <span class="built_in">range</span>(binary_dim):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># generate input and output</span></span><br><span class="line">        X = np.array([[a[binary_dim - position - <span class="number">1</span>], b[binary_dim - position - <span class="number">1</span>]]])</span><br><span class="line">        y = np.array([[c[binary_dim - position - <span class="number">1</span>]]]).T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden layer (input ~+ prev_hidden)</span></span><br><span class="line">        layer_1 = sigmoid(np.dot(X, synapse_0) + np.dot(layer_1_values[-<span class="number">1</span>], synapse_h))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output layer (new binary representation)</span></span><br><span class="line">        layer_2 = sigmoid(np.dot(layer_1, synapse_1))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># did we miss?... if so, by how much?</span></span><br><span class="line">        layer_2_error = y - layer_2</span><br><span class="line">        layer_2_deltas.append(layer_2_error * sigmoid_output_to_derivative(layer_2))</span><br><span class="line">        overallError += np.<span class="built_in">abs</span>(layer_2_error[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decode estimate so we can print it out</span></span><br><span class="line">        d[binary_dim - position - <span class="number">1</span>] = np.<span class="built_in">round</span>(layer_2[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># store hidden layer so we can use it in the next timestep</span></span><br><span class="line">        layer_1_values.append(copy.deepcopy(layer_1))</span><br><span class="line"></span><br><span class="line">    future_layer_1_delta = np.zeros(hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> position <span class="keyword">in</span> <span class="built_in">range</span>(binary_dim):</span><br><span class="line">        X = np.array([[a[position], b[position]]])</span><br><span class="line">        layer_1 = layer_1_values[-position - <span class="number">1</span>]</span><br><span class="line">        prev_layer_1 = layer_1_values[-position - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># error at output layer</span></span><br><span class="line">        layer_2_delta = layer_2_deltas[-position - <span class="number">1</span>]</span><br><span class="line">        <span class="comment"># error at hidden layer</span></span><br><span class="line">        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(</span><br><span class="line">            layer_1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># let&#x27;s update all our weights so we can try again</span></span><br><span class="line">        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)</span><br><span class="line">        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)</span><br><span class="line">        synapse_0_update += X.T.dot(layer_1_delta)</span><br><span class="line"></span><br><span class="line">        future_layer_1_delta = layer_1_delta</span><br><span class="line"></span><br><span class="line">        synapse_0 += synapse_0_update * alpha</span><br><span class="line">        synapse_1 += synapse_1_update * alpha</span><br><span class="line">        synapse_h += synapse_h_update * alpha</span><br><span class="line"></span><br><span class="line">        synapse_0_update *= <span class="number">0</span></span><br><span class="line">        synapse_1_update *= <span class="number">0</span></span><br><span class="line">        synapse_h_update *= <span class="number">0</span></span><br><span class="line">        <span class="comment"># print out progress</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Error:&quot;</span> + <span class="built_in">str</span>(overallError)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Pred:&quot;</span> + <span class="built_in">str</span>(d)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;True:&quot;</span> + <span class="built_in">str</span>(c)</span><br><span class="line">            out = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> index, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(d)):</span><br><span class="line">                out += x * <span class="built_in">pow</span>(<span class="number">2</span>, index)</span><br><span class="line">            <span class="built_in">print</span> <span class="built_in">str</span>(a_int) + <span class="string">&quot; + &quot;</span> + <span class="built_in">str</span>(b_int) + <span class="string">&quot; = &quot;</span> + <span class="built_in">str</span>(out)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;------------&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>引用与参考资料</p><ol><li>pytorch官方文档：<a href="http://pytorch.org/tutorials/">http://pytorch.org/tutorials/</a></li><li>pytorch官方项目样例：<a href="https://github.com/pytorch/examples">https://github.com/pytorch/examples</a></li><li>colah博客：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li>RNN求解过程：<a href="https://www.cnblogs.com/YiXiaoZhou/p/6058890.html">https://www.cnblogs.com/YiXiaoZhou/p/6058890.html</a></li><li>i am trask博客: <a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li><li>反向传播算法图来源：<a href="https://zhuanlan.zhihu.com/p/31623305">https://zhuanlan.zhihu.com/p/31623305</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> DeepLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rnn </tag>
            
            <tag> lstm </tag>
            
            <tag> deepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ionic常见报错解决方案&amp;常用命令</title>
      <link href="/2018/04/17/web-app-ionic/"/>
      <url>/2018/04/17/web-app-ionic/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="常见报错"><a href="#常见报错" class="headerlink" title="常见报错"></a>常见报错</h2><ul><li><p><strong>npm WARN checkPermissions Missing write access to</strong><br><strong>Solution</strong></p><ol><li><p>方案一</p><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/web-app-ionic-1.png" alt=""></p></li><li><p>方案二</p><ul><li>sudo npm i -g npm // 重新全局安装npm</li><li>sudo chown -R your_user_name /usr/local/lib // 修改npm文件夹的权限</li></ul></li></ol></li><li><p>Downloading and extracting blank starter - failed!Error: socket hang up<br><strong>Solution</strong>:</p></li></ul><ol><li>针对特定版本的解决方案：<a href="https://www.jianshu.com/p/6509070114e2">https://www.jianshu.com/p/6509070114e2</a></li><li>sudo cnpm install -g ionic cordova // 需要记住的核心思想：<strong>能用cnpm尽量不要用npm</strong></li><li>ionic start myApp <a href="https://github.com/ionic-team/starters">https://github.com/ionic-team/starters</a> // 如果模板源的网站没有响应，则可以尝试直接去github上clone源代码</li></ol><ul><li><p>ERROR] The package.json file seems malformed.<br><strong>Solution</strong>:</p><pre><code>In to of package.json at the root(base level), add ‘name’ key like &quot;name&quot;: &quot;My_App_Name&quot;,.Also at same base level add &quot;author&quot;: &quot;YourName&quot;,</code></pre></li><li><p>如果需要安装依赖</p><p>  cnpm install –save （安装完一些包之后要记得save，这个步骤十分容易忘记） </p></li><li><p>➜  ionicdemo ionic serve<br>Error: Cannot find module ‘tslint’<br>  at Function.Module._resolveFilename (module.js:547:15)<br>  at Function.Module._load (module.js:474:25)<br>  at Module.require (module.js:596:17)<br>  at require (internal/module.js:11:18)<br>  at Object.<anonymous> (/Users/zzy824/ionicPrijects/ionicdemo/node<em>modules/</em>@ionic_app-scripts@3.1.8@@ionic/app-scripts/dist/lint/lint-factory.js:3:16)<br>  at Module._compile (module.js:652:30)<br>  at Object.Module._extensions..js (module.js:663:10)<br>  at Module.load (module.js:565:32)<br>  at tryModuleLoad (module.js:505:12)<br>  at Function.Module._load (module.js:497:3)</p><p>  <strong>solution</strong>:</p><pre><code>&gt; rm -rf node_modules  &gt; npm restart&gt; npm update  &gt; cnpm install --save  </code></pre></li><li><p>npm install出现”Unexpected end of JSON input while parsing near” </p></li></ul><p>npm cache clean –force</p><ul><li>ionic中调用摄像头</li></ul><p>提前准备plugman：一个专门为了cordova提供插件的命令行工具<br>        npm installd -g plugman<br>第一步是安装ionic-native/core。所有用到Native的功能，这一步不能省。<br>        npm install @ionic-native/core –save</p><ul><li>“/usr/bin/env: node: No such file or directory”<br>软链接文件： <code>ln -s /usr/bin/nodejs /usr/bin/node</code></li></ul><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载依赖包</span></span><br><span class="line">cnpm i </span><br><span class="line"><span class="comment">//移动端启动</span></span><br><span class="line">ionic serve -l </span><br><span class="line"></span><br><span class="line"><span class="comment">//在一个连接的设备上运行项目</span></span><br><span class="line">ionic cordova run [platform] [options]</span><br><span class="line">ionic cordova run [ios/android] [-lc]</span><br><span class="line"></span><br><span class="line"><span class="comment">//在ios手机上运行需要安装平台</span></span><br><span class="line">ionic cordova platform add ios</span><br><span class="line">ionic cordova platform add android</span><br><span class="line"></span><br><span class="line"><span class="comment">//在平台下修改代码后打包</span></span><br><span class="line">ionic build</span><br><span class="line"><span class="comment">//在phoneGap上做测试</span></span><br><span class="line">ionic cordova plugin add phonegap-plugin-local-notification</span><br><span class="line">npm install --save @ionic-native/phonegap-local-notification</span><br><span class="line"><span class="comment">//声明一个组件</span></span><br><span class="line">ionic g component [component name] </span><br><span class="line"><span class="comment">//声明一个新页面</span></span><br><span class="line">ionic g page [page name]</span><br></pre></td></tr></table></figure><blockquote><p>引用与参考</p><ol><li><a href="https://blog.csdn.net/yuxiaofan1245023886/article/details/52251410">https://blog.csdn.net/yuxiaofan1245023886/article/details/52251410</a></li><li><a href="http://www.jb51.net/article/115645.htm">http://www.jb51.net/article/115645.htm</a></li><li><a href="https://github.com/nodejs/node-v0.x-archive/issues/3911">https://github.com/nodejs/node-v0.x-archive/issues/3911</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> App </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ionic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>量化投资</title>
      <link href="/2018/04/16/quantitative-investment-summary/"/>
      <url>/2018/04/16/quantitative-investment-summary/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="什么是量化投资？"><a href="#什么是量化投资？" class="headerlink" title="什么是量化投资？"></a>什么是量化投资？</h1><p>股市被认为是混乱的，复杂的，波动的和动态的。通过计算机<strong>程序化</strong>发出买卖指令，以获取<strong>稳定收益</strong>为目的的交易方式。通俗地讲，就是预先设定一套<strong>固定的逻辑</strong>，点明选股及何时买入何时卖出的标准。实盘操作时，实时接收行情数据并进行分析，当达到预先设定的标准时，即<strong>自动进行买入卖出</strong>的操作。</p><h1 id="量化投资的优越性到底在哪里？"><a href="#量化投资的优越性到底在哪里？" class="headerlink" title="量化投资的优越性到底在哪里？"></a>量化投资的优越性到底在哪里？</h1><p>打个比方，我们可以作一个机器人王亚伟。我们可以根据王亚伟什么时候买进，什么时候卖出，我们把一个股票分解成60多个因子。比如，基本面是什么样的，分析员是怎么说的，现在的价、量是什么情况，列出60几个因子。<br>其中，哪几个因子绿灯，是它该买的时候；该卖出的时候，哪几个灯灭掉了。<br>一一对比之后，你知道他买的股票，看重哪几个因子。<br>这样，按照他的方法，我们可以整个市场几千只股票全部扫一遍，计算机每天大概几分钟就扫完了，晒出一大批王亚伟类型的股票。<br>按这个方法去做，实际上可以了一个机器人王亚伟。</p><ul><li>就是把所有的投资决定分散，分地越散越好，而人类的物理性能无法与计算机相匹敌。</li><li>求胜率。每一个投资决定的胜率只要高出平均水平，达到51%。把这两步做到了之后，基本上量化就没有问题了。</li></ul><hr><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/quantitative_investment_1.png" alt=""></p><hr><h1 id="人工智能时代的量化投资策略"><a href="#人工智能时代的量化投资策略" class="headerlink" title="人工智能时代的量化投资策略"></a>人工智能时代的量化投资策略</h1><p>人工智能的浪潮已经席卷生活中越来越多的领域：语音识别、图像识别、风控监控、智能推荐、无人驾驶……<br>机器开始在许多方面逐步代替人类的工作，大大解放了人类的双手，在某些领域还能完成许多人类无法胜任的工作，那么投资如何？</p><h2 id="案例：利用LSTM预测真实金融数据"><a href="#案例：利用LSTM预测真实金融数据" class="headerlink" title="案例：利用LSTM预测真实金融数据"></a>案例：利用LSTM预测真实金融数据</h2><ol><li>通过合理途径获取到金融历史数据</li><li><p>数据清理与归一化</p><ul><li>去极值：跳水比赛或体操比赛中，通常会去掉最高分和最低分，在剩下的裁判打分中取平均值，同样的道理，对于某个单因子，3000多只股票就对应3000多个因子值，有些高的离谱，有些低的离谱，这些异常值就像跳水和体操比赛打分里的极端值，通常给予剔除的处理。</li><li>因子值标准化：试想一下如果小王同学用两个因子来择偶，一个是身高，一个是颜值，身高的取值范围在150-180cm，颜值的取值在0-10分，这两个因子的单位（量纲）都不同，自然不能够相加，这可怎么办呢，小王想了个办法，身高180cm对应10分，150cm对应0分，中间的身高对应得分则用下图的函数映射给出，这个过程就是因子值的标准化。</li><li>值、行业中性处理：每个单因子对股价的影响力都有着千丝万缕的联系，比如市盈率因子与市值因子对股价都有影响，小市值的股票里也有市盈率低的股票，市盈率低的股票也不都是大票。</li></ul><ol><li>特征降维：（主成分分析法）</li></ol></li><li><p>制作数据集：具体操作是：取当前时刻及其之前的N段数据构成序列，将每一段序列作为一个新的数据点。另外，该学习方法属于监督学习，每一个数据都要有其对应的标签（label），我们取其下一时刻的涨跌情况作为label，按照涨跌幅度分为五类，使其近似满足正态分布。</p></li><li><p>划分训练集与测试集</p></li><li>构建LSTM神经网络，开始训练</li><li>编写交易策略。最简单的策略即根据预测涨跌情况直接进行交易选择，如未来连续几个时间点上涨即买入，连续几个时间点下跌即卖出。当然也可以与一般的交易策略结合，如预测金叉与死叉点等，并加入止盈止损策略。</li></ol><h1 id="仰望星空，深度学习我们应该掌握的原理有什么？"><a href="#仰望星空，深度学习我们应该掌握的原理有什么？" class="headerlink" title="仰望星空，深度学习我们应该掌握的原理有什么？"></a>仰望星空，深度学习我们应该掌握的原理有什么？</h1><blockquote><p>近年来，深度神经网络在语音、图像领域取得突出进展，以至于很多人将深度学习与深度神经网络等同视之。</p></blockquote><h2 id="什么是深度学习？"><a href="#什么是深度学习？" class="headerlink" title="什么是深度学习？"></a>什么是深度学习？</h2><p>我们都知道现在人工智能很热，掀起这股的热潮最重要的技术之一就是深度学习技术。今天当我们谈到深度学习的时候，其实已经可以看到在各种各样的应用，包括图像、视频、声音、自然语言处理等等。如果我们问一个问题，什么是深度学习？大多数人基本会认为，深度学习差不多就等于深度神经网络。</p><p><strong>深度学习不等于深度神经网络</strong>：深度强化学习（D.silver&amp;Sutton）、gcForest（周志华）.etc</p><h2 id="什么是神经网络？"><a href="#什么是神经网络？" class="headerlink" title="什么是神经网络？"></a>什么是神经网络？</h2><ul><li><p>神经网络长成什么样？：类比人脑，（简单的计算模型）</p><ul><li>数学表达形式：$y = a(W*x+b)$;</li><li>形象的理解便是：1.升维/降维 2.放大/缩小 3. 旋转 4.平移 5.<strong>“弯曲”</strong> （保证连续，可微分—&gt;BP算法才有用武之地！） </li></ul></li><li><p>神经网络的训练过程如何？</p><ul><li>loss function(目标与预测差距有多少) 【我看到一直老虎却认成一只猫，loss就在于“王”上】</li><li>梯度下降：loss值向当前点对应梯度的反方向不断移动，来降低loss。（BP算法）</li></ul></li></ul><h2 id="为什么是深度学习而不是宽度学习？"><a href="#为什么是深度学习而不是宽度学习？" class="headerlink" title="为什么是深度学习而不是宽度学习？"></a>为什么是深度学习而不是宽度学习？</h2><ul><li>深度学习泛函表达能力会更强（学习到特征的能力会更强，<strong>逐层</strong>抽象能力更强）</li><li>神经网络的万有逼近能力（泰勒公式、傅里叶变换等）</li></ul><h1 id="脚踏实地，让我们谈谈到底该使用什么工具？"><a href="#脚踏实地，让我们谈谈到底该使用什么工具？" class="headerlink" title="脚踏实地，让我们谈谈到底该使用什么工具？"></a>脚踏实地，让我们谈谈到底该使用什么工具？</h1><ul><li>pytorch</li><li>numpy</li><li>pandas</li></ul><h1 id="学术界与工业界的前沿探索"><a href="#学术界与工业界的前沿探索" class="headerlink" title="学术界与工业界的前沿探索"></a>学术界与工业界的前沿探索</h1><ul><li>通过预测公司基本面来改善基于因子的量化投资</li></ul><p>上市公司需要定期发布报告反映公司基本面的财务数据，如收入，营业收入，债务等。这些数据点为公司的财务状况提供了一些参考。学术研究已经验证了一些有效因子，即通过回测分析历史报告数据的计算因子，可以获得超越市场平均水平的表现。其中，两个受欢迎的因子是账面价值（按市值归一化调整）和营业收入（按EBIT / EV归一化调整）。通过回测表明，如果我们能够（透视）选择使用基于未来基本面计算的因子（通过预测）来选择股票，那么我们的投资组合将远远超越标准因子选股方法。受此分析的启发，我们训练深度神经网络以预测未来5年的基本面数据。</p><ul><li>使用机器学习算法预测ETF</li></ul><p>专注于预测几个具有流动性的ETF的涨跌方向（向上或向下），并不试图预测价格变化的幅度。结论：1. 短期价格服从随机游走假设 2. 横截面和跨期的成交量对于一个强大的信息集的重要性 3. 大量特征是可预测性所必需的，因为每个特征提供的贡献非常小。 4. ETFs可以用机器学习算法进行预测，但实践者应该将先前的市场和直觉知识纳入资产类别行为。</p><hr><blockquote><p>引用参考资料和版权说明<br>    <a href="https://zhuanlan.zhihu.com/p/33430725">https://zhuanlan.zhihu.com/p/33430725</a><br>    <a href="https://zhuanlan.zhihu.com/p/29451486">https://zhuanlan.zhihu.com/p/29451486</a><br>    <a href="https://zhuanlan.zhihu.com/p/22260743">https://zhuanlan.zhihu.com/p/22260743</a><br>    <a href="https://zhuanlan.zhihu.com/p/25919734">https://zhuanlan.zhihu.com/p/25919734</a><br>    <a href="https://zhuanlan.zhihu.com/p/26037052">https://zhuanlan.zhihu.com/p/26037052</a><br>    <a href="https://zhuanlan.zhihu.com/p/35044817">https://zhuanlan.zhihu.com/p/35044817</a><br>    <a href="https://mp.weixin.qq.com/s/ROk5lK5gWj6pl4-ebrHG_A">https://mp.weixin.qq.com/s/ROk5lK5gWj6pl4-ebrHG_A</a><br>    <a href="https://www.zhihu.com/question/22553761/answer/36429105">https://www.zhihu.com/question/22553761/answer/36429105</a><br>    <a href="https://arxiv.org/pdf/1711.04837.pdf">https://arxiv.org/pdf/1711.04837.pdf</a><br>    <a href="https://arxiv.org/pdf/1801.01777.pdf">https://arxiv.org/pdf/1801.01777.pdf</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> QuantitativeInvestment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machinelearning </tag>
            
            <tag> rnn </tag>
            
            <tag> lstm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解谜计算机科学-王垠著（连载中）</title>
      <link href="/2018/03/15/yin-wang-riddle-computer-science/"/>
      <url>/2018/03/15/yin-wang-riddle-computer-science/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="绪论：解谜计算机科学"><a href="#绪论：解谜计算机科学" class="headerlink" title="绪论：解谜计算机科学"></a>绪论：解谜计算机科学</h2><h3 id="写作动机"><a href="#写作动机" class="headerlink" title="写作动机"></a>写作动机</h3><blockquote><p>爱因斯坦：如果你不能把一个问题跟六岁小孩解释清楚，那你并不真的理解它</p></blockquote><h3 id="写作目标"><a href="#写作目标" class="headerlink" title="写作目标"></a>写作目标</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(你看过此书):</span><br><span class="line">    <span class="keyword">while</span>(世界末日 and 计算机不复存在):</span><br><span class="line">        你可以从头开始制作计算机,cool!</span><br><span class="line">```     </span><br><span class="line">## 第一章：初识计算</span><br><span class="line">### 计算的本质</span><br><span class="line">- 手指算数便是最简单的计算机</span><br><span class="line">- 计算图：抽象是计算机科学至关重要的方法</span><br><span class="line">![](http:<span class="comment">//www.yinwang.org/csbook-images/adder.png)</span></span><br><span class="line">![](http:<span class="comment">//www.yinwang.org/csbook-images/add-mult.png)</span></span><br><span class="line">- 并行计算</span><br><span class="line">![](http:<span class="comment">//www.yinwang.org/csbook-images/parallel.png)</span></span><br><span class="line">#### 并行计算虽好，但是还会引发其他的问题：</span><br><span class="line">- 并行计算的计算机运算速度不一样：等待时间太长了！</span><br><span class="line">- 计算机之间的通信开销会极大地降低效率</span><br><span class="line">- 编译</span><br><span class="line"></span><br><span class="line">``` js</span><br><span class="line">(<span class="number">5</span> - <span class="number">3</span>) * (<span class="number">4</span> + (<span class="number">2</span> * <span class="number">3</span> - <span class="number">5</span>) * <span class="number">6</span>)</span><br><span class="line">===&gt;</span><br><span class="line">&#123;</span><br><span class="line">    a = <span class="number">2</span> * <span class="number">3</span></span><br><span class="line">    b = a - <span class="number">5</span></span><br><span class="line">    c = b * <span class="number">6</span></span><br><span class="line">    d = <span class="number">4</span> + c</span><br><span class="line">    e = <span class="number">5</span> - <span class="number">3</span></span><br><span class="line">    e * d</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使语句的结果与原来的表达式完全一致。这种保留原来语义的翻译过程，叫做编译"><a href="#使语句的结果与原来的表达式完全一致。这种保留原来语义的翻译过程，叫做编译" class="headerlink" title="使语句的结果与原来的表达式完全一致。这种保留原来语义的翻译过程，叫做编译"></a>使语句的结果与原来的表达式完全一致。这种保留原来语义的翻译过程，叫做编译</h4><ul><li>函数<br>考虑如下的场景：我们想要表达一个“风扇控制器”，风扇的转速总是当前气温的两倍。<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t-&gt;t*<span class="number">2</span> <span class="comment">//这便是我们的风扇控制器，也就是一个最简单的函数：</span></span><br><span class="line"><span class="title function_">f</span>(t) = t*<span class="number">2</span> <span class="comment">//让我们更规范一些</span></span><br><span class="line"><span class="title function_">f</span>(<span class="number">2</span>)   <span class="comment">// 值为4</span></span><br></pre></td></tr></table></figure></li><li><p>分支<br>考虑如下的场景：我们想要一个“饮料选择器”</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t-&gt; <span class="keyword">if</span>(t &lt; <span class="number">22</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;hotpot&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;ice cream&quot;</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>总结：计算的要素</p><ol><li>基础的数值。（整数，字符串，布尔值等等）</li><li>表达式。（基本的算数表达式）</li><li>变量和赋值语句。</li><li>分支语句</li><li><p>函数和函数调用</p><p>像学开车一样，一旦你掌握了油门，刹车，换挡器，方向盘，速度表的功能和用法，你就学会了开所有的汽车，不管它是什么型号的汽车。</p></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> ComputerScience </category>
          
      </categories>
      
      
        <tags>
            
            <tag> readingNote </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北邮复试笔试复习</title>
      <link href="/2018/02/20/BUPT-exam-note/"/>
      <url>/2018/02/20/BUPT-exam-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>这是我自己复习的时候做的笔记，需要的同学可以简单浏览，最主要的还是多去做真题。</p></blockquote><h1 id="第一门：软件工程"><a href="#第一门：软件工程" class="headerlink" title="第一门：软件工程"></a>第一门：软件工程</h1><h2 id="导学：软件开发过程-6点"><a href="#导学：软件开发过程-6点" class="headerlink" title="导学：软件开发过程:6点"></a>导学：软件开发过程:6点</h2><ul><li>做什么：需求分析</li><li>怎么做：系统设计</li><li>具体做：系统开发</li><li>检查对不对：测试人员</li><li>让用户用：工程实施人员</li><li>维护系统：系统维护人员</li></ul><h2 id="软件工程概述"><a href="#软件工程概述" class="headerlink" title="软件工程概述"></a>软件工程概述</h2><blockquote><p>软件的定义：软件=程序+数据+文档</p></blockquote><h3 id="软件的分类"><a href="#软件的分类" class="headerlink" title="软件的分类"></a>软件的分类</h3><ol><li>系统软件：操作系统、设备驱动程序、通信处理程序</li><li>中间件软件：各种中间件，应用服务器</li><li>应用软件：特定领域内开发，为特定目的服务的一类软件</li></ol><h3 id="软件工程三要素"><a href="#软件工程三要素" class="headerlink" title="软件工程三要素"></a>软件工程三要素</h3><ol><li><strong>方法</strong>：如何做的技术</li><li><strong>工具</strong>：软件支撑环境</li><li><strong>过程</strong>：综合软件工程的方法和工具</li></ol><h3 id="八条一般原理"><a href="#八条一般原理" class="headerlink" title="八条一般原理"></a>八条一般原理</h3><blockquote><p>1.抽象 2.信息隐藏 3.模块化 4.局部化 5.确定性 6.一致性 7.完备性 8.可验证性</p></blockquote><h2 id="软件生命周期模型"><a href="#软件生命周期模型" class="headerlink" title="软件生命周期模型"></a>软件生命周期模型</h2><ul><li>瀑布模型：阶段性顺序串行</li><li>原型方法：通过小型原型软件添枝加叶</li><li>演化模型：综合瀑布模型和原型方法，两次开发。</li><li>增量模型：建立原型然后增量更新</li><li>螺旋模型：引入了明确的风险管理的四个象限（制定计划、风险分析、实施工程、客户评价）</li><li>喷泉模型：佛系开发，无章法。</li><li>构建组装模型：模块化组件开发</li><li>快速应用开发模型：快速增量型开发</li><li>统一过程模型：博采众家之长</li><li><p>敏捷模型：<strong>一种态度，非方法论</strong></p><blockquote><p>极限编程：最新理论成果</p><ul><li>小版本高速迭代</li><li>简单设计</li><li>结对编程</li><li>代码共有</li></ul></blockquote></li></ul><h2 id="系统需求分析和可行性分析"><a href="#系统需求分析和可行性分析" class="headerlink" title="系统需求分析和可行性分析"></a>系统需求分析和可行性分析</h2><blockquote><p>系统需求分析和可行性分析的目的：明确系统是否值得做，避免投资损失。</p></blockquote><ul><li>需求规格说明书：软件验收的依据（<strong>不包括可行性研究</strong>）；软件要做什么的共同理解；软件设计的依据</li></ul><h3 id="可行性分析：经济、技术、法律、实施"><a href="#可行性分析：经济、技术、法律、实施" class="headerlink" title="可行性分析：经济、技术、法律、实施"></a>可行性分析：<strong>经济</strong>、<strong>技术</strong>、<strong>法律</strong>、<strong>实施</strong></h3><h3 id="技术可行性分析：开发风险、资源可用性、技术条件"><a href="#技术可行性分析：开发风险、资源可用性、技术条件" class="headerlink" title="技术可行性分析：开发风险、资源可用性、技术条件"></a>技术可行性分析：<strong>开发风险</strong>、<strong>资源可用性</strong>、<strong>技术条件</strong></h3><h2 id="软件需求分析"><a href="#软件需求分析" class="headerlink" title="软件需求分析"></a>软件需求分析</h2><h3 id="需求分析的操作性原则"><a href="#需求分析的操作性原则" class="headerlink" title="需求分析的操作性原则"></a>需求分析的操作性原则</h3><ul><li>问题的信息域必须被表示和理解，即<strong>数据模型</strong>(ER图)</li><li>软件将完成的工程必须被定义，即<strong>功能模型</strong>(数据流图)</li><li>软件的行为必须被表示，即<strong>行为模型</strong>(状态迁移图)</li></ul><h2 id="结构化需求分析"><a href="#结构化需求分析" class="headerlink" title="结构化需求分析"></a>结构化需求分析</h2><blockquote><p>数据建模：实体关系图（ER图）把用户的数据要求表达出来所建立的概念性的模型</p></blockquote><ul><li>第一范式：每个属性值都是不可在分的最小数据单位（详细至极）</li><li>第二范式：非主属性<strong>完全依赖</strong>于关键字（非主属性开始拆分出去）</li><li>第三范式：非主属性相互独立，消除函数依赖（关系通过外键连接，形成多个关系集合）</li></ul><h2 id="软件设计"><a href="#软件设计" class="headerlink" title="软件设计"></a>软件设计</h2><blockquote><p>总体设计包括：<strong>处理方式</strong>设计；<strong>数据结构</strong>设计；<strong>可靠性</strong>设计：</p></blockquote><h3 id="模块化："><a href="#模块化：" class="headerlink" title="模块化："></a>模块化：</h3><ul><li>模块的三个基本属性：<strong>功能</strong>、<strong>逻辑</strong>、<strong>状态</strong></li><li>度量准则：模块间的耦合和模块间的内聚<ul><li>内聚:模块功能强度的度量：最低为<strong>巧合内聚</strong>，最高为<strong>功能内聚</strong></li><li>耦合：模块之间的相对独立性<ul><li>最好使用数据耦合</li><li>完全不用内容耦合</li><li>少用控制耦合</li></ul></li></ul></li></ul><h2 id="结构化设计方法"><a href="#结构化设计方法" class="headerlink" title="结构化设计方法"></a>结构化设计方法</h2><ul><li>扇入：是指直接调用该模块的上级模块的个数。扇入大表示模块的<strong>复用程度高</strong>。</li><li>扇出: 是指该模块直接调用的下级模块的个数。扇出大表示模块的<strong>复杂度高</strong>。</li></ul><blockquote><p>用例图–功能需求模型– 用例模型–领域模型–识别系统–系统操作契约</p></blockquote><h2 id="面向对象基本思想：一切都看成是对象"><a href="#面向对象基本思想：一切都看成是对象" class="headerlink" title="面向对象基本思想：一切都看成是对象"></a>面向对象基本思想：一切都看成是对象</h2><blockquote><p>面向对象 = 对象 + 类 + 继承 + 通信</p></blockquote><ul><li>对象：类的实例</li><li>类：</li><li>关联和链</li><li>继承/泛华</li><li>聚合</li></ul><h3 id="几种经典的面向对象的分析和设计方法"><a href="#几种经典的面向对象的分析和设计方法" class="headerlink" title="几种经典的面向对象的分析和设计方法"></a>几种经典的面向对象的分析和设计方法</h3><ul><li>OOA/OOD（面向对象分析、面向对象设计）：概念清晰，简单易学</li><li>Booch方法：四个主图（实体）和两个辅图（状态）</li><li>对象建模技术（OMT）</li><li>面向对象软件工程方法（OOSE）</li></ul><h3 id="统一建模语言UML（类似于状态图）"><a href="#统一建模语言UML（类似于状态图）" class="headerlink" title="统一建模语言UML（类似于状态图）"></a>统一建模语言UML（类似于状态图）</h3><ul><li>静态图：<ul><li>用例图</li><li>类图</li><li>对象图</li><li>构件图</li><li>部署图</li></ul></li><li>动态图：<ul><li>顺序图</li><li>协作图</li><li>状态图</li><li>活动图</li></ul></li></ul><h2 id="面向对象分析"><a href="#面向对象分析" class="headerlink" title="面向对象分析"></a>面向对象分析</h2><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><blockquote><p>用例图-&gt;功能需求模型-&gt;用例模型-&gt;领域模型-&gt;识别系统操作-&gt;建立系统操作契约 </p></blockquote><ol><li>用例建模</li><li>创建领域类型</li><li>绘制系统顺序图</li><li>创建系统操作契约</li></ol><h2 id="面向对象设计"><a href="#面向对象设计" class="headerlink" title="面向对象设计"></a>面向对象设计</h2><h3 id="模型的层次化"><a href="#模型的层次化" class="headerlink" title="模型的层次化"></a>模型的层次化</h3><h3 id="面向对象设计原则"><a href="#面向对象设计原则" class="headerlink" title="面向对象设计原则"></a>面向对象设计原则</h3><ol><li>单一职责原则(SRP)：就一个类而言，应该仅有一个引起它变化的原因</li><li>开闭原则：软件实体应该是可以扩展但是不可以修改的。</li><li>里氏替换原则：子类可以替换父类并出现在父类能够出现的任何地方。</li><li>依赖倒置原则：高层模块不应该依赖于底层模块，二者都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。</li><li>接口隔离原则:采用多个接口好于采用一个接口来涵盖多个业务方法</li><li>组合/聚合复用原则：在一个新对象里面使用一些已有对象，使之成为新对象的一部分。（达到复用目的）</li><li>迪米特法则（最少知识法则）：一个对象应当尽可能少的了解其他对象。</li></ol><h3 id="设计用例实现方案"><a href="#设计用例实现方案" class="headerlink" title="设计用例实现方案"></a>设计用例实现方案</h3><h2 id="软件实现"><a href="#软件实现" class="headerlink" title="软件实现"></a>软件实现</h2><ul><li>程序设计语言与集成开发环境</li><li>程序设计方法<ul><li>结构化程序设计方法：自顶向下，逐步求精<ul><li>三种基本控制结构：<strong>顺序、选择、重复</strong></li></ul></li><li>面向对象程序设计方法</li></ul></li><li>程序设计风格</li><li>程序效率<blockquote><p>指程序执行速度和占用的内存存储空间</p></blockquote></li></ul><h2 id="软件测试"><a href="#软件测试" class="headerlink" title="软件测试"></a>软件测试</h2><ul><li>软件测试基础：<strong>发现错误而非证明软件正确</strong></li><li>软件的正确性：软件产品达到预期的功能</li><li>软件测试<strong>不包括对代码进行调试</strong><h3 id="软件测试方法与技术"><a href="#软件测试方法与技术" class="headerlink" title="软件测试方法与技术"></a>软件测试方法与技术</h3></li><li>静态测试</li><li>动态测试<ul><li>黑盒测试：单纯从外部<ul><li>等价类划分</li><li>边界值分析</li><li>错误推测法</li><li>因果图</li></ul></li><li>白盒测试：证明内部操作和内部工作流程<ul><li>基本路径测试：<strong>最强</strong></li><li>语句覆盖：<strong>最弱</strong></li><li>判断覆盖</li><li>条件覆盖</li><li>判断-条件覆盖</li><li>条件组合覆盖</li></ul></li></ul></li></ul><h3 id="软件测试过程"><a href="#软件测试过程" class="headerlink" title="软件测试过程"></a>软件测试过程</h3><ul><li>单元测试：白盒为主，黑盒测试为辅</li><li>集成测试：测试所有模块组成的系统</li><li>确认测试：验证软件的有效性</li><li><p>系统测试：不同实际运行环境下测试</p><ul><li>恢复测试</li><li>压力测试</li><li>性能测试</li><li>安全测试</li></ul></li><li><p>何时停止测试：一定测试时间内出故障的次数。</p></li></ul><h3 id="非重点"><a href="#非重点" class="headerlink" title="非重点"></a>非重点</h3><h4 id="面向对象的测试方法"><a href="#面向对象的测试方法" class="headerlink" title="面向对象的测试方法"></a>面向对象的测试方法</h4><h4 id="程序的静态分析方法"><a href="#程序的静态分析方法" class="headerlink" title="程序的静态分析方法"></a>程序的静态分析方法</h4><h4 id="软件调试方法"><a href="#软件调试方法" class="headerlink" title="软件调试方法"></a>软件调试方法</h4><h4 id="软件测试工具"><a href="#软件测试工具" class="headerlink" title="软件测试工具"></a>软件测试工具</h4><h4 id="软件的可靠性"><a href="#软件的可靠性" class="headerlink" title="软件的可靠性"></a>软件的可靠性</h4><hr><h1 id="第二门：编译原理"><a href="#第二门：编译原理" class="headerlink" title="第二门：编译原理"></a>第二门：编译原理</h1><h2 id="编译概述"><a href="#编译概述" class="headerlink" title="编译概述"></a>编译概述</h2><ol><li>编译、解释和翻译的概念</li></ol><ul><li>编译：<strong>把源程序转换成等价的目标程序的过程</strong>(高-&gt;低)</li><li>解释：<strong>解释执行源程序，不生成目标程序</strong>（python）</li><li>翻译：<strong>将用某种语言编写的程序转换成另一种语言形式的程序的程序</strong>（高-&gt;高、低-&gt;低）</li></ul><ol><li>编译的阶段、任务、典型结构<ol><li>分析阶段:<ul><li>词法分析：<strong>从左到右一个字符一个字符地读入源程序</strong></li><li>语法分析：<strong>将单词序列组合成各类语法短语</strong></li><li>语义分析:<strong>对结构上正确的源程序进行上下文有关性质的审查</strong></li></ul></li><li>综合阶段：<strong>得到与源程序等价的目标程序</strong><ul><li>中间代码生成：一种易于产生、翻译的抽象机器程序（三地址码等）</li><li>代码优化</li><li>目标代码生成：把中间代码变换成依赖具体机器的目标代码</li></ul></li><li>符号表的管理</li><li>错误诊断和处理</li></ol></li><li>编译程序的伙伴工具&amp;遍<ul><li>预处理器：第一遍找出所有标识符。</li><li>汇编程序：第二遍将操作码翻译为机器代码</li><li>连接装配程序</li></ul></li></ol><h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><h3 id="词法分析器的作用"><a href="#词法分析器的作用" class="headerlink" title="词法分析器的作用"></a>词法分析器的作用</h3><blockquote><p>词法分析期的作用：扫描源程序的字符流识别出单词符号</p></blockquote><h3 id="记号、模式"><a href="#记号、模式" class="headerlink" title="记号、模式"></a>记号、模式</h3><blockquote><p>记号：某一类单词符号的编码。（标识符为id,数字为num）<br>模式：某一类单词符号的构词规则。</p></blockquote><h3 id="词法分析器的状态转换图"><a href="#词法分析器的状态转换图" class="headerlink" title="词法分析器的状态转换图"></a>词法分析器的状态转换图</h3><h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><h3 id="语法分析程序-输入记号序列输出语法分析树"><a href="#语法分析程序-输入记号序列输出语法分析树" class="headerlink" title="语法分析程序:输入记号序列输出语法分析树"></a>语法分析程序:输入记号序列输出语法分析树</h3><h3 id="chomsky文法："><a href="#chomsky文法：" class="headerlink" title="chomsky文法："></a>chomsky文法：</h3><ul><li>0型文法：无限制文法（图灵机识别）</li><li>1型文法：上下文有关文法（线性界限自动机识别）</li><li>2型文法：上下文无关文法（下推自动机接受）</li><li>3型文法：正规文法（有限状态自动机）</li></ul><h3 id="推导与归约"><a href="#推导与归约" class="headerlink" title="推导与归约"></a>推导与归约</h3><p>最右推导为规范推导，最左规约为规范规约</p><h3 id="A-自顶向下分析方法-从树根到叶子来建立分析树"><a href="#A-自顶向下分析方法-从树根到叶子来建立分析树" class="headerlink" title="A.自顶向下分析方法:从树根到叶子来建立分析树"></a>A.自顶向下分析方法:从树根到叶子来建立分析树</h3><ol><li><p>要进行确定分析，则文法必须无左递归和回溯</p><ul><li>消除左递归：避免死循环</li><li>消除回溯：提取公共左因子，判断是否为LL(1)文法；提高分析效率</li></ul></li><li><p>LL(k)文法：</p><ul><li>最左推导</li><li>从左到右扫描源程序</li><li>每次向前查看k个字符。</li></ul></li><li><p>预测分析法：预测分析表的构造，反序入栈；</p></li></ol><h3 id="B-自底向上分析方法：从树叶到树根来分析树"><a href="#B-自底向上分析方法：从树叶到树根来分析树" class="headerlink" title="B.自底向上分析方法：从树叶到树根来分析树"></a>B.自底向上分析方法：从树叶到树根来分析树</h3><ol><li>可规约串在<strong>规范归约分析法</strong>中是句柄，在<strong>算符优先分析法</strong>中是最左素短语；</li><li><p>算符优先分析法</p><ul><li>First集和Follow集</li><li>短语：不同层的语法树中叶子节点组成的符号串</li><li>直接（素）短语：不同层的语法树中节点不在包含其他节点有子树</li><li>句柄：最左直接短语</li><li>句型：叶子节点大集合</li></ul></li><li><p>LR（k）分析技术:</p><ul><li>上下文无关文法：G(V,sigma,R,S)[非终结符，终结符，开始变量，规则/产生式]</li><li>L表示自左至右扫描输入符号串</li><li>R表示最右推导的逆过程</li><li>k表示输入符号的个数</li></ul></li><li>LR(0)分析<ul><li>指明对活前缀的识别状态，分为归约、移进、待约和接受项目</li><li>没有移进规约冲突和归约归约冲突则是LR（0）文法</li></ul></li><li>SLR（1）分析法<ul><li>在LR(0)分析法的基础上向前查看一个输入符号，避免无脑归约</li></ul></li><li>LR(1)分析法<br> 归约仅在输入符号是搜索符时进行</li></ol><h2 id="语法制导翻译技术"><a href="#语法制导翻译技术" class="headerlink" title="语法制导翻译技术"></a>语法制导翻译技术</h2><blockquote><p>输入符号串-&gt;分析树-&gt;依赖图-&gt;语义规则的计算顺序-&gt;计算结果</p></blockquote><ol><li>语义分析的任务：静态语义审查，执行真正的翻译（生成中间代码和目标代码）</li><li>常见中间代码：逆波兰式、三元式、四元式</li></ol><h3 id="语法制导定义"><a href="#语法制导定义" class="headerlink" title="语法制导定义"></a>语法制导定义</h3><ul><li>S-属性定义</li><li>L-属性定义（继承属性应满足的<strong>限制条件</strong>）</li></ul><h3 id="翻译方案"><a href="#翻译方案" class="headerlink" title="翻译方案"></a>翻译方案</h3><ul><li>构造S-属性定义的翻译方案（语义动作放在产生式右尾）</li><li>构造L-属性定义的翻译方案（语义动作插入产生式之中）</li></ul><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><ol><li>语义分析的概念</li></ol><ul><li>编译的一个重要任务、检查语义的合法性</li><li>符号表的建立和管理</li><li>语义检查</li></ul><ol><li>符号表</li></ol><ul><li>操作：检索、插入、定位、重定位</li></ul><hr><h1 id="第三门：数据库原理"><a href="#第三门：数据库原理" class="headerlink" title="第三门：数据库原理"></a>第三门：数据库原理</h1><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><h3 id="四个基本概念"><a href="#四个基本概念" class="headerlink" title="四个基本概念"></a>四个基本概念</h3><ul><li>数据：数据库中存储的基本对象</li><li>数据库：<strong>长期存储</strong>、<strong>有组织</strong>、<strong>可共享</strong>的大量数据集合</li><li>数据库管理系统：用户与操作系统之间的数据管理软件</li><li>数据库系统：包含数据库管理系统和数据库。</li></ul><h3 id="数据模型-数据结构-数据操作-完整性约束条件"><a href="#数据模型-数据结构-数据操作-完整性约束条件" class="headerlink" title="数据模型 = 数据结构 + 数据操作 + 完整性约束条件"></a>数据模型 = 数据结构 + 数据操作 + 完整性约束条件</h3><ul><li>概念模型</li><li>逻辑模型和物理模型</li></ul><h3 id="常用数据模型"><a href="#常用数据模型" class="headerlink" title="常用数据模型"></a>常用数据模型</h3><ul><li>格式化模型<ul><li>层次模型（树）</li><li>网状模型（图）</li></ul></li><li>关系模型（表）</li><li>面向对象模型</li><li>对象关系模型</li></ul><h3 id="数据库系统结构"><a href="#数据库系统结构" class="headerlink" title="数据库系统结构"></a>数据库系统结构</h3><ul><li>单用户DBS</li><li>主从式DBS</li><li>C/S结构DBS</li><li>分布式DBS</li></ul><h3 id="数据库系统的三级模式结构：（1-1-n）"><a href="#数据库系统的三级模式结构：（1-1-n）" class="headerlink" title="数据库系统的三级模式结构：（1:1:n）"></a>数据库系统的三级模式结构：（1:1:n）</h3><ul><li>模式：数据库中全体数据的<strong>逻辑</strong>结构和特征的描述(一个数据库只有一个模式)</li><li>外模式：数据库<strong>用户</strong>使用的局部数据的模式（一个数据库只有一个外模式）</li><li>内模式：数据<strong>物理</strong>结构和存储方式的描述</li></ul><h2 id="关系数据库"><a href="#关系数据库" class="headerlink" title="关系数据库"></a>关系数据库</h2><h3 id="关系数据结构及形式化定义"><a href="#关系数据结构及形式化定义" class="headerlink" title="关系数据结构及形式化定义"></a>关系数据结构及形式化定义</h3><ul><li>域：相同数据类型的值的集合</li><li>笛卡尔积：多个域中的交集。</li><li>关系：二维表</li><li>属性：二维表的一列</li><li>码：能唯一标识一个元组的属性组<ul><li>候选码：能唯一标识一个元组的属性组，且不含多余属性</li><li>主码：多个候选码之一</li><li>主属性：主码的属性</li><li>外码：不是表一主码，是表二的主码</li></ul></li></ul><h2 id="关系数据理论"><a href="#关系数据理论" class="headerlink" title="关系数据理论"></a>关系数据理论</h2><h3 id="关系模式的形式化定义"><a href="#关系模式的形式化定义" class="headerlink" title="关系模式的形式化定义"></a>关系模式的形式化定义</h3><blockquote><p>R(U, D, DOM, F) // R:关系名 U:属性名集合 D:属性来自的域 DOM:属性向域的映象集合 F:属性间数据的依赖关系集合</p></blockquote><h3 id="函数依赖"><a href="#函数依赖" class="headerlink" title="函数依赖"></a>函数依赖</h3><ul><li>函数依赖：x可以唯一确定y</li><li>完全函数依赖：x1或者x2不可以唯一确定y,但是（x1,x2）可以完全确定y</li><li>部分函数依赖：x1,x2其中之一可以唯一确定y；</li></ul><h3 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h3><ul><li>1NF:关系模式的所有属性都是不可分的基本数据项</li><li>2NF:每一个非主属性完全函数依赖于码（消去了非主属性对主码的部分依赖）</li><li>3NF:每一个非主属性既不部分依赖于码也不传递依赖于码（消去了非主属性对主码的传递依赖）</li><li>4NF:消除了任何属性对码的传递依赖与部分依赖</li></ul><h3 id="关系操作"><a href="#关系操作" class="headerlink" title="关系操作"></a>关系操作</h3><ul><li>查询：选择、投影、链接、除、并、交、差、笛卡尔积</li><li>更新：插入、删除、修改</li></ul><h3 id="关系代数"><a href="#关系代数" class="headerlink" title="关系代数"></a>关系代数</h3><ul><li>并：直观取并集</li><li>差：直观取前者的补集</li><li>交：直观取交集</li><li>笛卡尔积：两者各元组的并集</li><li>选择：<code>where</code></li><li>投影：从关系中去若干子关系组成新的关系（选多列）</li><li>连接：从两个关系的笛卡尔积中选择部分元组</li><li>除：获得满足关系组的属性组</li></ul><h3 id="关系的完整性"><a href="#关系的完整性" class="headerlink" title="关系的完整性"></a>关系的完整性</h3><ul><li>实体完整性：属性A是基本关系R的主属性，则属性A不能取空值</li><li>参照完整性：<ul><li>关系间的引用</li><li>外码：设F是基本关系R的一个或一组属性，但不是关系R的码。如果F与基本关系S的主码Ks相对应，则称F是基本关系R的外码</li><li>参照完整性规则</li></ul></li><li>用户定义的完整性：<code>NOT NULL</code>/<code>UNIQUE</code>/<code>CHECK</code></li></ul><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查询指定列</span></span><br><span class="line">SELECT Sno,Sname</span><br><span class="line">FROM Student</span><br><span class="line">WHERE Sdept = <span class="string">&#x27;CS&#x27;</span></span><br><span class="line">ORDER BY Grade DESC;</span><br><span class="line"><span class="comment">#查询全部列</span></span><br><span class="line">SELECT *</span><br><span class="line">FROM Student</span><br><span class="line">WHERE Sage &lt; <span class="number">20</span></span><br><span class="line">ORDER BY Sdept ASC,Sage DESC;</span><br><span class="line"><span class="comment">#消除取值重复的行</span></span><br><span class="line">SELECT DISTINCT Sno</span><br><span class="line">FROM SC</span><br><span class="line">WHERE Grade &lt; <span class="number">60</span>;</span><br></pre></td></tr></table></figure><p>　聚合函数：</p><ul><li>where:过滤表中数据的条件；</li><li>group by:如何将上面过滤出的数据分组；- having:对上面已经分组的数据进行过滤的条件 ；</li><li>order by :按照什么样的顺序来查看返回的数据 </li></ul><h2 id="数据库保护"><a href="#数据库保护" class="headerlink" title="数据库保护"></a>数据库保护</h2><h3 id="授权与回收"><a href="#授权与回收" class="headerlink" title="授权与回收"></a>授权与回收</h3><ul><li>授权</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把查询Student表权限/全部权限给用户U1/所有用户</span></span><br><span class="line">GRANT SELECT/ALL PRIBILIGES</span><br><span class="line">ON TABLE Student</span><br><span class="line">TO U1/PUBLIC;</span><br></pre></td></tr></table></figure><ul><li>回收</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把用户U4修改学生学号的权限收回</span></span><br><span class="line">REVOKE UPDATE(Sno)</span><br><span class="line">ON TABLE Student</span><br><span class="line">FROM U4;</span><br></pre></td></tr></table></figure><ul><li>触发器：事件驱动</li></ul><h2 id="数据库恢复技术"><a href="#数据库恢复技术" class="headerlink" title="数据库恢复技术"></a>数据库恢复技术</h2><ul><li><strong>事务</strong><ul><li>原子性：事务中的所有操作要么全部执行要么不执行</li><li>一致性：执行事务前后数据库是一致的</li><li>隔离性：每个事务都感觉不到系统中有其他事务在执行</li><li>持续性：事务成功执行后对数据库的修改是永久的</li></ul></li><li>数据的锁定：<ul><li>导致数据不一致性包括：<strong>并发操作破坏了事务的隔离性</strong><ul><li>丢失修改</li><li>不可重复读</li><li>读“脏”数据</li></ul></li><li>封锁<ul><li>排它锁（X锁）：其他事务不能读取和修改A</li><li>共享锁（S锁）：其他事务只能读取A不能修改A</li></ul></li></ul></li></ul><hr><h1 id="第四门：计算机系统结构"><a href="#第四门：计算机系统结构" class="headerlink" title="第四门：计算机系统结构"></a>第四门：计算机系统结构</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul><li>虚拟机：抽象的计算机，由软件实现，都具有指令集并使用不同的存储区域。</li><li>透明性：本来存在的事物或属性，从某种角度看似乎不存在</li><li>系统结构发展的影响因素：<ul><li>程序访存的局部性：</li><li>数据访问的局部性：时间局部性&amp;空间局部性</li></ul></li><li>系统结构设计中最常用的方法：大概率事件优先原理</li><li>可移植性：<ul><li>采用系列机</li><li>模拟与仿真</li><li>统一高级语言</li></ul></li><li><p>提高并行性的技术途径：</p><ul><li>时间重叠</li><li>资源重复</li><li>资源共享</li></ul></li><li><p>流水线的性能指标</p><ul><li>吞吐率： $$P = \frac{n}{T_k}=\frac{n}{(k+n-1)\Delta T}$$</li><li>加速比： $$ S = \frac{T_0}{T_k} = \frac{k*n}{k+n-1}$$</li><li>效率：$$E = \frac{任务有效面积}{对应总面积}= \frac{n}{k+n-1} $$</li></ul></li></ul><h2 id="流水线技术"><a href="#流水线技术" class="headerlink" title="流水线技术"></a>流水线技术</h2><ul><li>静态流水线（同一时间，同一种功能）&amp;动态流水线（同一时间，不同的方式）</li><li>线性流水线（串行连接，没有反馈）&amp;非线性流水线（多加了反馈回路）</li></ul><h2 id="向量处理机器-互联网络-阵列机"><a href="#向量处理机器-互联网络-阵列机" class="headerlink" title="向量处理机器+互联网络+阵列机"></a>向量处理机器+互联网络+阵列机</h2><ol><li>向量处理机：在流水线处理机中，设置向量数据表示和相应的向量指令，成为向量处理机<ul><li>向量处理机的结构：<ul><li>存储器-存储器型：向量长度不受限</li><li>寄存器-寄存器型：</li></ul></li></ul></li><li>指令级别并行：<ul><li>Tomasulo算法：</li></ul></li></ol><h2 id="多处理机-后面的看不完了，请直接去刷13套题"><a href="#多处理机-后面的看不完了，请直接去刷13套题" class="headerlink" title="多处理机:(后面的看不完了，请直接去刷13套题)"></a>多处理机:(后面的看不完了，请直接去刷13套题)</h2>]]></content>
      
      
      <categories>
          
          <category> BUPT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reexamine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tornado学习小结</title>
      <link href="/2018/02/14/tornado-usage/"/>
      <url>/2018/02/14/tornado-usage/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>它是非阻塞式服务器，而且性能十分优越，Tornado 每秒可以处理数以千计的连接，因此 Tornado 是实时 Web 服务的一个理想框架，专门解决C10K问题。</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><h3 id="主要模块"><a href="#主要模块" class="headerlink" title="主要模块"></a>主要模块</h3><ul><li>tornado.web：包含了大多数重要的功能</li><li>tornado.escape: XHTML、JSON、URL的编码解码方法</li><li>database: 对Mysql的简单封装</li><li>template:web模板系统</li><li>httpclient:非阻塞式 HTTP 客户端，它被设计用来和 web 及 httpserver 协同工作</li><li>auth：第三方认证的实现（包括 Google OpenID/OAuth、Facebook Platform、Yahoo BBAuth、FriendFeed OpenID/OAuth、Twitter OAuth）</li><li>local:针对本地化和翻译的支持</li><li>options:命令行和配置文件解析工具，针对服务器环境做了优化</li></ul><h3 id="底层模块："><a href="#底层模块：" class="headerlink" title="底层模块："></a>底层模块：</h3><ul><li>httpserver:服务于 web 模块的一个非常简单的 HTTP 服务器的实现</li><li>iostream:对非阻塞式的 socket的简单封装，以方便常用读写操作</li><li>ioloop：核心的I/O循环</li></ul><h2 id="代码中学习Tornado的基本功能"><a href="#代码中学习Tornado的基本功能" class="headerlink" title="代码中学习Tornado的基本功能"></a>代码中学习Tornado的基本功能</h2><h3 id="helloTornado展示Tornado最频繁使用的功能"><a href="#helloTornado展示Tornado最频繁使用的功能" class="headerlink" title="helloTornado展示Tornado最频繁使用的功能"></a>helloTornado展示Tornado最频繁使用的功能</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tornado.httpserver</span><br><span class="line"><span class="keyword">import</span> tornado.ioloop</span><br><span class="line"><span class="keyword">import</span> tornado.options</span><br><span class="line"><span class="keyword">import</span> tornado.web</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tornado.options <span class="keyword">import</span> define, options</span><br><span class="line">define(<span class="string">&quot;port&quot;</span>, default=<span class="number">8000</span>, <span class="built_in">help</span>=<span class="string">&quot;run on the given port&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IndexHandler</span>(tornado.web.RequestHandler):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self</span>):</span><br><span class="line">        greeting = self.get_argument(<span class="string">&#x27;greeting&#x27;</span>, <span class="string">&#x27;Hello&#x27;</span>)</span><br><span class="line">        self.write(greeting + <span class="string">&#x27;, friendly user!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    tornado.options.parse_command_line()</span><br><span class="line">    app = tornado.web.Application(handlers=[(<span class="string">r&quot;/&quot;</span>, IndexHandler)])</span><br><span class="line">    http_server = tornado.httpserver.HTTPServer(app)</span><br><span class="line">    http_server.listen(options.port)</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br><span class="line"> </span><br><span class="line">$ curl http://localhost:<span class="number">8000</span>/</span><br><span class="line">Hello, friendly user!</span><br><span class="line">$ curl http://localhost:<span class="number">8000</span>/?greeting=Salutations</span><br><span class="line">Salutations, friendly user!</span><br></pre></td></tr></table></figure><h3 id="除了get方法，来尝试一下post方法！"><a href="#除了get方法，来尝试一下post方法！" class="headerlink" title="除了get方法，来尝试一下post方法！"></a>除了get方法，来尝试一下post方法！</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tornado.httpserver</span><br><span class="line"><span class="keyword">import</span> tornado.ioloop</span><br><span class="line"><span class="keyword">import</span> tornado.options</span><br><span class="line"><span class="keyword">import</span> tornado.web</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tornado.options <span class="keyword">import</span> define, options</span><br><span class="line">define(<span class="string">&quot;port&quot;</span>, default=<span class="number">8000</span>, <span class="built_in">help</span>=<span class="string">&quot;run on the given port&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReverseHandler</span>(tornado.web.RequestHandler):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        self.write(<span class="built_in">input</span>[::-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrapHandler</span>(tornado.web.RequestHandler):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post</span>(<span class="params">self</span>):</span><br><span class="line">        text = self.get_argument(<span class="string">&#x27;text&#x27;</span>)</span><br><span class="line">        width = self.get_argument(<span class="string">&#x27;width&#x27;</span>, <span class="number">40</span>)</span><br><span class="line">        self.write(textwrap.fill(text, <span class="built_in">int</span>(width)))</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    tornado.options.parse_command_line()</span><br><span class="line">    app = tornado.web.Application(</span><br><span class="line">        handlers=[</span><br><span class="line">            (<span class="string">r&quot;/reverse/(\w+)&quot;</span>, ReverseHandler),</span><br><span class="line">            (<span class="string">r&quot;/wrap&quot;</span>, WrapHandler)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    http_server = tornado.httpserver.HTTPServer(app)</span><br><span class="line">    http_server.listen(options.port)</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br><span class="line"></span><br><span class="line">$ http://localhost:<span class="number">8000</span>/reverse/slipup</span><br><span class="line">pupils</span><br><span class="line">$ http://localhost:<span class="number">8000</span>/wrap -d text=Lorem+ipsum+dolor+sit+amet,+consectetuer+adipiscing+elit. </span><br><span class="line">Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</span><br></pre></td></tr></table></figure><h4 id="修改用户名代码示例："><a href="#修改用户名代码示例：" class="headerlink" title="修改用户名代码示例："></a>修改用户名代码示例：</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matched with (r&quot;/widget/(\d+)&quot;, WidgetHandler)</span></span><br><span class="line"><span class="comment"># widget：小机械，小部件</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WidgetHandler</span>(tornado.web.RequestHandler):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, widget_id</span>):</span><br><span class="line">        widget = retrieve_from_db(widget_id)</span><br><span class="line">        self.write(widget.serialize())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post</span>(<span class="params">self, widget_id</span>):</span><br><span class="line">        widget = retrieve_from_db(widget_id)</span><br><span class="line">        widget[<span class="string">&#x27;foo&#x27;</span>] = self.get_argument(<span class="string">&#x27;foo&#x27;</span>)</span><br><span class="line">        save_to_db(widget)</span><br></pre></td></tr></table></figure><h4 id="HTTP的状态码"><a href="#HTTP的状态码" class="headerlink" title="HTTP的状态码"></a>HTTP的状态码</h4><ul><li>404 NOT Found:Tornado会在HTTP请求的路径无法匹配任何RequestHandler类相对应的模式时返回404（Not Found）响应码。</li><li>400 Bad Request:如果你调用了一个没有默认值的get_argument函数，并且没有发现给定名称的参数，Tornado将自动返回一个400（Bad Request）响应码。</li><li>405 Method Not Allowed:如果传入的请求使用了RequestHandler中没有定义的HTTP方法（比如，一个POST请求，但是处理函数中只有定义了get方法），Tornado将返回一个405（Methos Not Allowed）响应码。</li><li>500 Internal Server Error:当程序遇到任何不能让其退出的错误时，Tornado将返回500（Internal Server Error）响应码。你代码中任何没有捕获的异常也会导致500响应码。</li><li>200 OK:如果响应成功，并且没有其他返回码被设置，Tornado将默认返回一个200（OK）响应码。</li></ul>]]></content>
      
      
      <categories>
          
          <category> App </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tornado </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北邮机试打印资料</title>
      <link href="/2018/02/13/BUPT-OJ-experience/"/>
      <url>/2018/02/13/BUPT-OJ-experience/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote><p>北邮复试的机试环节是可以带资料的。我整理了当时机试我准备的资料给需要的同学做参考。以下代码均可在机试环境中运行。</p></blockquote><h2 id="机试导入头文件"><a href="#机试导入头文件" class="headerlink" title="机试导入头文件"></a>机试导入头文件</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line">```    </span><br><span class="line">## 结构体</span><br><span class="line">``` c++</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">student</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> score;</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">&#125;a[<span class="number">101</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">comp</span><span class="params">(<span class="type">const</span> student &amp;a,<span class="type">const</span> student &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.score&gt;b.score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n,k;<span class="comment">//n个人，求第k名的成绩</span></span><br><span class="line">    cin&gt;&gt;n&gt;&gt;k;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;++i)      </span><br><span class="line">        cin&gt;&gt;a[i].id&gt;&gt;a[i].score;</span><br><span class="line">    <span class="built_in">sort</span>(a+<span class="number">1</span>,a+n+<span class="number">1</span>,comp);</span><br><span class="line">    cout&lt;&lt;a[k].id&lt;&lt;<span class="string">&#x27; &#x27;</span>&lt;&lt;a[k].score&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="scanf-amp-cin"><a href="#scanf-amp-cin" class="headerlink" title="scanf &amp; cin"></a>scanf &amp; cin</h2><ul><li>浮点数判定相等<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">fabs</span>(a-b)&lt;<span class="number">0.000001</span></span><br></pre></td></tr></table></figure></li><li><p>带逗号分隔的输入形式：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">scanf</span>(<span class="string">&quot;%d, %d&quot;</span>,&amp;a, &amp;b);</span><br></pre></td></tr></table></figure></li><li><p><code>getchar()；</code>用来吃进空格，有奇用！ </p></li></ul><h2 id="补充：二维数组传参：尽量少用指针！"><a href="#补充：二维数组传参：尽量少用指针！" class="headerlink" title="补充：二维数组传参：尽量少用指针！"></a>补充：二维数组传参：尽量少用指针！</h2><ul><li>合法形式：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Foo1</span><span class="params">(<span class="type">int</span> array[<span class="number">30</span>][<span class="number">30</span>])</span>；</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Foo2</span><span class="params">(<span class="type">int</span> array[][<span class="number">30</span>])</span></span>;</span><br></pre></td></tr></table></figure></li><li>不能省略高维的大小！不合法形式：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Foo3</span><span class="params">(<span class="type">int</span> array[][])</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Foo4</span><span class="params">(<span class="type">int</span> **array)</span></span>;<span class="comment">//这种方式也是错误的</span></span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="树与图"><a href="#树与图" class="headerlink" title="树与图"></a>树与图</h2><h3 id="二叉链表建树套路"><a href="#二叉链表建树套路" class="headerlink" title="二叉链表建树套路"></a>二叉链表建树套路</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据二叉排序树</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span>&#123;</span><br><span class="line">    <span class="type">char</span> letter;</span><br><span class="line">    node *lchild,*rchild;<span class="comment">//</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">node *<span class="title">create</span><span class="params">(<span class="type">char</span> c)</span></span>&#123;</span><br><span class="line">    node *p = (node *)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(node));</span><br><span class="line">    (*p).letter = c;</span><br><span class="line">    (*p).lchild = (*p).rchild = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">node *<span class="title">insert</span><span class="params">(node *t,<span class="type">char</span> x)</span></span>&#123;<span class="comment">//递归建树</span></span><br><span class="line">    <span class="keyword">if</span>(t==<span class="literal">NULL</span>)&#123;<span class="comment">//若树为空</span></span><br><span class="line">        t = <span class="built_in">create</span>(x);</span><br><span class="line">        <span class="keyword">return</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(x-<span class="string">&#x27;0&#x27;</span> &lt; t-&gt;letter-<span class="string">&#x27;0&#x27;</span>)&#123;<span class="comment">//插入左子树</span></span><br><span class="line">        t-&gt;lchild = <span class="built_in">insert</span>(t-&gt;lchild, x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(x-<span class="string">&#x27;0&#x27;</span> &gt; t-&gt;letter-<span class="string">&#x27;0&#x27;</span>)&#123;</span><br><span class="line">        t-&gt;rchild = <span class="built_in">insert</span>(t-&gt;rchild, x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> t;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据中序后序建树</span></span><br><span class="line"><span class="function">node *<span class="title">rebuild</span><span class="params">(<span class="type">char</span>* post,<span class="type">char</span>* in,<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">char</span> ch = post[n<span class="number">-1</span>];</span><br><span class="line">    node *p = <span class="built_in">create</span>(ch);</span><br><span class="line">    <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; n &amp;&amp; in[i] != ch) i++;</span><br><span class="line">    <span class="type">int</span> l_len = i;</span><br><span class="line">    <span class="type">int</span> r_len = n-i<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span>(l_len&gt;<span class="number">0</span>) (*p).lchild = <span class="built_in">rebuild</span>(post,in,l_len);</span><br><span class="line">    <span class="keyword">if</span>(r_len&gt;<span class="number">0</span>) (*p).rchild = <span class="built_in">rebuild</span>(post+l_len,in+l_len+<span class="number">1</span>,r_len);</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//前序、后序遍历</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">preorder</span><span class="params">(node *t)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ptr) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\t%d&quot;</span>, ptr-&gt;data);</span><br><span class="line">        <span class="built_in">preorder</span>(ptr-&gt;left_child);</span><br><span class="line">        <span class="built_in">preorder</span>(ptr-&gt;right_child);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//后序遍历</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">postorder</span><span class="params">(node *t)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ptr) &#123;</span><br><span class="line">        <span class="built_in">postorder</span>(ptr-&gt;left_child);</span><br><span class="line">        <span class="built_in">postorder</span>(ptr-&gt;right_child);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\t%d&quot;</span>, ptr-&gt;data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">char</span> tree[<span class="number">30</span>];</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,tree)!=EOF)&#123;</span><br><span class="line">        node *t = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; <span class="built_in">strlen</span>(tree);i++)&#123;</span><br><span class="line">            t = <span class="built_in">insert</span>(t,in[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">preorder</span>(t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图套路之建图"><a href="#图套路之建图" class="headerlink" title="图套路之建图"></a>图套路之建图</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Edge</span>&#123;</span><br><span class="line">    <span class="type">int</span> nextNode;<span class="comment">//下一个节点编号</span></span><br><span class="line">    <span class="type">int</span> cost;<span class="comment">//该边得权重</span></span><br><span class="line">&#125;;</span><br><span class="line">vector&lt;Edge&gt; edge[N];<span class="comment">//vector模拟单链表</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; N;i++)&#123;</span><br><span class="line">    edge[i].<span class="built_in">clear</span>();<span class="comment">//清空单链表</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//向单链表添加信息</span></span><br><span class="line">Edge tmp;</span><br><span class="line">tmp.nextNode = <span class="number">3</span>;</span><br><span class="line">tmp.cost = <span class="number">38</span>;</span><br><span class="line">edge[<span class="number">1</span>].<span class="built_in">push_back</span>(tmp)</span><br><span class="line"><span class="comment">//查询某个节点的所有邻接信息，对vector进行遍历</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; edge[<span class="number">2</span>].<span class="built_in">size</span>();i++)&#123;<span class="comment">//对edge[2]进行遍历</span></span><br><span class="line">    <span class="type">int</span> nextNode = edge[<span class="number">2</span>][i].nextNode;</span><br><span class="line">    <span class="type">int</span> cost = edge[<span class="number">2</span>][i].cost;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除边</span></span><br><span class="line">edge[<span class="number">1</span>].<span class="built_in">erase</span>(edge[<span class="number">1</span>].<span class="built_in">begin</span>()+i,edge.<span class="built_in">begin</span>()+i+<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="floyd算法"><a href="#floyd算法" class="headerlink" title="floyd算法"></a>floyd算法</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">1</span>;k &lt;=  N;k++)&#123;<span class="comment">//每次我们选取的中间节点,Floyd算法的核心。</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= N;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>;j &lt;= N;j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(a[i][k] == N || a[k][j] == N) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span>(a[i][k]+a[k][j] &lt; a[i][j])   a[i][j] = a[i][k] + a[k][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="dijstra算法"><a href="#dijstra算法" class="headerlink" title="dijstra算法"></a>dijstra算法</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">E</span>&#123;</span><br><span class="line">    <span class="type">int</span> next;<span class="comment">// 临街的节点</span></span><br><span class="line">    <span class="type">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">vector&lt;E&gt; edge[<span class="number">1005</span>];<span class="comment">//邻接链表</span></span><br><span class="line"><span class="type">int</span> Dis[<span class="number">1005</span>];</span><br><span class="line"><span class="type">bool</span> mark[<span class="number">1005</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n,m,a,b,c;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n&gt;&gt;m)&#123;<span class="comment">//n个节点，m条边</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)&#123;</span><br><span class="line">            edge[i].<span class="built_in">clear</span>();<span class="comment">//邻接链表的初始化</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)&#123;</span><br><span class="line">            Dis[i] = <span class="number">-1</span>;</span><br><span class="line">            mark[i] = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">            cin&gt;&gt;a&gt;&gt;b&gt;&gt;c;<span class="comment">//a到b有一条权值c的路径</span></span><br><span class="line">            E tmp;</span><br><span class="line">            tmp.c = c;</span><br><span class="line">            tmp.next = b;</span><br><span class="line">            edge[a].<span class="built_in">push_back</span>(tmp);<span class="comment">//把b加入a的邻接链表, 针对无向图</span></span><br><span class="line">            tmp.next = a;</span><br><span class="line">            edge[b].<span class="built_in">push_back</span>(tmp);<span class="comment">//把a加入b的邻接链表</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> newP = <span class="number">1</span>;<span class="comment">//起点</span></span><br><span class="line">        Dis[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">        mark[<span class="number">1</span>] = <span class="literal">true</span>;<span class="comment">//求节点1到其他节点的最短路径 访问到了mark就要变为true.</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt; n;i++)&#123;<span class="comment">//要循环的次数 n-1次</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>;j &lt; edge[newP].<span class="built_in">size</span>();j++)&#123;<span class="comment">//更新距离数组</span></span><br><span class="line">                <span class="type">int</span> t = edge[newP][j].next;<span class="comment">//保存邻接节点</span></span><br><span class="line">                <span class="type">int</span> cost = edge[newP][j].c;</span><br><span class="line">                <span class="keyword">if</span>(mark[t]==<span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(Dis[t] == <span class="number">-1</span> || Dis[t] &gt; Dis[newP] + cost)&#123;</span><br><span class="line">                    Dis[t] = Dis[newP]+cost;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> min = <span class="number">1000000</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>;j &lt;= n;j++)&#123;<span class="comment">//找到Dis数组中的最小值</span></span><br><span class="line">                <span class="keyword">if</span>(mark[j]==<span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(Dis[j]==<span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(Dis[j]&lt;min)&#123;</span><br><span class="line">                    min = Dis[j];</span><br><span class="line">                    newP = j;<span class="comment">//起点的更新 每次选完最小节点后 起点都要进行更新。</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            mark[newP] = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)&#123;</span><br><span class="line">            cout&lt;&lt;Dis[i]&lt;&lt;<span class="string">&quot; &quot;</span>&lt;&lt;endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; edge[<span class="number">501</span>];</span><br><span class="line">queue&lt;<span class="type">int</span>&gt; Q;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> degree[<span class="number">501</span>];<span class="comment">//存储入度，拓扑排序判断的就是入度</span></span><br><span class="line">    <span class="type">int</span> n,m;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n&gt;&gt;m)&#123;<span class="comment">//n是节点个数，m是边的个数</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; <span class="number">501</span>;i++)&#123;<span class="comment">//</span></span><br><span class="line">            degree[i] = <span class="number">0</span>;</span><br><span class="line">            edge[i].<span class="built_in">clear</span>();<span class="comment">//链表初始化</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(Q.<span class="built_in">empty</span>()==<span class="literal">false</span>)&#123;<span class="comment">//初始化，弹出队列中的元素知道Q为空。</span></span><br><span class="line">            Q.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;<span class="comment">//输入数据</span></span><br><span class="line">            <span class="type">int</span> a,b;</span><br><span class="line">            cin&gt;&gt;a&gt;&gt;b;<span class="comment">//输入a-&gt;b的一条边</span></span><br><span class="line">            degree[b]++;<span class="comment">//入度加一</span></span><br><span class="line">            edge[a].<span class="built_in">push_back</span>(b);<span class="comment">//链表存b</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)&#123;<span class="comment">//拓扑排序，找入度为0的节点。</span></span><br><span class="line">            <span class="keyword">if</span>(degree[i]==<span class="number">0</span>)&#123;</span><br><span class="line">                Q.<span class="built_in">push</span>(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(Q.<span class="built_in">empty</span>()==<span class="literal">false</span>)&#123;</span><br><span class="line">            <span class="type">int</span> now = Q.<span class="built_in">front</span>();<span class="comment">//访问队头元素</span></span><br><span class="line">            Q.<span class="built_in">pop</span>();</span><br><span class="line">            cout&lt;&lt;now&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>;k &lt; edge[now].<span class="built_in">size</span>();k++)&#123;</span><br><span class="line">                degree[edge[now][k]]--;<span class="comment">//类似二维数组的访问 因此now节点已经入队，所以入度要减去1</span></span><br><span class="line">                <span class="keyword">if</span>(degree[edge[now][k][==<span class="number">0</span>) Q.<span class="built_in">push</span>(edge[now][k]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="最长公共子序列与最长公共子串"><a href="#最长公共子序列与最长公共子串" class="headerlink" title="最长公共子序列与最长公共子串"></a>最长公共子序列与最长公共子串</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//最长公共子序列：输出最长序列长度</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">lcse</span><span class="params">(string str1,string str2,vector&lt;vector&lt;<span class="type">int</span>&gt; &gt;&amp; vec)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> len1 = str1.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> len2 = str2.<span class="built_in">size</span>();</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt; &gt; <span class="built_in">c</span>(len1 + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(len2 + <span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt;= len1;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>;j &lt;= len2;j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i==<span class="number">0</span> || j==<span class="number">0</span>)&#123;</span><br><span class="line">                c[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>])&#123;</span><br><span class="line">                c[i][j] = c[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">                vec[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(c[i<span class="number">-1</span>][j] &gt;= c[i][j<span class="number">-1</span>])&#123;</span><br><span class="line">                c[i][j] = c[i<span class="number">-1</span>][j];</span><br><span class="line">                vec[i][j] = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                c[i][j] = c[i][j<span class="number">-1</span>];</span><br><span class="line">                vec[i][j] = <span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> c[len1][len2];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//最长公共子串：输出最长子串长度</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">lcst</span><span class="params">(string str1,string str2,vector&lt;vector&lt;<span class="type">int</span> &gt; &gt;&amp; vec)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> len1 = str1.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> len2 = str2.<span class="built_in">size</span>();</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt; &gt; <span class="built_in">c</span>(len1+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(len2+<span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line">    <span class="type">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt;= len1;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>;j &lt;= len2;j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i==<span class="number">0</span> || j==<span class="number">0</span>)&#123;</span><br><span class="line">                c[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>])&#123;</span><br><span class="line">                c[i][j] = c[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">                vec[i][j] = <span class="number">0</span>;</span><br><span class="line">                result = <span class="built_in">max</span>(c[i][j],result);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                c[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//打印公共子序列/子串</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print_lcs</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt; &gt;&amp; vec,string str,<span class="type">int</span> i,<span class="type">int</span> j)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i== <span class="number">0</span> || j == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(vec[i][j] == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">print_lcs</span>(vec,str,i<span class="number">-1</span>,j<span class="number">-1</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>,str[i<span class="number">-1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(vec[i][j] == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">print_lcs</span>(vec,str,i<span class="number">-1</span>,j);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">print_lcs</span>(vec,str,i,j<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    string str1,str2;</span><br><span class="line">    cin&gt;&gt;str1&gt;&gt;str2;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt; &gt; <span class="built_in">vec</span>(str1.<span class="built_in">size</span>()+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(str2.<span class="built_in">size</span>() +<span class="number">1</span>, <span class="number">-1</span>));</span><br><span class="line">    <span class="type">int</span> ans = <span class="built_in">lcse</span>(str1, str2, vec);</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">print_lcs</span>(vec, str1, str1.<span class="built_in">size</span>(), str2.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="模板类和库函数"><a href="#模板类和库函数" class="headerlink" title="模板类和库函数"></a>模板类和库函数</h2><h3 id="模板类vector"><a href="#模板类vector" class="headerlink" title="模板类vector"></a>模板类vector</h3><ul><li>定义二维向量<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt; &gt; p(m, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n,<span class="number">0</span>)); <span class="comment">//m*n的二维向量，注意空格 所有元素初始化为0</span></span><br></pre></td></tr></table></figure></li><li>vector成员函数</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一维向量</span></span><br><span class="line"><span class="built_in">vector</span>&lt;type&gt; v;</span><br><span class="line"><span class="comment">// 末尾插入一个元素</span></span><br><span class="line">v.push_back(s);</span><br><span class="line"><span class="comment">// 任意位置插入元素</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;::iterator it = v.begin()</span><br><span class="line">v.insert(it+pos,elem);<span class="comment">//插入一个</span></span><br><span class="line">v.insert(it+pos,<span class="number">4</span>,elem);<span class="comment">//插入四个</span></span><br><span class="line">v.insert(it+pos,起始，终止);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除一个元素</span></span><br><span class="line">v.erase(positon)</span><br><span class="line">v.erase(v.end()<span class="number">-1</span>)/v.erase(v.back());<span class="comment">//删除最后一个元素</span></span><br><span class="line"><span class="comment">// 求和:将vector中所有元素加到0上</span></span><br><span class="line">accumulate(v.begin(),v.end(),<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 反转：</span></span><br><span class="line">reverse(v.begin(),v.end());</span><br><span class="line"><span class="comment">// 排序：</span></span><br><span class="line">sort(data.begin(),data.end(),::greater&lt;<span class="type">int</span>&gt;());</span><br><span class="line"><span class="comment">// iterator遍历</span></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;::iterator it = v.begin(); it != v.end(); ++it)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; *it &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 清空vector</span></span><br><span class="line">v.clear();</span><br><span class="line"><span class="comment">// 交换元素</span></span><br><span class="line">swap(sentence[<span class="number">0</span>],sentence[<span class="number">4</span>]); </span><br></pre></td></tr></table></figure><h3 id="模板类algrithom"><a href="#模板类algrithom" class="headerlink" title="模板类algrithom"></a>模板类algrithom</h3><ul><li><p>不修改内容的序列操作</p><ul><li><p>统计返回值个数</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">count</span>(v.<span class="built_in">begin</span>(),v.<span class="built_in">end</span>(),num)<span class="comment">//统计6的个数</span></span><br></pre></td></tr></table></figure></li><li><p>统计符合调剂返回值个数</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">BigThan5</span><span class="params">(<span class="type">int</span> &amp;n)</span></span>&#123; <span class="keyword">return</span> n &gt; <span class="number">5</span>; &#125;</span><br><span class="line"><span class="built_in">count</span>(v.<span class="built_in">begin</span>(),v.<span class="built_in">end</span>(),BigThan5)</span><br></pre></td></tr></table></figure></li><li><p>查找值/符合条件的值</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">        <span class="built_in">find</span>(v.<span class="built_in">begin</span>(),v.<span class="built_in">end</span>(),num)</span><br><span class="line">        <span class="built_in">find_if</span>(v.<span class="built_in">begin</span>(),v.<span class="built_in">end</span>(),foo)</span><br><span class="line">        ``` </span><br><span class="line"></span><br><span class="line">### 模板类stack&amp;queue&amp;map</span><br><span class="line"></span><br><span class="line">- stack&amp;queue</span><br><span class="line"></span><br><span class="line">    ``` c</span><br><span class="line">    # 定义</span><br><span class="line">    stack&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">    queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">    # 访问栈顶</span><br><span class="line">    s.<span class="built_in">top</span>()/s.<span class="built_in">front</span>()/s.<span class="built_in">back</span>()</span><br><span class="line">    # 判断栈空/返回有效个数</span><br><span class="line">    s.<span class="built_in">empty</span>()/s.<span class="built_in">size</span>()</span><br><span class="line">    # 插入、删除、交换</span><br><span class="line">    s.<span class="built_in">push</span>()/s.<span class="built_in">pop</span>()/s.<span class="built_in">swap</span>()</span><br></pre></td></tr></table></figure></li></ul></li><li><p>map</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#初始化</span><br><span class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="type">double</span>&gt; m</span><br><span class="line">#声明即插入</span><br><span class="line">m[<span class="string">&quot;Li&quot;</span>] = <span class="number">123.4</span>;</span><br><span class="line">#查找</span><br><span class="line">data_x = m.find(key);</span><br><span class="line">data_y = m[key];</span><br><span class="line"><span class="keyword">if</span>(m.find(key)==maplive.end())<span class="comment">//查找失败</span></span><br><span class="line">#遍历</span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="type">double</span>&gt;::iterator it = m.begin(); it != m.end(); ++it)</span><br><span class="line">    &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; it-&gt;first &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; it-&gt;second &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">#逆序遍历</span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">map</span>&lt;<span class="type">int</span>,<span class="type">int</span>&gt;::reverse_iterator it = bag.rbegin();it!=bag.rend();it++)</span><br><span class="line">#删除</span><br><span class="line">m.erase(key);</span><br></pre></td></tr></table></figure></li><li><p>multimap和multimap的排序</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#初始化</span><br><span class="line"><span class="built_in">multimap</span>&lt;<span class="type">int</span>,<span class="type">int</span>&gt; bag;</span><br><span class="line">#插入数据</span><br><span class="line">bag.insert(<span class="built_in">make_pair</span>(v,w));</span><br><span class="line">#排序</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">pair</span>&lt;<span class="type">int</span>,<span class="type">int</span>&gt; &gt; BAG(bag.begin(),bag.end())</span><br><span class="line"><span class="comment">//按键升序</span></span><br><span class="line">sort(BAG.begin(),BAG.end());</span><br><span class="line"><span class="comment">//按值升序</span></span><br><span class="line">sort(BAG.begin(),BAG.end(),cmp_by_value);</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">cmp_by_value</span><span class="params">(<span class="type">const</span> <span class="built_in">pair</span>&lt;<span class="type">int</span>,<span class="type">int</span>&gt; a,<span class="type">const</span> <span class="built_in">pair</span>&lt;<span class="type">int</span>,<span class="type">int</span>&gt; b)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a.first == b.first)&#123;</span><br><span class="line">        <span class="keyword">return</span> a.second&lt;b.second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a.first&gt;b.first;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//遍历</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; BAG.size();i++)&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;BAG[i].first&lt;&lt;<span class="string">&quot; &quot;</span>&lt;&lt;BAG[i].second&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="数学问题"><a href="#数学问题" class="headerlink" title="数学问题"></a>数学问题</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//浮点数判定相等</span></span><br><span class="line"><span class="built_in">fabs</span>(a-b)&lt;<span class="number">0.000001</span></span><br><span class="line"><span class="comment">//将M进制的大数X转换为N进制</span></span><br><span class="line"><span class="type">int</span> M,N;</span><br><span class="line"><span class="built_in">string</span> X;</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;M&gt;&gt;N)&#123;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;X;</span><br><span class="line">    <span class="type">int</span> data[<span class="number">1010</span>];<span class="comment">//保存M进制下的各个位数</span></span><br><span class="line">    <span class="type">int</span> output[<span class="number">1010</span>];<span class="comment">//保存N进制下的各个位数</span></span><br><span class="line">    <span class="built_in">memset</span>(output,<span class="number">0</span>,<span class="keyword">sizeof</span>(output));</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; X.length();i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">isalpha</span>(X[i]))&#123; data[i] = X[i]-<span class="string">&#x27;A&#x27;</span>+<span class="number">10</span>;&#125;</span><br><span class="line">        <span class="keyword">else</span>&#123; data[i] = X[i] - <span class="string">&#x27;0&#x27;</span>;&#125;</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">1</span>,d = <span class="number">0</span>,len = X.length(),k = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(sum)&#123;</span><br><span class="line">            sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; len;i++)&#123;</span><br><span class="line">                d = data[i] / N;</span><br><span class="line">                sum += d;</span><br><span class="line">                <span class="keyword">if</span>(i == len - <span class="number">1</span>)&#123; output[k++] = data[i] % N;&#125;</span><br><span class="line">                <span class="keyword">else</span>&#123; data[i+<span class="number">1</span>] += (data[i] % N) * M;&#125;</span><br><span class="line">                data[i] = d;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(k == <span class="number">0</span>)&#123; output[k] = <span class="number">0</span>;k--;&#125;</span><br><span class="line">        <span class="keyword">if</span>(k == <span class="number">-1</span>)&#123; <span class="built_in">cout</span>&lt;&lt;<span class="number">0</span>&lt;&lt;<span class="built_in">endl</span>;&#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; k;i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(output[k-i<span class="number">-1</span>] &gt; <span class="number">9</span>)&#123; <span class="built_in">cout</span>&lt;&lt;(<span class="type">char</span>)(output[k-i<span class="number">-1</span>]+<span class="string">&#x27;a&#x27;</span><span class="number">-10</span>);&#125;</span><br><span class="line">                <span class="keyword">else</span>&#123; <span class="built_in">cout</span>&lt;&lt;output[k-i<span class="number">-1</span>];&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//大数阶乘</span></span><br><span class="line"><span class="type">int</span> a[<span class="number">20001</span>];<span class="comment">//存储每一位的数字</span></span><br><span class="line"><span class="type">int</span> temp,digit,n,i,j = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;n;</span><br><span class="line">a[<span class="number">0</span>] = <span class="number">1</span>;<span class="comment">//从1开始阶乘</span></span><br><span class="line">digit = <span class="number">1</span>;<span class="comment">//位数从第一位开始</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">2</span>;i &lt;= n;i++)&#123;</span><br><span class="line">    <span class="type">int</span> num = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>;j &lt; digit;j++)&#123;</span><br><span class="line">        temp = a[j]*i+num;<span class="comment">//将一个数字的每一位数都乘i</span></span><br><span class="line">        a[j] = temp % <span class="number">10</span>;</span><br><span class="line">        num = temp/<span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(num)&#123;</span><br><span class="line">        a[digit] = num%<span class="number">10</span>;</span><br><span class="line">        num = num/<span class="number">10</span>;</span><br><span class="line">        digit++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = digit<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>,a[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//判定素数</span></span><br><span class="line"><span class="type">bool</span> <span class="title function_">judge</span><span class="params">(<span class="type">int</span> x)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x &lt;= <span class="number">1</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">2</span>; i&lt;(<span class="type">int</span>)<span class="built_in">sqrt</span>(x)+<span class="number">1</span>;i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(x % i == <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//求两个数字的公约数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">gcd</span><span class="params">(<span class="type">int</span> a,<span class="type">int</span> b)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b==<span class="number">0</span>) <span class="keyword">return</span> a;<span class="comment">// 递归的跳出条件是a mod b =5 ;0</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> gcd(b, a % b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="库函数-string的增删查改"><a href="#库函数-string的增删查改" class="headerlink" title="库函数 string的增删查改"></a>库函数 string的增删查改</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#获取带空格的字符串</span><br><span class="line">getchar();<span class="comment">//专门吃回车</span></span><br><span class="line">getline(<span class="built_in">cin</span>,mystr);</span><br><span class="line">#插入字符串</span><br><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;BUPT&quot;</span>;</span><br><span class="line">s.insert(position,<span class="built_in">string</span>);</span><br><span class="line">#尾部插入字符串</span><br><span class="line">s.append(<span class="built_in">string</span>);</span><br><span class="line">s+=<span class="built_in">string</span>;</span><br><span class="line">#删除字符串</span><br><span class="line">s.erase(position,length);</span><br><span class="line">#比较字符串</span><br><span class="line"><span class="built_in">strcmp</span>(s1,s2);<span class="comment">//相同返回0,不相同返回1;</span></span><br><span class="line">#提取字符串</span><br><span class="line">s.substr(position,length);</span><br><span class="line">#查找字符串：str表示待查找的子串，position表示开始查找的下标/结束查找的下标</span><br><span class="line">res_pos = s.find(str,positon);</span><br><span class="line">res_pos = s.rfind(str,position)</span><br><span class="line"><span class="comment">//查找所有的bupt变成大写</span></span><br><span class="line"><span class="keyword">while</span> ((pos = s.find(<span class="string">&quot;bupt&quot;</span>,pos)) != <span class="built_in">string</span>::npos)&#123;</span><br><span class="line">    s.replace(pos, <span class="number">4</span>, <span class="string">&quot;BUPT&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">#字符串转换成其他类型</span><br><span class="line"><span class="built_in">string</span> str = <span class="string">&quot;12345.67&quot;</span>;</span><br><span class="line">**atoi(str.c_str())**/atof(str)/atol(str)这个特别牛逼</span><br><span class="line">#将整形转换为字符串</span><br><span class="line"><span class="type">char</span>(<span class="number">101</span>)<span class="comment">//输出e</span></span><br><span class="line"><span class="comment">//另外一种形式的</span></span><br><span class="line"><span class="type">char</span> str[<span class="number">100</span>];</span><br><span class="line">itoa(num, str, <span class="number">10</span>)<span class="comment">//10为进制数</span></span><br><span class="line">#代替：第<span class="number">19</span>个字符串以及后面的<span class="number">5</span>个字符用str的第<span class="number">7</span>个字符以及后面的<span class="number">5</span>个字符代替</span><br><span class="line">s.replace(<span class="number">19</span>,<span class="number">6</span>,str3,<span class="number">7</span>,<span class="number">6</span>);<span class="comment">//7&amp;6可以选择</span></span><br><span class="line">#比较：s1&amp;s2是否相同</span><br><span class="line"><span class="built_in">string</span> s1=<span class="string">&quot;123&quot;</span>,s2=<span class="string">&quot;123&quot;</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;s1.compare(s2)&lt;&lt;<span class="built_in">endl</span>;<span class="comment">//0</span></span><br><span class="line">#大小写转换</span><br><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;Hello world&quot;</span>;</span><br><span class="line">transform(s.begin(),s.end(),s.begin(),::<span class="built_in">toupper</span>);</span><br><span class="line">或者s[i] = <span class="built_in">toupper</span>(s[i]);</span><br><span class="line">#翻转字符串</span><br><span class="line"><span class="built_in">string</span> str = <span class="string">&quot;song&quot;</span>;</span><br><span class="line">reverse(str.begin(),str.end());</span><br></pre></td></tr></table></figure><ul><li><p>判定字符串是否结束</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[i] == <span class="string">&#x27;\0&#x27;</span>;<span class="comment">//字符串最后一位的后面还有&#x27;\0&#x27;结束符</span></span><br></pre></td></tr></table></figure></li><li><p>字符串逆序套路：</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">    temp = a[left];</span><br><span class="line">    a[left] = a[right];</span><br><span class="line">    a[right] = temp;</span><br><span class="line">    left++;</span><br><span class="line">    right--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="ascii码对照表"><a href="#ascii码对照表" class="headerlink" title="ascii码对照表"></a>ascii码对照表</h2><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/BUPT_OJ_experience.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> BUPT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>colah对BP算法的详细讲解</title>
      <link href="/2018/01/24/colah-bp-algorithm/"/>
      <url>/2018/01/24/colah-bp-algorithm/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="引入计算图"><a href="#引入计算图" class="headerlink" title="引入计算图"></a>引入计算图</h2><p>如下计算图是为$e=(a+b)(b+1)$.我们可以为节点$a$&amp;节点$b$赋值来计算$e$<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png" alt=""></p><h2 id="Factoring-Path（路径因式分解）"><a href="#Factoring-Path（路径因式分解）" class="headerlink" title="Factoring Path（路径因式分解）"></a>Factoring Path（路径因式分解）</h2><p>考虑对下图的链式法则：<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/chain-def-greek.png" alt=""><br>如果我们想要计算出$\frac{\partial Z}{\partial X}$，则必须要计算$3*3=9$条路径：</p><p>$$<br>\frac{\partial Z}{\partial X} = \alpha\delta + \alpha\epsilon + \alpha\zeta + \beta\delta + \beta\epsilon + \beta\zeta + \gamma\delta + \gamma\epsilon + \gamma\zeta（1）<br>$$</p><p>对上式进行因式分解则有：</p><p>$$<br>\frac{\partial Z}{\partial X} = (\alpha + \beta + \gamma)(\delta + \epsilon + \zeta) （2）<br>$$</p><p>公式（2）引出了“前向求导”和“反向求导”：它们都通过因式分解来高效计算偏导数。</p><h3 id="前向求导"><a href="#前向求导" class="headerlink" title="前向求导"></a>前向求导</h3><p>通过对每个节点上的路径的输入求和，我们可以得到总路径上的每个输入“影响”节点的偏导数。<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png" alt=""></p><h3 id="反向求导"><a href="#反向求导" class="headerlink" title="反向求导"></a>反向求导</h3><p>从计算图的输出节点出发，向初始节点移动并且在每个节点处合并从该节点引出的所有路径：<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png" alt=""></p><h3 id="前向求导和反向求导的对比"><a href="#前向求导和反向求导的对比" class="headerlink" title="前向求导和反向求导的对比"></a>前向求导和反向求导的对比</h3><ul><li>前向求导：<strong>跟踪每个输入如何影响所有节点</strong>,需要计算每个节点的$\frac{\partial}{\partial X}$。</li><li>反向求导：<strong>跟踪所有节点如何影响一个输出</strong>,需要计算每个节点的$\frac{\partial Z}{\partial}$。</li></ul><h2 id="计算速度的提升"><a href="#计算速度的提升" class="headerlink" title="计算速度的提升"></a>计算速度的提升</h2><p>如果对计算图的每条边添加偏导数则有：<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" alt=""></p><ul><li><p>当我们使用前向求导来计算每个节点对$b$的偏导数则有：<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/tree-forwradmode.png" alt=""><br>由上图可见计算出$\frac{\partial e}{\partial b}$的代价是需要计算很多“无用”的偏导数。[注：“无用”是因为在梯度下降的过程中，我们只关心输出对每个节点的偏导数]</p></li><li><p>当我们使用反向求导计算$e$对各个节点的偏导数有：<br><img src="https://colah.github.io/posts/2015-08-Backprop/img/tree-backprop.png" alt=""><br>对比两张图可以发现，反向求导“真正”的给出了我们想要的偏导数。在有百万级别的输入时的极端情况下，前向求导需要计算百万次才能得到输出结果对所有节点的偏导数。而反向求导只需要计算一次。<br>在我们训练神经网络的时候需要计算对所有参数的偏导数用以梯度下降，因此反向求导极大的提升了计算速度。</p></li></ul><p><strong>Once you’ve framed the question, the hardest work is already done</strong> </p><blockquote><p>参考与引用：<br><a href="https://colah.github.io/posts/2015-08-Backprop/">https://colah.github.io/posts/2015-08-Backprop/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> backpropagation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见技术投资指标学习</title>
      <link href="/2018/01/23/quantitative-investment-index/"/>
      <url>/2018/01/23/quantitative-investment-index/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="DDX"><a href="#DDX" class="headerlink" title="DDX"></a>DDX</h2><blockquote><p>大单买入量 (以占流通盘比例的方式) ，目前主力造假较多(拆单软件)。<br>红色买入，绿色卖出。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>若股价大幅拉升&amp;高DDX值：主力疯狂拉动，介入风险极大。</li><li>安全介入点：DDX与DDY背离</li><li>换手率作为补充条件：以超过3%为标准</li></ul></blockquote><h2 id="DDY"><a href="#DDY" class="headerlink" title="DDY"></a>DDY</h2><blockquote><p>当日卖出单数减去买入单数占散户人数(估算值)的比例。也即散户出货比例,红柱说明出的多，散户仓位下降，主力仓位相对增加。</p><h3 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>DDX与DDY的同向运动：既有大单而且主力加仓/主力偷偷用中小单加仓。</li></ul></blockquote><h2 id="DDZ"><a href="#DDZ" class="headerlink" title="DDZ"></a>DDZ</h2><blockquote><p>行情启动力度：对DDX和DDY的重要辅助。</p><h3 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>入场必要条件：DDX和DDY同向运动。入场充分条件：DDZ大于20</li></ul></blockquote><h2 id="MACD"><a href="#MACD" class="headerlink" title="MACD"></a>MACD</h2><blockquote><p>即指数平滑移动平均线。中长期分析手段，产生的交叉信号对短线操作反应相对滞后。</p></blockquote><h3 id="MACD的指标包含三个子指标"><a href="#MACD的指标包含三个子指标" class="headerlink" title="MACD的指标包含三个子指标:"></a>MACD的指标包含三个子指标:</h3><ul><li>$DIF=EMA(close，12）-EMA（close，26）$<br>其中：指数平滑移动平均线（EMA）</li><li>DEA=DIFF的M日的平均的指数平滑移动平均线，记为DEA</li><li>$ MACD=DIFF-DEA $</li></ul><h3 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>当DIF和DEA处于0轴以上时，属于多头市场。</li><li>当DIF和DEA处于0轴以下时，属于空头市场。</li><li>长期！长期！长期！</li></ul><h2 id="KDJ"><a href="#KDJ" class="headerlink" title="KDJ"></a>KDJ</h2><blockquote><p>用于中短期趋势分析的随机指标</p><h3 id="指标计算说明"><a href="#指标计算说明" class="headerlink" title="指标计算说明"></a>指标计算说明</h3><p>$$RSV_n = 100(\frac{C_n-L_n}{H_n-L_n})$$</p></blockquote><p>其中$C_n$为第n日收盘价，$L_n$为n日内最低价，$H_n$为n日内最高价。</p><p>$$K<em>t = \frac23K</em>{t-1}+\frac13RSV_t$$<br>$$D<em>t = \frac23D</em>{t-1}+\frac13K_t$$<br>$$J<em>t = 3K</em>{t}-2D_t$$</p><h3 id="注意事项-4"><a href="#注意事项-4" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>D大于80时，行情呈现超买现象。D小于20时，行情呈现超卖现象。</li><li>上涨趋势中，K值大于D值，K线向上突破D线时，为买进信号。下跌趋势中，K值小于D值，K线向下跌破D线时，为卖出信号。</li><li>KD指标不适于发行量小、交易不活跃的股票，但是KD指标对大盘和热门大盘股有极高准确性。当随机指标与股价出现背离时，一般为转势的信号。</li></ul><h2 id="RSI"><a href="#RSI" class="headerlink" title="RSI"></a>RSI</h2><blockquote><p>通过测量某一个期间内股价上涨总幅度占股价变化总幅度平均值的百分比，来评估多空力量的强弱程度。</p></blockquote><h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>$$ RSI_n = \frac{100*收盘涨幅平均值_n}{(收盘涨幅平均值_n+收盘跌幅平均值_n)} $$</p><p>其中：上的力量较大，则计算出来的指标上升；若下的力量较大，则指标下降，由此测算出市场走势的强弱。</p><h3 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h3><p>: RSI值如果超过50,表明市场进入强市,可以考虑买入,但是如果继续进入”极强”区,就要考虑物极必反,准备卖出了。同理RSI值在50以下也是如此,如果进入了”极弱”区,则表示超卖,应该伺机买入。</p><h2 id="BOLL"><a href="#BOLL" class="headerlink" title="BOLL"></a>BOLL</h2><blockquote><p>标准差原理设计而来，核心概念是：“股价通道”。且BOLL线分为上轨线UP 、中轨线MB、下轨线DN和价格线。</p></blockquote><h3 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h3><p>$$中轨线=N日的移动平均线$$<br>$$上轨线=中轨线+两倍的标准差$$<br>$$下轨线=中轨线－两倍的标准差$$</p><h3 id="使用说明-1"><a href="#使用说明-1" class="headerlink" title="使用说明"></a>使用说明</h3><p>如果股价脱离股价信道运行，则意味着行情处于极端的状态下。</p><ul><li>当布林线的上、中、下轨线同时向上运行时，表明股价强势特征非常明显，股价短期内将继续上涨，投资者应坚决持股待涨或逢低买入。</li></ul><h2 id="WR"><a href="#WR" class="headerlink" title="WR"></a>WR</h2><blockquote><p>威廉指标：短线超买超卖指标。<br>$$WR_n = \frac{High_n-Close}{High_n-Low_n}$$</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>反映过于灵敏，指标波动频率过快，引起信号频发现象，误差率也非常高，过多的信号和严重的误差率造成投资者不敢轻易使用它。应改动时间参数使之适合模型。</p></blockquote><h2 id="OBV：能量潮指标"><a href="#OBV：能量潮指标" class="headerlink" title="OBV：能量潮指标"></a>OBV：能量潮指标</h2><blockquote><p>On Balance Volumn,结合成交量的指标！<strong>重要假设原理</strong>：通常股价上升所需的成交量总是较大；下跌时，则成交量可能放大，也可能较小。价格升降而成交量不相应升降，则市场价格的变动难以为继。</p></blockquote><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><ul><li>当股价上升而OBV线下降，表示买盘无力，股价可能会回跌。</li><li>股价下降时而OBV线上升，表示买盘旺盛，逢低接手强股，股价可能会止跌回升。</li></ul><h2 id="BIAS"><a href="#BIAS" class="headerlink" title="BIAS"></a>BIAS</h2><blockquote><p>乖离率指标：用百分比表示价格与MA之间的偏离程度。</p></blockquote><h3 id="计算公式："><a href="#计算公式：" class="headerlink" title="计算公式："></a>计算公式：</h3><p>$$乖离率=\frac{(当日收盘价-N日平均价)}{N日平均价}*100%$$</p>]]></content>
      
      
      <categories>
          
          <category> QuantitativeInvestment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> index </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python语言学习</title>
      <link href="/2017/12/07/Python-language/"/>
      <url>/2017/12/07/Python-language/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h1><h2 id="静态函数，类函数，成员函数，属性函数的区别"><a href="#静态函数，类函数，成员函数，属性函数的区别" class="headerlink" title="静态函数，类函数，成员函数，属性函数的区别"></a>静态函数，类函数，成员函数，属性函数的区别</h2><h3 id="补充知识："><a href="#补充知识：" class="headerlink" title="补充知识："></a>补充知识：</h3><ul><li>实例变量：每个实例独有的变量</li><li><p>类变量：所有实例共享的变量</p><p>形象理解：哈士奇（类变量）是狗的一种，而每只狗都只会有一个主人给它取<strong>唯一</strong>的名字，例如“蛤蛤”（成员变量）。用代码来解释就是</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>:</span><br><span class="line">    kind = <span class="string">&quot;Husky&quot;</span> <span class="comment"># 类变量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name</span>):</span><br><span class="line">        self.name = name <span class="comment"># 实例变量</span></span><br></pre></td></tr></table></figure><h3 id="四类函数的定义"><a href="#四类函数的定义" class="headerlink" title="四类函数的定义"></a>四类函数的定义</h3><p>明确了实例变量和类变量之后，四类函数就很好理解了。先看官方定义：</p></li><li><p>静态函数（@staticmethod）:即静态方法,不可以访问实例变量或类变量。</p></li><li>类函数（@classmethod）：只能访问类变量，不能访问实例变量。</li><li>成员函数：实例后使用的方法，且只能通过实例进行调用。</li><li>属性函数（@property）：通过装饰器@property把一个方法变成一个静态属性</li></ul><h3 id="四类函数的形象解释"><a href="#四类函数的形象解释" class="headerlink" title="四类函数的形象解释"></a>四类函数的形象解释</h3><p>再次请出“蛤蛤”，首先讲解静态函数:通俗的说静态方法就是函数的形参中没有<strong>self</strong>。也即，静态函数与类中的静态变量&amp;实例变量都没有半毛钱关系<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bark</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;my name is Haha&quot;</span>)</span><br><span class="line"></span><br><span class="line">d1 = Dog()</span><br><span class="line">d1.bark()</span><br><span class="line"><span class="comment"># 使用静态方法，输出结果为my name is Haha</span></span><br></pre></td></tr></table></figure><br>那么类函数呢？<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    name = <span class="string">&quot;Haha&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bark</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;my name is %s&quot;</span> % cls.name)</span><br><span class="line"></span><br><span class="line">d2 = Dog()</span><br><span class="line">d2.bark()</span><br><span class="line"><span class="comment"># 使用类方法，输出结果为my name is Haha</span></span><br></pre></td></tr></table></figure><br>再看看成员函数<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    name = <span class="string">&quot;Haha&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nick_name</span>):</span><br><span class="line">        self.nick_name = nick_name</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bark</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;my name is %s and my nick name is %s&quot;</span> % (name,self.nick_name))</span><br><span class="line">d3 = Dog(<span class="string">&quot;Heiha&quot;</span>)</span><br><span class="line">d3.bark()</span><br><span class="line"><span class="comment"># 使用成员函数，输出结果为my name is Haha and my nick name is Heiha</span></span><br><span class="line">```    </span><br><span class="line">最后看看属性函数：</span><br><span class="line">```py</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>(<span class="title class_ inherited__">obkect</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name</span>):</span><br><span class="line">        self.name = name</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bark</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;my name is %s&quot;</span> % self.name)</span><br><span class="line"><span class="comment"># 使用属性函数的方式需要注意！此时bark函数已经是Dog类的一个属性。</span></span><br><span class="line">d4 = Dog(<span class="string">&quot;Haha&quot;</span>)</span><br><span class="line"><span class="comment"># 如此使用属性函数，输出结果为myname is Haha</span></span><br><span class="line">d4.talk</span><br></pre></td></tr></table></figure></p><h3 id="补充：-property的使用"><a href="#补充：-property的使用" class="headerlink" title="补充：@property的使用"></a>补充：@property的使用</h3><h4 id="为什么需要有-property装饰器？"><a href="#为什么需要有-property装饰器？" class="headerlink" title="为什么需要有@property装饰器？"></a>为什么需要有@property装饰器？</h4><blockquote><p>编程中出现的问题：绑定属性的时候无法检查参数！</p></blockquote><ul><li>解决方案一：在类中定义set()和get()方法来设置与获取成绩。这么写是OK的，但是当你把类进行实例化之后，就会懂得这个方案的麻烦程度了。</li></ul><h4 id="property把方法变成属性调用"><a href="#property把方法变成属性调用" class="headerlink" title="@property把方法变成属性调用"></a>@property把<strong>方法变成属性</strong>调用</h4><p>　使用方法：把你想要设置的属性变成一个getter方法，在加上<code>@property</code>装饰器，在setter方法里就可以检查参数的合法性了。<br>　实例化之后，就可以直接绑定属性而不用再去调用类方法！</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._score</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @score.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, value</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(value, <span class="built_in">int</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;score must be an integer!&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;scofre must between 0-100!&#x27;</span>)</span><br><span class="line">        self._score = value</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">s = Student()</span><br><span class="line">s.score = <span class="number">60</span> <span class="comment"># equal to s.set_score(60)</span></span><br><span class="line">s.score = <span class="number">101</span> </span><br><span class="line"><span class="comment"># Traceback (most recent call last):</span></span><br><span class="line">  ...</span><br><span class="line">ValueError: score must between <span class="number">0</span> ~ <span class="number">100</span>!</span><br></pre></td></tr></table></figure><h3 id="四类函数的小总结："><a href="#四类函数的小总结：" class="headerlink" title="四类函数的小总结："></a>四类函数的小总结：</h3><p>综上所述，其实我们可以四类函数当做一个逐渐释放权力的过程。当使用<strong>静态函数</strong>时，类的任何信息你都无权得到；当你使用<strong>类函数</strong>时，你只可以得到类变量里面的信息，而无法得到实例化之后的类的信息；只有当你使用<strong>成员函数</strong>，你的权力才得到完整的释放，你既可以使用类变量的信息又可以使用实例化后类的信息。而<strong>属性函数</strong>则是把一个函数当做属性来看待，这是最好理解的。</p><h3 id="我还是不懂何时使用它们怎么办？"><a href="#我还是不懂何时使用它们怎么办？" class="headerlink" title="我还是不懂何时使用它们怎么办？"></a>我还是不懂何时使用它们怎么办？</h3><p>记住核心思想：</p><ol><li>类函数主要用途是作为构造函数。</li><li>静态函数的形参没有self</li><li>成员函数用的最多</li></ol><h2 id="类的访问控制：单下划线-与双下划线"><a href="#类的访问控制：单下划线-与双下划线" class="headerlink" title="类的访问控制：单下划线_与双下划线__"></a>类的访问控制：单下划线_与双下划线__</h2><ol><li>“_”:单下划线表示只允许其本身与其子类进行访问</li><li>“__”:双下划线表示只允许这个类本身进行访问。</li></ol><h1 id="python语言特性"><a href="#python语言特性" class="headerlink" title="python语言特性"></a>python语言特性</h1><h2 id="python是动态类型语言"><a href="#python是动态类型语言" class="headerlink" title="python是动态类型语言"></a>python是动态类型语言</h2><blockquote><p>在编写代码的时候可以不指定变量的数据类型</p></blockquote><h2 id="with语句："><a href="#with语句：" class="headerlink" title="with语句："></a>with语句：</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. with语句的语法格式</span></span><br><span class="line"><span class="keyword">with</span> content_expression [<span class="keyword">as</span> targer(s)]:</span><br><span class="line">    <span class="keyword">with</span>-body</span><br><span class="line"><span class="comment"># 2. with语句操作文件对象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&#x27;somefileName&#x27;</span>) <span class="keyword">as</span> somefile:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> somefile:</span><br><span class="line">        <span class="built_in">print</span> line</span><br><span class="line">        <span class="comment"># ... more code</span></span><br><span class="line"><span class="comment"># 3. 上文代码等价于下文代码：</span></span><br><span class="line">somefile = <span class="built_in">open</span>(<span class="string">r&#x27;openfileName&#x27;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> somefile:</span><br><span class="line">        <span class="built_in">print</span> line</span><br><span class="line">        <span class="comment"># ...more code</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    somefile.close()</span><br></pre></td></tr></table></figure><h2 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h2><ol><li>一个简单易懂的例子:如果某些代码<strong>可能会出错</strong><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    r = <span class="number">10</span> / <span class="number">0</span></span><br><span class="line"><span class="keyword">except</span> ZeroDivisonError, e:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;ZeroDivisionError&quot;</span>, e</span><br><span class="line"><span class="keyword">except</span> ValueError, e:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;ValueError&quot;</span>, e</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;finally&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>记录错误：打印错误信息，同时让程序继续执行</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span> / <span class="built_in">int</span>(s)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        foo(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> StandardError, e:</span><br><span class="line">        logging.exception(e)</span><br><span class="line">main()</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;END&quot;</span></span><br></pre></td></tr></table></figure><p>　程序打印完错误信息后会继续执行，并且正常退出。</p></li><li><p>抛出错误</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="number">10</span> / <span class="number">0</span></span><br><span class="line"><span class="keyword">except</span> ZeroDivisionError:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&#x27;input error!&#x27;</span>)</span><br></pre></td></tr></table></figure></li></ol><h1 id="代码trick"><a href="#代码trick" class="headerlink" title="代码trick"></a>代码trick</h1><h2 id="再见！中间变量"><a href="#再见！中间变量" class="headerlink" title="再见！中间变量"></a>再见！中间变量</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line"><span class="comment">#不使用中间变量交换两个变量的值</span></span><br><span class="line">a, b = b, a</span><br></pre></td></tr></table></figure><h2 id="字符串逆序"><a href="#字符串逆序" class="headerlink" title="字符串逆序"></a>字符串逆序</h2><p>实质是把字符串转换成列表进行操作<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">&quot;python&quot;</span></span><br><span class="line"><span class="comment"># 方法一:看起来很装逼的方法</span></span><br><span class="line"><span class="built_in">print</span> a[::-<span class="number">1</span>] <span class="comment"># &quot;nohtyp&quot; </span></span><br><span class="line"><span class="comment"># 方法二：可读性强的方法</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">list</span>(a).reverse()</span><br></pre></td></tr></table></figure></p><h2 id="python可以使用连续赋值-从左至右"><a href="#python可以使用连续赋值-从左至右" class="headerlink" title="python可以使用连续赋值, 从左至右"></a>python可以使用连续赋值, 从左至右</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="comment"># 当我们连续赋值的时候</span></span><br><span class="line">i, x[i] = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"><span class="comment"># print(x)的结果为[0, 2]:编译器先把1赋值给了i，那么x[i]即为x[1]，接下来把2赋值给了x[1]。</span></span><br></pre></td></tr></table></figure><h2 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a>矩阵转置</h2><p>其原理为zip()函数：将对象中对应的元素打包成一个个元组，并且返回由元组组成的列表。这恰好符合矩阵转置的操作<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mat = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line"><span class="comment"># *可以简单理解为解压</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">zip</span>(*mat)</span><br><span class="line"><span class="comment">#[(1, 4), (2, 5), (3, 6)]</span></span><br></pre></td></tr></table></figure></p><h2 id="列表转字符串"><a href="#列表转字符串" class="headerlink" title="列表转字符串"></a>列表转字符串</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">&quot;Code&quot;</span>, <span class="string">&quot;mentor&quot;</span>, <span class="string">&quot;Python&quot;</span>, <span class="string">&quot;Developer&quot;</span>]</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot; &quot;</span>.join(a)</span><br><span class="line"><span class="comment"># Code mentor Python Developer</span></span><br></pre></td></tr></table></figure><h2 id="在一个for循环中遍历两个列表"><a href="#在一个for循环中遍历两个列表" class="headerlink" title="在一个for循环中遍历两个列表"></a>在一个for循环中遍历两个列表</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>]</span><br><span class="line">list2 = [<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;boy&#x27;</span>,<span class="string">&#x27;cat&#x27;</span>,<span class="string">&#x27;dog&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(list1, list2):</span><br><span class="line">    <span class="built_in">print</span> x <span class="string">&#x27;is&#x27;</span> y</span><br></pre></td></tr></table></figure><h2 id="同时处理多个文件"><a href="#同时处理多个文件" class="headerlink" title="同时处理多个文件"></a>同时处理多个文件</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename1) <span class="keyword">as</span> fp1, <span class="built_in">open</span>(filename2) <span class="keyword">as</span> fp2, <span class="built_in">open</span>(filename3) <span class="keyword">as</span> fp3:</span><br><span class="line"><span class="keyword">for</span> l1 <span class="keyword">in</span> fp1:</span><br><span class="line">    l2 = fp2.readline()</span><br><span class="line">    l3 = fp3.readline()</span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><h2 id="给字符串前面补“0”"><a href="#给字符串前面补“0”" class="headerlink" title="给字符串前面补“0”"></a>给字符串前面补“0”</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num = <span class="string">&quot;123&quot;</span></span><br><span class="line"><span class="comment"># 向num前面填充0至总位数为5</span></span><br><span class="line">str_n = num.zfill(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h1 id="回调函数-callback"><a href="#回调函数-callback" class="headerlink" title="回调函数 callback()"></a>回调函数 callback()</h1><h2 id="官方解释："><a href="#官方解释：" class="headerlink" title="官方解释："></a>官方解释：</h2><blockquote><p>回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。</p></blockquote><h2 id="形象解释："><a href="#形象解释：" class="headerlink" title="形象解释："></a>形象解释：</h2><blockquote><p><strong>吃饭、喝水、睡觉</strong>就是一个通过<strong>大脑</strong>调用的<strong>动作</strong>。如果你把<strong>这些动作</strong>作为参数传递给<strong>大脑</strong>，当<strong>这些动作</strong>被用来<strong>真正去执行</strong>时，我们就说这是<strong>回调函数</strong>。</p></blockquote><h2 id="代码解释："><a href="#代码解释：" class="headerlink" title="代码解释："></a>代码解释：</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eat</span>(<span class="params">food</span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;I will eat &quot;</span>, food</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drink</span>(<span class="params">beverage</span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;I will drink &quot;</span>, beverage</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">brain</span>(<span class="params">action, target</span>):</span><br><span class="line">    <span class="keyword">return</span> action(target)</span><br><span class="line">```     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># __all__与__metaclass__的使用</span></span><br><span class="line"><span class="comment">## __all__</span></span><br><span class="line">**第一时间**展现了模块的内容大纲，更**清晰**地提供了外部访问的接口。但是要注意，只有列表里内内容才会暴露出去！</span><br><span class="line"></span><br><span class="line">请看代码：</span><br><span class="line"><span class="comment">### 测试文件foo.py</span></span><br><span class="line">```py</span><br><span class="line">__all__ = [<span class="string">&#x27;pig&#x27;</span>, <span class="string">&#x27;monkey&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>]</span><br><span class="line"></span><br><span class="line">deer = <span class="number">10086</span></span><br><span class="line">monkey = <span class="number">10010</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pig</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;awesome&quot;</span></span><br></pre></td></tr></table></figure><h3 id="测试文件"><a href="#测试文件" class="headerlink" title="测试文件"></a>测试文件</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> foo <span class="keyword">import</span> *</span><br><span class="line"><span class="built_in">print</span>(bar)</span><br><span class="line"><span class="built_in">print</span>(monkey)</span><br><span class="line"><span class="built_in">print</span>(deer) </span><br></pre></td></tr></table></figure><blockquote><p>参考与引用</p><ol><li><a href="https://m.baidu.com/?from=844b&amp;vit=fps&amp;nsukey=QL%2FYJKapoliD84rv4ROf08qlZJ83uWDoXzrVj">https://m.baidu.com/?from=844b&amp;vit=fps&amp;nsukey=QL%2FYJKapoliD84rv4ROf08qlZJ83uWDoXzrVj</a></li><li><a href="https://www.cnblogs.com/scf141592/p/5726347.html">https://www.cnblogs.com/scf141592/p/5726347.html</a></li><li><a href="https://www.cnblogs.com/crazyrunning/p/6945183.html">https://www.cnblogs.com/crazyrunning/p/6945183.html</a></li><li><a href="https://www.cnblogs.com/polly333/p/8143672.html">https://www.cnblogs.com/polly333/p/8143672.html</a></li><li><a href="https://www.cnblogs.com/crazyrunning/p/6945183.html">https://www.cnblogs.com/crazyrunning/p/6945183.html</a></li><li><a href="https://www.cnblogs.com/imageSet/p/7473326.html">https://www.cnblogs.com/imageSet/p/7473326.html</a></li><li><a href="https://www.jianshu.com/p/4bd8a1c93cbe">https://www.jianshu.com/p/4bd8a1c93cbe</a></li><li><a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386820062641f3bcc60a4b164f8d91df476445697b9e000">https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386820062641f3bcc60a4b164f8d91df476445697b9e000</a></li><li><a href="https://www.liaoxuefeng.com/">https://www.liaoxuefeng.com/</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/">https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/</a></li><li><a href="https://www.cnblogs.com/alamZ/p/6943869.html">https://www.cnblogs.com/alamZ/p/6943869.html</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> language </tag>
            
            <tag> code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex&amp;Mathjax使用</title>
      <link href="/2017/09/06/Latex-Mathjex-usage/"/>
      <url>/2017/09/06/Latex-Mathjex-usage/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a>Latex</h1><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><p>在Mac上通过Sublime、Skim编辑LaTeX</p><h3 id="在Sublime-Text中安装Package-Control"><a href="#在Sublime-Text中安装Package-Control" class="headerlink" title="在Sublime Text中安装Package Control"></a>在Sublime Text中安装Package Control</h3><ul><li>进入<a href="https://packagecontrol.io/installation">Package Control官网</a>复制灰色区块的代码。</li><li>打开Sublime Text。</li><li>使用快捷键“control+~”（~就在Esc键的下方）打开控制面板Console。你会在Sublime Text的底部看到弹出一个白色窗口。</li><li>将刚才复制的代码粘贴到控制面板。</li><li>按下“Enter”回车键。然后退出并重启Sublime Text。</li></ul><h3 id="安装LaTex-Tools"><a href="#安装LaTex-Tools" class="headerlink" title="安装LaTex Tools"></a>安装LaTex Tools</h3><ul><li>Sublime Text重启后，按下“Command+Shift+P”打开命令托盘Command pallet，这一步也可以通过Tools下拉菜单完成。</li><li>在命令托盘里输入“Install Package”，按下Enter回车建。</li><li>完成之后，输入“LaTeX Tools”，找到这一项并回车安装。</li><li>退出并重启Sublime Text。</li></ul><h3 id="安装Skim"><a href="#安装Skim" class="headerlink" title="安装Skim"></a>安装Skim</h3><ul><li>进<a href="https://skim-app.sourceforge.io/">Skim</a>下载Skim并安装</li><li>打开Skim，在菜单栏中Skim &gt; Preference(选项) &gt; Sync(同步)</li><li>在预设菜单中选择Sublime Text<br>skim。</li><li>关闭上面这个窗口。</li></ul><h3 id="环境的基本使用"><a href="#环境的基本使用" class="headerlink" title="环境的基本使用"></a>环境的基本使用</h3><ol><li>打开Sublim Text, Command+N新建文件在里面编写LaTeX代码了。</li><li>完成编辑之后，Command+S保存文件。</li><li>Command+B编译并运行，这时就可以在Skim里面看到PDF预览了。</li></ol><hr><h2 id="命令与环境-对大小写敏感"><a href="#命令与环境-对大小写敏感" class="headerlink" title="命令与环境(对大小写敏感)"></a>命令与环境(对大小写敏感)</h2><ul><li>命令以\开头。 如<code>\LaTeX</code></li><li>源代码结构<figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>&#123;...&#125; //使用文档类</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;...&#125; //调用宏包</span><br><span class="line">    <span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line">    <span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="宏包和文档类"><a href="#宏包和文档类" class="headerlink" title="宏包和文档类"></a>宏包和文档类</h2><h3 id="文档类-documentclass-lt-options-gt-lt-class-name-gt"><a href="#文档类-documentclass-lt-options-gt-lt-class-name-gt" class="headerlink" title="文档类\documentclass[&lt;options&gt;]{&lt;class-name&gt;}"></a>文档类<code>\documentclass[&lt;options&gt;]&#123;&lt;class-name&gt;&#125;</code></h3><h4 id="lt-class-name-gt-包括："><a href="#lt-class-name-gt-包括：" class="headerlink" title="&lt;class-name&gt;包括："></a><code>&lt;class-name&gt;</code>包括：</h4><ul><li><strong>article</strong> 论文，报告，说明文档</li><li>report 长篇文档类</li><li>book 书籍文档类</li><li>proc 基于article文档类的学术文档</li><li>slides 幻灯格式的文档类</li><li>minimal 精简的文档类</li></ul><h4 id="lt-options-gt-包括："><a href="#lt-options-gt-包括：" class="headerlink" title="&lt;options&gt;包括："></a><code>&lt;options&gt;</code>包括：</h4><p>　例：纸张为A4,基本字号为11pt，双面排版<code>\documentclass[11pt,twoside,a4paper]&#123;article&#125;</code></p><ul><li>基本字号：10pt，11pt,12pt，缺省为10pt</li><li>纸张大小：a4paper,letterpaper,a5paper,b5pape</li><li>公式位置：fleqn 令行间公式左对齐（缺省为居中）</li></ul><h2 id="文件的组织方式"><a href="#文件的组织方式" class="headerlink" title="文件的组织方式"></a>文件的组织方式</h2><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\include</span>&#123;&lt;filename&gt;&#125;</span><br><span class="line"><span class="keyword">\input</span>&#123;&lt;filename&gt;&#125;</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="params">##</span> 用LaTeX排版文字</span><br><span class="line"><span class="params">###</span> UTF-8编码</span><br><span class="line">```latex</span><br><span class="line"><span class="keyword">\usepackage</span>[utf-8]&#123;inputenc&#125;</span><br></pre></td></tr></table></figure><h3 id="排版中文"><a href="#排版中文" class="headerlink" title="排版中文"></a>排版中文</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>&#123;ctexart&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line">中文LaTeX排版</span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h3 id="LaTeX中的符号"><a href="#LaTeX中的符号" class="headerlink" title="LaTeX中的符号"></a>LaTeX中的符号</h3><ol><li><p>空格与分段<br>　一个与多个空格&amp;一个与多个回车效果相同</p></li><li><p>注释 %</p></li></ol><h2 id="文档元素"><a href="#文档元素" class="headerlink" title="文档元素"></a>文档元素</h2><h3 id="章节标题"><a href="#章节标题" class="headerlink" title="章节标题"></a>章节标题</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\section</span>&#123;&lt;title&gt;&#125; <span class="keyword">\subsection</span>&#123;&lt;title&gt;&#125; <span class="keyword">\subsubsection</span>&#123;&lt;title&gt;&#125; <span class="keyword">\paragraph</span>&#123;&lt;title&gt;&#125; <span class="keyword">\subparagraph</span>&#123;&lt;title&gt;&#125;</span><br></pre></td></tr></table></figure><h3 id="居中插入url"><a href="#居中插入url" class="headerlink" title="居中插入url"></a>居中插入url</h3><p>\begin{center}<br>  \url{<a href="https://cmt.research.microsoft.com/NIPS2018/}">https://cmt.research.microsoft.com/NIPS2018/}</a><br>\end{center}</p><hr><h2 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h2><h3 id="Latex中文utf-8编码"><a href="#Latex中文utf-8编码" class="headerlink" title="Latex中文utf-8编码"></a>Latex中文utf-8编码</h3><p>使用CJKutf8解决问题<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;CJKutf8&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;&lt;font&gt;&#125;</span><br><span class="line"> ...</span><br><span class="line">\end&#123;CJK&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure></p><p>font为简体中文字体，CJK自带的utf-8简体字体有gbsn（宋体）和gkai（楷体）。以下代码是一个简单的例子（一定要将tex文件保存成utf-8格式）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%test.tex</span><br><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\usepackage&#123;CJKutf8&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</span><br><span class="line">这是一个CJKutf8的例子，使用的字体是gbsn。</span><br><span class="line">\end&#123;CJK&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="MathJax的基本使用"><a href="#MathJax的基本使用" class="headerlink" title="MathJax的基本使用"></a>MathJax的基本使用</h1><ol><li>希腊字母</li></ol><p>　　　　名称：alpha,大写:$A$,Tex:A,小写:$\alpha$,Tex:\alpha</p><ol><li>括号</li></ol><ul><li>大括号：\lbrace,\rbrace </li><li>尖括号：\langle,\rangle</li><li>取整：\lceil,\lfloor,\rceil,\floor</li></ul><ol><li>求和积分累乘</li></ol><ul><li>求和与乘法：\sum，\cdot</li><li>积分：\int,\iint</li><li>累乘：\prod</li></ul><ol><li>特殊函数和符号</li></ol><ul><li>三角函数：\sin,\arctan,\lim</li><li>比较运算符：小于（\lt）、大于(\gt)、小于等于(\le)、大于等于(ge)、不等于(neq)</li><li>箭头：右箭头(\rightarrow)、左箭头(\leftarrow)、双重右箭头(\Rightarrow)、双重左箭头(\Leftarrow)</li><li>顶部符号：单字符估计(\hat)、多字符估计(\widehat)、均值(\overline)、向量(\vec)</li><li>其他运算符：无穷(\infty)，微分算子(\nabla)，偏导数(\partial)</li></ul><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><ul><li>Typewriter：\mathtt{A}呈现为𝙰, 𝙰𝙱𝙲𝙳𝙴𝙵𝙶𝙷𝙸𝙹𝙺𝙻𝙼𝙽𝙾𝙿𝚀𝚁𝚂𝚃𝚄𝚅𝚆𝚇𝚈𝚉</li><li>Blackboard Bold：\mathbb{A}呈现为𝔸, 𝔸𝔹ℂ𝔻𝔼𝔽𝔾ℍ𝕀𝕁𝕂𝕃𝕄ℕ𝕆ℙℚℝ𝕊𝕋𝕌𝕍𝕎𝕏𝕐ℤ</li><li>Sans Serif：\mathsf{A}呈现为𝖠, 𝖠𝖡𝖢𝖣𝖤𝖥𝖦𝖧𝖨𝖩𝖪𝖫𝖬𝖭𝖮𝖯𝖰𝖱𝖲𝖳𝖴𝖵𝖶𝖷𝖸𝖹</li></ul><h2 id="给公式添加序号"><a href="#给公式添加序号" class="headerlink" title="给公式添加序号"></a>给公式添加序号</h2><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数学公式<span class="keyword">\eqno</span>编号</span><br></pre></td></tr></table></figure><h2 id="非常见错误"><a href="#非常见错误" class="headerlink" title="非常见错误"></a>非常见错误</h2><h3 id="无法识别公式内十分少用的符号"><a href="#无法识别公式内十分少用的符号" class="headerlink" title="无法识别公式内十分少用的符号"></a>无法识别公式内十分少用的符号</h3><p>尝试引入包：<br><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;amssymb&#125;        <span class="comment">% prevent bugs for formula symbol</span></span><br></pre></td></tr></table></figure></p><blockquote><p>参考与引用</p><ol><li><a href="https://www.cnblogs.com/linxd/p/4955530.html">https://www.cnblogs.com/linxd/p/4955530.html</a></li><li><a href="https://blog.csdn.net/ethmery/article/details/50670297">https://blog.csdn.net/ethmery/article/details/50670297</a></li><li><a href="https://www.cnblogs.com/gslyyq/p/5043848.html">https://www.cnblogs.com/gslyyq/p/5043848.html</a></li><li><a href="https://www.cnblogs.com/dezheng/p/3874434.html">https://www.cnblogs.com/dezheng/p/3874434.html</a></li><li><a href="https://blog.csdn.net/woniuxyy/article/details/80300322">https://blog.csdn.net/woniuxyy/article/details/80300322</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> latex </tag>
            
            <tag> mathjex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手写笔记目录</title>
      <link href="/2017/07/30/handwriting-note/"/>
      <url>/2017/07/30/handwriting-note/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="研究生课程笔记"><a href="#研究生课程笔记" class="headerlink" title="研究生课程笔记"></a>研究生课程笔记</h2><h3 id="模式识别与机器学习"><a href="#模式识别与机器学习" class="headerlink" title="模式识别与机器学习"></a>模式识别与机器学习</h3><h4 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h4><h4 id="考试大抄"><a href="#考试大抄" class="headerlink" title="考试大抄"></a>考试大抄</h4><h3 id="信息科学原理"><a href="#信息科学原理" class="headerlink" title="信息科学原理"></a>信息科学原理</h3><h4 id="课堂笔记-1"><a href="#课堂笔记-1" class="headerlink" title="课堂笔记"></a>课堂笔记</h4><h4 id="题目索引"><a href="#题目索引" class="headerlink" title="题目索引"></a>题目索引</h4><h4 id="题目参考"><a href="#题目参考" class="headerlink" title="题目参考"></a>题目参考</h4><h2 id="自学课程笔记"><a href="#自学课程笔记" class="headerlink" title="自学课程笔记"></a>自学课程笔记</h2><h3 id="DeepLearning-ai"><a href="#DeepLearning-ai" class="headerlink" title="DeepLearning.ai"></a>DeepLearning.ai</h3><h2 id="阅读随笔"><a href="#阅读随笔" class="headerlink" title="阅读随笔"></a>阅读随笔</h2><h3 id="深度学习设计理念"><a href="#深度学习设计理念" class="headerlink" title="深度学习设计理念"></a>深度学习设计理念</h3><h2 id="语言学习"><a href="#语言学习" class="headerlink" title="语言学习"></a>语言学习</h2><h3 id="GRE"><a href="#GRE" class="headerlink" title="GRE"></a>GRE</h3><h4 id="GRE阅读直播讲义"><a href="#GRE阅读直播讲义" class="headerlink" title="GRE阅读直播讲义"></a>GRE阅读直播讲义</h4><h4 id="GRE阅读作业任务"><a href="#GRE阅读作业任务" class="headerlink" title="GRE阅读作业任务"></a>GRE阅读作业任务</h4><h4 id="GRE填空直播录播合集"><a href="#GRE填空直播录播合集" class="headerlink" title="GRE填空直播录播合集"></a>GRE填空直播录播合集</h4><p>####　GRE填空单词</p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> handwriting_note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python常用模块</title>
      <link href="/2017/06/10/Python-module/"/>
      <url>/2017/06/10/Python-module/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="python-logging模块"><a href="#python-logging模块" class="headerlink" title="python logging模块"></a>python logging模块</h2><blockquote><p>  Python引入了logging模块来记录用户想要查看的信息。</p><h3 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h3><p>当我们执行如下代码：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging  <span class="comment"># 引入logging模块</span></span><br><span class="line"><span class="comment"># 将信息打印到控制台上</span></span><br><span class="line">logging.debug(<span class="string">u&quot;A&quot;</span>)</span><br><span class="line">logging.info(<span class="string">u&quot;B&quot;</span>)</span><br><span class="line">logging.warning(<span class="string">u&quot;C&quot;</span>)</span><br><span class="line">logging.error(<span class="string">u&quot;D&quot;</span>)</span><br><span class="line">logging.critical(<span class="string">u&quot;E&quot;</span>)</span><br></pre></td></tr></table></figure><br>控制台的输出结果为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARNING:root:C</span><br><span class="line">ERROR:root:D</span><br><span class="line">CRITICAL:root:E</span><br></pre></td></tr></table></figure><br>我们会发现控制台只输出了后三个日志，这是因为<strong>默认生成的root logger的level是logging.WARNING,低于该级别的就不输出了</strong>。因此在代码最上方需要<strong>改变日志级别为NOTSET</strong>：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging <span class="comment"># 引入logging模块</span></span><br><span class="line">logging.basciConfig(level=logging.NOTSET) <span class="comment"># 设置日志级别</span></span><br><span class="line">logging.debug(<span class="string">u&quot;如果设置了日志级别为NOTSET,那么这里可以采取debug、info的级别的内容也可以显示在控制台上了&quot;</span>)</span><br></pre></td></tr></table></figure></p><h3 id="控制台日志输出"><a href="#控制台日志输出" class="headerlink" title="控制台日志输出"></a>控制台日志输出</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging  <span class="comment"># 引入logging模块</span></span><br><span class="line">logging.basicConfig(level=logging.DEBUG,<span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;</span>)  <span class="comment"># logging.basicConfig函数对日志的输出格式及方式做相关配置</span></span><br><span class="line"><span class="comment"># 由于日志基本配置中级别设置为DEBUG，所以一下打印信息将会全部显示在控制台上</span></span><br><span class="line">logging.info(<span class="string">&#x27;this is a loggging info message&#x27;</span>)</span><br><span class="line">logging.debug(<span class="string">&#x27;this is a loggging debug message&#x27;</span>)</span><br><span class="line">logging.warning(<span class="string">&#x27;this is loggging a warning message&#x27;</span>)</span><br><span class="line">logging.error(<span class="string">&#x27;this is an loggging error message&#x27;</span>)</span><br><span class="line">logging.critical(<span class="string">&#x27;this is a loggging critical message&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/python_model_1.png" alt=""></p></blockquote><h3 id="日志文件输出"><a href="#日志文件输出" class="headerlink" title="日志文件输出"></a>日志文件输出</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging  <span class="comment"># 引入logging模块</span></span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 第一步，创建一个logger</span></span><br><span class="line">logger = logging.getLogger()</span><br><span class="line">logger.setLevel(logging.INFO)  <span class="comment"># Log等级总开关</span></span><br><span class="line"><span class="comment"># 第二步，创建一个handler，用于写入日志文件</span></span><br><span class="line">rq = time.strftime(<span class="string">&#x27;%Y%m%d%H%M&#x27;</span>, time.localtime(time.time()))</span><br><span class="line">log_path = os.path.dirname(os.getcwd()) + <span class="string">&#x27;/Logs/&#x27;</span></span><br><span class="line">log_name = log_path + rq + <span class="string">&#x27;.log&#x27;</span></span><br><span class="line">logfile = log_name</span><br><span class="line">fh = logging.FileHandler(logfile, mode=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">fh.setLevel(logging.DEBUG)  <span class="comment"># 输出到file的log等级的开关</span></span><br><span class="line"><span class="comment"># 第三步，定义handler的输出格式</span></span><br><span class="line">formatter = logging.Formatter(<span class="string">&quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&quot;</span>)</span><br><span class="line">fh.setFormatter(formatter)</span><br><span class="line"><span class="comment"># 第四步，将logger添加到handler里面</span></span><br><span class="line">logger.addHandler(fh)</span><br><span class="line"><span class="comment"># 日志</span></span><br><span class="line">logger.debug(<span class="string">&#x27;this is a logger debug message&#x27;</span>)</span><br><span class="line">logger.info(<span class="string">&#x27;this is a logger info message&#x27;</span>)</span><br><span class="line">logger.warning(<span class="string">&#x27;this is a logger warning message&#x27;</span>)</span><br><span class="line">logger.error(<span class="string">&#x27;this is a logger error message&#x27;</span>)</span><br><span class="line">logger.critical(<span class="string">&#x27;this is a logger critical message&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="python单元测试框架-unittest"><a href="#python单元测试框架-unittest" class="headerlink" title="python单元测试框架-unittest"></a>python单元测试框架-unittest</h2><h3 id="unittest最核心的原理"><a href="#unittest最核心的原理" class="headerlink" title="unittest最核心的原理"></a>unittest最核心的原理</h3><ul><li>TestCase:测试用例<ul><li>setUP:测试前准备环境的搭建</li><li>run:执行测试代码</li><li>tearDown:测试后环境的还原</li></ul></li><li>TestSuite：多个测试用例的集合</li><li>TestLoader：加载TestCase到TestSuite中</li><li>TextTestRunner：执行测试用例，并且把结果保存在TextTestResult之中</li><li>test fixture：对测试用例环境的搭建和销毁</li></ul><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> unnittest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下定义四个函数进行单元测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minus</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a-b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multi</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a*b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">divide</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a/b</span><br><span class="line"><span class="comment"># 是时候展现真正的(unittest)技术了:单元测试主体部分</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestMathFunc</span>(unittest.TestCase):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Test mathfuc.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_add</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Test method add(a, b)&quot;&quot;&quot;</span></span><br><span class="line">        self.assertEqual(<span class="number">3</span>, add(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        self.assertNotEqual(<span class="number">3</span>, add(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_minus</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Test method minus(a, b)&quot;&quot;&quot;</span></span><br><span class="line">        self.assertEqual(<span class="number">1</span>, minus(<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_multi</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Test method multi(a, b)&quot;&quot;&quot;</span></span><br><span class="line">        self.assertEqual(<span class="number">6</span>, multi(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_divide</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Test method divide(a, b)&quot;&quot;&quot;</span></span><br><span class="line">        self.assertEqual(<span class="number">2</span>, divide(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">        self.assertEqual(<span class="number">2.5</span>, divide(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    unittest.main()</span><br><span class="line"><span class="string">&quot;&quot;&quot;执行结果为：</span></span><br><span class="line"><span class="string">.F..</span></span><br><span class="line"><span class="string">======================================================================</span></span><br><span class="line"><span class="string">FAIL: test_divide (__main__.TestMathFunc)</span></span><br><span class="line"><span class="string">Test method divide(a, b)</span></span><br><span class="line"><span class="string">----------------------------------------------------------------------</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;D:/py/test_mathfunc.py&quot;, line 26, in test_divide</span></span><br><span class="line"><span class="string">    self.assertEqual(2.5, divide(5, 2))</span></span><br><span class="line"><span class="string">AssertionError: 2.5 != 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----------------------------------------------------------------------</span></span><br><span class="line"><span class="string">Ran 4 tests in 0.000s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">FAILED (failures=1)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>在第一行给出了每一个用例执行的结果的标识，成功是 .，失败是 F，出错是 E，跳过是 S。从上面也可以看出，测试的执行跟方法的顺序没有关系，test_divide写在了第4个，但是却是第2个执行的。</li><li>每个测试方法均以 test 开头，否则是不被unittest识别的。</li></ul><h3 id="进阶：TestSuite-组织测试"><a href="#进阶：TestSuite-组织测试" class="headerlink" title="进阶：TestSuite(组织测试)"></a>进阶：TestSuite(组织测试)</h3><blockquote><p>将程序按照添加的顺序来执行。此处只需针对上文的main函数进行改进</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    suite = unittest.TestSuite()</span><br><span class="line"></span><br><span class="line">    tests = [TestMathFunc(<span class="string">&quot;test_add&quot;</span>), TestMathFunc(<span class="string">&quot;test_minus&quot;</span>), TestMathFunc(<span class="string">&quot;test_divide&quot;</span>)]</span><br><span class="line">    suite.addTests(tests)</span><br><span class="line"></span><br><span class="line">    runner = unittest.TextTestRunner(verbosity=<span class="number">2</span>)</span><br><span class="line">    runner.run(suite)</span><br></pre></td></tr></table></figure><h3 id="进阶：准备测试环境"><a href="#进阶：准备测试环境" class="headerlink" title="进阶：准备测试环境"></a>进阶：准备测试环境</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TestMathFunc</span>(unittest.TestCase):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Test mathfuc.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setUp</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;do something before test.Prepare environment.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tearDown</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;do something after test.Clean up.&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setUpClass</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;This setUpClass() method only called once.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tearDownClass</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;This tearDownClass() method only called once too.&quot;</span></span><br></pre></td></tr></table></figure><ul><li>这两个方法在每个测试方法执行前以及执行后执行一次，<code>setUp</code>用来为测试准备环境，<code>tearDown</code>用来清理环境，已备之后的测试。</li><li>如果想要在所有case执行之前准备一次环境，并在所有case执行结束之后再清理环境，我们可以用 <code>setUpClass()</code>与<code>tearDownClass()</code></li></ul><h2 id="os模块与sys的模块"><a href="#os模块与sys的模块" class="headerlink" title="os模块与sys的模块"></a>os模块与sys的模块</h2><h3 id="os模块与sys模块的区分"><a href="#os模块与sys模块的区分" class="headerlink" title="os模块与sys模块的区分"></a>os模块与sys模块的区分</h3><blockquote><p>os模块：提供了一种方便使用操作系统函数的方法</p><p>sys模块：提供访问由解释器使用或维护的变量和在与解释器交互使用到的函数 </p></blockquote><h3 id="os模块常用方法"><a href="#os模块常用方法" class="headerlink" title="os模块常用方法"></a>os模块常用方法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">os.remove()删除文件  </span><br><span class="line">os.rename()重命名文件  </span><br><span class="line">os.walk()生成目录树下的所有文件名  </span><br><span class="line">os.chdir()改变目录  </span><br><span class="line">os.mkdir/makedirs创建目录/多层目录  </span><br><span class="line">os.rmdir/removedirs删除目录/多层目录  </span><br><span class="line">os.listdir()列出指定目录的文件  </span><br><span class="line">os.getcwd()取得当前工作目录  </span><br><span class="line">os.chmod()改变目录权限  </span><br><span class="line">os.path.basename()去掉目录路径，返回文件名  </span><br><span class="line">os.path.dirname()去掉文件名，返回目录路径  </span><br><span class="line">os.path.join()将分离的各部分组合成一个路径名  </span><br><span class="line">os.path.split()返回（dirname(),basename())元组  </span><br><span class="line">os.path.splitext()(返回filename,extension)元组  </span><br><span class="line">os.path.getatime\ctime\mtime分别返回最近访问、创建、修改时间  </span><br><span class="line">os.path.getsize()返回文件大小  </span><br><span class="line">os.path.exists()是否存在  </span><br><span class="line">os.path.isabs()是否为绝对路径  </span><br><span class="line">os.path.isdir()是否为目录  </span><br><span class="line">os.path.isfile()是否为文件 </span><br></pre></td></tr></table></figure><h2 id="sys模块常用方法："><a href="#sys模块常用方法：" class="headerlink" title="sys模块常用方法："></a>sys模块常用方法：</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sys.argv           命令行参数<span class="type">List</span>，第一个元素是程序本身路径   </span><br><span class="line">sys.modules.keys() 返回所有已经导入的模块列表   </span><br><span class="line">sys.exc_info()     获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息   </span><br><span class="line">sys.exit(n)        退出程序，正常退出时exit(<span class="number">0</span>)   </span><br><span class="line">sys.hexversion     获取Python解释程序的版本值，<span class="number">16</span>进制格式如：<span class="number">0x020403F0</span>   </span><br><span class="line">sys.version        获取Python解释程序的版本信息   </span><br><span class="line">sys.maxint         最大的Int值   </span><br><span class="line">sys.maxunicode     最大的Unicode值   </span><br><span class="line">sys.modules        返回系统导入的模块字段，key是模块名，value是模块   </span><br><span class="line">sys.path           返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值   </span><br><span class="line">sys.platform       返回操作系统平台名称   </span><br><span class="line">sys.stdout         标准输出  </span><br><span class="line">sys.stdin          标准输入  </span><br><span class="line">sys.stderr         错误输出  </span><br><span class="line">sys.exc_clear()    用来清除当前线程所出现的当前的或最近的错误信息  </span><br><span class="line">sys.exec_prefix    返回平台独立的python文件安装的位置  </span><br><span class="line">sys.byteorder      本地字节规则的指示器，big-endian平台的值是<span class="string">&#x27;big&#x27;</span>,little-endian平台的值是<span class="string">&#x27;little&#x27;</span>  </span><br><span class="line">sys.copyright      记录python版权相关的东西  </span><br><span class="line">sys.api_version    解释器的C的API版本  </span><br><span class="line">sys.version_info    </span><br></pre></td></tr></table></figure><h2 id="关于时间的模块"><a href="#关于时间的模块" class="headerlink" title="关于时间的模块"></a>关于时间的模块</h2><h3 id="datetime模块获取当前时间"><a href="#datetime模块获取当前时间" class="headerlink" title="datetime模块获取当前时间"></a>datetime模块获取当前时间</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">nowTime = datatime.datatime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>) <span class="comment"># 现在</span></span><br></pre></td></tr></table></figure><h2 id="retry模块"><a href="#retry模块" class="headerlink" title="retry模块"></a>retry模块</h2><p>使用场景通常是在网络情况不太稳定时，预防超时异常<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"></span><br><span class="line"><span class="meta">@retry</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">do_something_unreliable</span>():</span><br><span class="line">    <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">10</span>) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> IOError(<span class="string">&quot;Broken sauce, everything is hosed!!!111one&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Awesome sauce!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> do_something_unreliable()</span><br></pre></td></tr></table></figure><br>以上是官方文档的代码：给函数添加@retry装饰器后，只要有异常，那么函数会不断地重试，直到有返回值。<br>下面是几个常常传入装饰器的参数<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最多尝试7次，这个最常用！</span></span><br><span class="line"><span class="meta">@retry(<span class="params">stop_max_attempt_number=<span class="number">7</span></span>)</span></span><br><span class="line"><span class="comment"># 最长等待10秒</span></span><br><span class="line"><span class="meta">@retry(<span class="params">stop_max_delay=<span class="number">10000</span></span>)</span></span><br></pre></td></tr></table></figure></p><h2 id="timeit"><a href="#timeit" class="headerlink" title="timeit"></a>timeit</h2><p>专门用来测试代码的运行时间，运行十分方便：<br>    实例化<code>Timer</code>类需要两个参数，第一个参数为你需要计算运行时间的函数，类型为字符串；第二个参数为构建环境的导入语句，类型也为字符串。</p><h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用来计算运行时间的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">        n += i</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line"><span class="keyword">from</span> timeit <span class="keyword">import</span> Timer</span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">t = Timer(<span class="string">&quot;test1()&quot;</span>, <span class="string">&quot;from __main__ import test1&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> t.timeit()</span><br><span class="line"><span class="built_in">print</span> t.repeat(<span class="number">3</span>, <span class="number">10000</span>) <span class="comment"># 执行测试三次</span></span><br></pre></td></tr></table></figure><h2 id="glob"><a href="#glob" class="headerlink" title="glob"></a>glob</h2><p>glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，支持<strong>通配符操作</strong>: <code></code> <code>?</code> <code>[]]</code>这三个通配符，<code></code>代表0个或多个字符，<code>?</code>代表一个字符，<code>[]</code>匹配指定范围内的字符，如[0-9]匹配数字。主要方法如下.</p><h3 id="glob方法：返回所有匹配的文件路径列表"><a href="#glob方法：返回所有匹配的文件路径列表" class="headerlink" title="glob方法：返回所有匹配的文件路径列表"></a>glob方法：返回所有匹配的文件路径列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">glob.glob(<span class="string">r&#x27;C:*.txt&#x27;</span>) <span class="comment"># 获取C盘下的所有txt文件</span></span><br><span class="line">glog.glob(<span class="string">r&#x27;../*.py&#x27;</span>) <span class="comment"># 获取相对路径下的python文件</span></span><br></pre></td></tr></table></figure><h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><p>PIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。</p><h3 id="诡异的安装方式"><a href="#诡异的安装方式" class="headerlink" title="诡异的安装方式"></a>诡异的安装方式</h3><p><code>pip install pillow</code></p><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;path/to/Image.jpg&#x27;</span>) <span class="comment"># 打开图像文件</span></span><br><span class="line">w, h = im.size <span class="comment"># 获取图片尺寸</span></span><br><span class="line">im.thumbnail((w//<span class="number">2</span>, h//<span class="number">2</span>)) <span class="comment"># 缩放图片到50%</span></span><br><span class="line">im.save(<span class="string">&#x27;path/to/Image.jpg&#x27;</span>, <span class="string">&#x27;jpeg&#x27;</span>) <span class="comment"># 保存图片格式为jpeg</span></span><br></pre></td></tr></table></figure><h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><h3 id="在远端服务器利用matplotlib绘图"><a href="#在远端服务器利用matplotlib绘图" class="headerlink" title="在远端服务器利用matplotlib绘图"></a>在远端服务器利用matplotlib绘图</h3><p>在<code>import matplotlib.pyplot</code><strong>之前</strong>加上:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.use(<span class="string">&#x27;Agg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>在<code>plt.draw</code><strong>之后</strong>加上:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.savefig(<span class="string">&quot;/path/to/pic.jpg&quot;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="setuptool"><a href="#setuptool" class="headerlink" title="setuptool"></a>setuptool</h2><h3 id="basic-usage"><a href="#basic-usage" class="headerlink" title="basic usage"></a>basic usage</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;MyApp&#x27;</span>,         <span class="comment"># 应用名</span></span><br><span class="line">    version=<span class="string">&#x27;1.0&#x27;</span>,        <span class="comment"># 版本号</span></span><br><span class="line">    packages=[<span class="string">&#x27;myapp&#x27;</span>]    <span class="comment"># 包括在安装包内的Python包</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="pydot-amp-graphviz"><a href="#pydot-amp-graphviz" class="headerlink" title="pydot &amp; graphviz"></a>pydot &amp; graphviz</h2><p>两个包为keras的可视化模块。可以可视化神经网络结构</p><h2 id="easydict"><a href="#easydict" class="headerlink" title="easydict"></a>easydict</h2><blockquote><p>引用与参考资料</p><ol><li>logging:<a href="https://www.cnblogs.com/CJOKER/p/8295272.html">https://www.cnblogs.com/CJOKER/p/8295272.html</a></li><li>unittest:<a href="https://blog.csdn.net/huilan_same/article/details/52944782#t4">https://blog.csdn.net/huilan_same/article/details/52944782#t4</a></li><li><a href="https://my.oschina.net/liuyuantao/blog/755907">https://my.oschina.net/liuyuantao/blog/755907</a></li><li><a href="https://blog.csdn.net/u010472607/article/details/76857493/">https://blog.csdn.net/u010472607/article/details/76857493/</a></li><li><a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/00140767171357714f87a053a824ffd811d98a83b58ec13000">https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/00140767171357714f87a053a824ffd811d98a83b58ec13000</a></li><li><a href="https://blog.csdn.net/i_is_a_energy_man/article/details/77833040">https://blog.csdn.net/i_is_a_energy_man/article/details/77833040</a></li><li><a href="https://blog.csdn.net/zzytmxk/article/details/53402257">https://blog.csdn.net/zzytmxk/article/details/53402257</a></li><li><a href="http://python.jobbole.com/87240/">http://python.jobbole.com/87240/</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logging </tag>
            
            <tag> unittest </tag>
            
            <tag> datetime </tag>
            
            <tag> retry </tag>
            
            <tag> os </tag>
            
            <tag> timeit </tag>
            
            <tag> glob </tag>
            
            <tag> PIL </tag>
            
            <tag> pickle </tag>
            
            <tag> matplotlib </tag>
            
            <tag> numpy </tag>
            
            <tag> setuptool </tag>
            
            <tag> graphviz </tag>
            
            <tag> pydot </tag>
            
            <tag> easydict </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git使用备忘录</title>
      <link href="/2017/04/16/Git-usage/"/>
      <url>/2017/04/16/Git-usage/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="指令速查"><a href="#指令速查" class="headerlink" title="指令速查"></a>指令速查</h2><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/git_command_1.png" alt=""></p><h2 id="雷区勿踩"><a href="#雷区勿踩" class="headerlink" title="雷区勿踩"></a>雷区勿踩</h2><p><img src="https://raw.githubusercontent.com/824zzy/blogResources/master/picResources/git_command_2.png" alt=""></p><hr><h2 id="常见错误和小技巧"><a href="#常见错误和小技巧" class="headerlink" title="常见错误和小技巧"></a>常见错误和小技巧</h2><h3 id="git-pull失败-提示：fatal-refusing-to-merge-unrelated-histories"><a href="#git-pull失败-提示：fatal-refusing-to-merge-unrelated-histories" class="headerlink" title="git pull失败,提示：fatal: refusing to merge unrelated histories"></a>git pull失败,提示：fatal: refusing to merge unrelated histories</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">之后再</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h3 id="git-add-添加文件夹而非全部修改文件"><a href="#git-add-添加文件夹而非全部修改文件" class="headerlink" title="git add 添加文件夹而非全部修改文件"></a>git add 添加文件夹而非全部修改文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加整个文件夹及内容</span></span><br><span class="line">git add yourfile/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加目录中所有此文件类型的文件</span></span><br><span class="line">git add *.your_文件类型</span><br></pre></td></tr></table></figure><h3 id="与fork别人的仓库保持同步"><a href="#与fork别人的仓库保持同步" class="headerlink" title="与fork别人的仓库保持同步"></a>与fork别人的仓库保持同步</h3><ol><li>将fork的项目添加到remote中<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add your_forked_project_name(whatever you like) https://your_forked_project_url.git</span><br></pre></td></tr></table></figure></li><li>更新remote中所有远程repo<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch --all</span><br></pre></td></tr></table></figure></li><li>进行同步<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rebase your_forked_project_name/master</span><br></pre></td></tr></table></figure></li><li>将自己的远程仓库也同步<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure></li></ol><h3 id="处理-github-不允许上传超过-100MB-文件的问题"><a href="#处理-github-不允许上传超过-100MB-文件的问题" class="headerlink" title="处理 github 不允许上传超过 100MB 文件的问题"></a>处理 github 不允许上传超过 100MB 文件的问题</h3><h4 id="移除错误缓存"><a href="#移除错误缓存" class="headerlink" title="移除错误缓存"></a>移除错误缓存</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for file</span></span><br><span class="line">git rm --cached path_of_a_giant_file</span><br><span class="line"><span class="comment"># for document</span></span><br><span class="line">git rm --cached -r path_of_a_giant_file</span><br></pre></td></tr></table></figure><h4 id="重新提交后将大文件加入-Git-Large-File-Stroage"><a href="#重新提交后将大文件加入-Git-Large-File-Stroage" class="headerlink" title="重新提交后将大文件加入 Git Large File Stroage:"></a>重新提交后将大文件加入 Git Large File Stroage:</h4><ol><li>安装git-lfs: <code>brew install git-lfs</code></li><li>在repository的根目录初始化：<code>git lfs install</code></li><li>跟踪<strong>大文件名</strong>： <code>git lfs track &quot;name_of_your_giant_file_not_your_path!&quot;</code></li><li>正常提交推送：<code>git add your_giant_file_path</code></li><li>搜索大文件：<code>find ./ -size +100M</code></li></ol><h3 id="git-commit后想要撤销到上一个commit"><a href="#git-commit后想要撤销到上一个commit" class="headerlink" title="git commit后想要撤销到上一个commit"></a>git commit后想要撤销到上一个commit</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></figure><h3 id="git-push-提交代码时writing-objects特别慢解决方案"><a href="#git-push-提交代码时writing-objects特别慢解决方案" class="headerlink" title="git push 提交代码时writing objects特别慢解决方案"></a>git push 提交代码时writing objects特别慢解决方案</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure><blockquote><p>引用参考资料和版权说明</p><ol><li><a href="http://www.git-tower.com/blog/git-cheat-sheet-cn">http://www.git-tower.com/blog/git-cheat-sheet-cn</a></li><li><a href="https://blog.csdn.net/wxs0124/article/details/50126953">https://blog.csdn.net/wxs0124/article/details/50126953</a></li><li><a href="http://stackoverflow.com/questions/37937984/git-refusing-to-merge-unrelated-histories">http://stackoverflow.com/questions/37937984/git-refusing-to-merge-unrelated-histories</a></li><li><a href="https://blog.csdn.net/xinqingwuji/article/details/79391453">https://blog.csdn.net/xinqingwuji/article/details/79391453</a></li><li><a href="https://www.cnblogs.com/-walker/p/7278951.html">https://www.cnblogs.com/-walker/p/7278951.html</a></li><li><a href="https://blog.csdn.net/smart_graphics/article/details/78475735">https://blog.csdn.net/smart_graphics/article/details/78475735</a></li><li><a href="http://www.liuxiao.org/2017/02/git-%E5%A4%84%E7%90%86-github-%E4%B8%8D%E5%85%81%E8%AE%B8%E4%B8%8A%E4%BC%A0%E8%B6%85%E8%BF%87-100mb-%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98/">http://www.liuxiao.org/2017/02/git-%E5%A4%84%E7%90%86-github-%E4%B8%8D%E5%85%81%E8%AE%B8%E4%B8%8A%E4%BC%A0%E8%B6%85%E8%BF%87-100mb-%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98/</a></li><li><a href="https://blog.csdn.net/w958796636/article/details/53611133">https://blog.csdn.net/w958796636/article/details/53611133</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2017/04/03/Linux-command/"/>
      <url>/2017/04/03/Linux-command/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Linux基本命令"><a href="#Linux基本命令" class="headerlink" title="Linux基本命令"></a>Linux基本命令</h2><h3 id="cd命令"><a href="#cd命令" class="headerlink" title="cd命令"></a>cd命令</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据绝对路径，切换到目录/root/Docements  </span></span><br><span class="line">cd /root/Docements </span><br><span class="line"><span class="comment"># 根据相对路径，切换到当前目录下的path目录中，“.”表示当前目录 </span></span><br><span class="line">cd ./path</span><br><span class="line"><span class="comment"># 返回上一级目录</span></span><br><span class="line">cd ..</span><br></pre></td></tr></table></figure><h3 id="ls命令"><a href="#ls命令" class="headerlink" title="ls命令"></a>ls命令</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看文件与目录的命令</span></span><br><span class="line">ls</span><br><span class="line"><span class="comment"># 列出全部的文件，连同隐藏文件（开头为.的文件）一起列出来</span></span><br><span class="line">ls -a</span><br><span class="line"><span class="comment"># 列出全部的文件，包含文件的属性与权限数据</span></span><br><span class="line">ls -l</span><br></pre></td></tr></table></figure><h3 id="grep命令"><a href="#grep命令" class="headerlink" title="grep命令"></a>grep命令</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于搜索过滤文件中的数据</span></span><br><span class="line">grep [-aciv] [--color=auto] <span class="string">&#x27;待查找字符串&#x27;</span> filename</span><br></pre></td></tr></table></figure><h4 id="常用参数解释："><a href="#常用参数解释：" class="headerlink" title="常用参数解释："></a>常用参数解释：</h4><ul><li>-a ：将binary文件以text文件的方式查找数据  </li><li>-c ：计算找到‘查找字符串’的次数  </li><li>-i ：忽略大小写的区别，即把大小写视为相同  </li><li>-v ：反向选择，即显示出没有‘查找字符串’内容的那一行  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如：  </span></span><br><span class="line"><span class="comment"># 取出文件/etc/man.config中包含MANPATH的行，并把找到的关键字加上颜色  </span></span><br><span class="line">grep --color=auto <span class="string">&#x27;MANPATH&#x27;</span> /etc/man.config  </span><br><span class="line"><span class="comment"># 把ls -l的输出中包含字母file（不区分大小写）的内容输出  </span></span><br><span class="line">ls -l | grep -i file  </span><br></pre></td></tr></table></figure></li></ul><h3 id="cp命令"><a href="#cp命令" class="headerlink" title="cp命令"></a>cp命令</h3><blockquote><p>复制文件命令，常用参数如下</p><ul><li>-a ：将文件的特性一起复制  </li><li>-p ：连同文件的属性一起复制，而非使用默认方式，与-a相似，常用于备份  </li><li>-i ：若目标文件已经存在时，在覆盖时会先询问操作的进行  </li><li>-r ：递归持续复制，用于目录的复制行为  </li><li>-u ：目标文件与源文件有差异时才会复制 </li></ul></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp -a file1 file2 <span class="comment">#连同文件的所有特性把文件file1复制成文件file2  </span></span><br><span class="line">cp -r file1 file2 file3 <span class="built_in">dir</span> <span class="comment">#把文件file1、file2、file3递归复制到目录dir中 </span></span><br></pre></td></tr></table></figure><h3 id="mv命令"><a href="#mv命令" class="headerlink" title="mv命令"></a>mv命令</h3><blockquote><p>重命名文件使用方式<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv file1 file2 <span class="comment"># 把文件file1重命名为file2  </span></span><br></pre></td></tr></table></figure><br>把一个文件或者多个文件一次移动到一个文件夹中（最后一个目标文件一定是目录）<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv file1 file2 file3 <span class="built_in">dir</span> <span class="comment"># 把文件file1、file2、file3移动到目录dir中  </span></span><br></pre></td></tr></table></figure></p></blockquote><h3 id="rm命令（小心使用！！！！！！）"><a href="#rm命令（小心使用！！！！！！）" class="headerlink" title="rm命令（小心使用！！！！！！）"></a>rm命令（小心使用！！！！！！）</h3><blockquote><p>删除文件或者目录，常用参数</p><ul><li>-f ：就是force的意思，忽略不存在的文件，不会出现警告消息  </li><li>-i ：互动模式，在删除前会询问用户是否操作  </li><li>-r ：递归删除，最常用于目录删除，它是一个非常危险的参数  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -i file <span class="comment"># 删除文件file，在删除之前会询问是否进行该操作  </span></span><br><span class="line">rm -fr <span class="built_in">dir</span> <span class="comment"># 强制删除目录dir中的所有文件 </span></span><br></pre></td></tr></table></figure></li></ul></blockquote><h3 id="ps命令"><a href="#ps命令" class="headerlink" title="ps命令"></a>ps命令</h3><blockquote><p>查看进行巡行情况，该命令与其他命令的常用搭配如下<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ps aux <span class="comment"># 查看系统所有的进程数据  </span></span><br><span class="line">ps ax <span class="comment"># 查看不与terminal有关的所有进程  </span></span><br><span class="line">ps -lA <span class="comment"># 查看系统所有的进程数据  </span></span><br><span class="line">ps axjf <span class="comment"># 查看连同一部分进程树状态  </span></span><br><span class="line">ps aux| grep -ic python <span class="comment">#　查看系统中所有关于python的进程</span></span><br></pre></td></tr></table></figure></p></blockquote><h3 id="kill命令"><a href="#kill命令" class="headerlink" title="kill命令"></a>kill命令</h3><blockquote><p>对进程发送信号，常用signal如下（使用时可以用代号代替相应的信号）</p><ul><li>1：SIGHUP，启动被终止的进程  </li><li>2：SIGINT，相当于输入ctrl+c，中断一个程序的进行  </li><li>9：SIGKILL，强制中断一个进程的进行  </li><li>15：SIGTERM，以正常的结束进程方式来终止进程  </li><li>17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kill -signal PID</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程 </span></span><br><span class="line">kill -SIGTERM %<span class="number">1</span>   </span><br><span class="line"><span class="comment"># 重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得  </span></span><br><span class="line">kill -SIGHUP PID</span><br></pre></td></tr></table></figure></li></ul></blockquote><h3 id="tar命令"><a href="#tar命令" class="headerlink" title="tar命令"></a>tar命令</h3><blockquote><p>对文件进行打包默认情况并不会压缩，如果指定了相应的参数，它还会调用相应的压缩程序（如gzip和bzip等）进行压缩和解压。它的常用参数如下：</p><ul><li>-c ：新建打包文件  </li><li>-t ：查看打包文件的内容含有哪些文件名  </li><li>-x ：解打包或解压缩的功能，可以搭配-C（大写）指定解压的目录，注意-c,-t,-x不能同时出现在同一条命令中  </li><li>-j ：通过bzip2的支持进行压缩/解压缩  </li><li>-z ：通过gzip的支持进行压缩/解压缩  </li><li>-v ：在压缩/解压缩过程中，将正在处理的文件名显示出来  </li><li>-f filename ：filename为要处理的文件  </li><li>-C dir ：指定压缩/解压缩的目录dir <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通常我们只需要使用如下三条命令：</span></span><br><span class="line">压缩：tar -jcv -f filename 要被处理的文件或目录名称  </span><br><span class="line">查询：tar -jtv -f filename</span><br><span class="line">解压：tar -jxv -f filename -C 欲解压缩的目录</span><br></pre></td></tr></table></figure></li></ul></blockquote><h3 id="cat命令"><a href="#cat命令" class="headerlink" title="cat命令"></a>cat命令</h3><blockquote><p>查看文本文件内容<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat test|less <span class="comment"># 查看text文件中的内容</span></span><br></pre></td></tr></table></figure></p></blockquote><h3 id="chgrp-chown命令"><a href="#chgrp-chown命令" class="headerlink" title="chgrp/chown命令"></a>chgrp/chown命令</h3><blockquote><p>改变用户所述用户组/改变文件的所有者<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chgrp/chown users -R ./<span class="built_in">dir</span> <span class="comment"># 递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users </span></span><br></pre></td></tr></table></figure></p></blockquote><h3 id="chmod命令"><a href="#chmod命令" class="headerlink" title="chmod命令"></a>chmod命令</h3><blockquote><p>改变文件的权限<br>常用权限表示形式:</p><ul><li>-rw——- (600)      只有拥有者有读写权限。</li><li>-rw-r–r– (644)      只有拥有者有读写权限；而属组用户和其他用户只有读权限。</li><li>-rwx—— (700)     只有拥有者有读、写、执行权限。</li><li>-rwxr-xr-x (755)    拥有者有读、写、执行权限；而属组用户和其他用户只有读、执行权限。</li><li>-rwx–x–x (711)    拥有者有读、写、执行权限；而属组用户和其他用户只有执行权限。</li><li>-rw-rw-rw- (666)   所有用户都有文件读、写权限。</li><li>-rwxrwxrwx (777)  所有用户都有读、写、执行权限。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R <span class="number">777</span> file <span class="comment">#　将file设置为最高权限</span></span><br></pre></td></tr></table></figure><h3 id="查看系统的配置"><a href="#查看系统的配置" class="headerlink" title="查看系统的配置"></a>查看系统的配置</h3>打开/proc目录查看系统硬件配置<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看cpu信息</span></span><br><span class="line">cat /proc/cpuinfo</span><br><span class="line"><span class="comment">// 查看内存</span></span><br><span class="line"><span class="built_in">free</span> -m</span><br><span class="line"><span class="comment">// 查看硬盘空间</span></span><br><span class="line">df -h</span><br></pre></td></tr></table></figure></li></ul></blockquote><h2 id="Linux更高层命令"><a href="#Linux更高层命令" class="headerlink" title="Linux更高层命令"></a>Linux更高层命令</h2><h3 id="curl命令"><a href="#curl命令" class="headerlink" title="curl命令"></a>curl命令</h3><blockquote><p>http命令行工具，支持文件的上传与下载。</p></blockquote><h4 id="其基本语法为："><a href="#其基本语法为：" class="headerlink" title="其基本语法为："></a>其基本语法为：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl [option] [url]</span><br></pre></td></tr></table></figure><h4 id="其常用使用方法为："><a href="#其常用使用方法为：" class="headerlink" title="其常用使用方法为："></a>其常用使用方法为：</h4><ol><li>测试<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://www.linux.com // 测试服务器与网站的连通性</span><br></pre></td></tr></table></figure></li><li>下载<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o filename http_url // 下载url的文件并保存名为filename</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -O -u user_name:password ftp_url // 根据user_name和password下载ftp_url的文件</span><br></pre></td></tr></table></figure></li><li>上传文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -T upload_file -u user_name:password ftp_url // 根据user_name和password上传file到url</span><br></pre></td></tr></table></figure></li><li>查看公网ip<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl ifconfig.me 或者 curl cip.cc</span><br></pre></td></tr></table></figure></li></ol><h3 id="查看当前GPU使用情况"><a href="#查看当前GPU使用情况" class="headerlink" title="查看当前GPU使用情况"></a>查看当前GPU使用情况</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvdia-smi</span><br></pre></td></tr></table></figure><h4 id="使用指定GPU运行程序"><a href="#使用指定GPU运行程序" class="headerlink" title="使用指定GPU运行程序"></a>使用指定GPU运行程序</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=&#123;&#123;number of interface&#125;&#125; python &#123;&#123;file&#125;&#125;.py</span><br><span class="line"> ```   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 服务器间文件的传输（scp命令）</span></span><br><span class="line">&gt; 服务器其实就是电脑的意思，两个可以等价替换；&#123;&#123;&#125;&#125;代表你需要修改的参数</span><br><span class="line"><span class="comment">#### 复制远程服务器的文件到本地服务器</span></span><br><span class="line">```py</span><br><span class="line">scp -r &#123;&#123;远程服务器名称&#125;&#125;@&#123;&#123;远程服务器ip&#125;&#125;:&#123;&#123;远程服务器目录&#125;&#125; &#123;&#123;本地服务器目录&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="将本地服务器的文件上传到远端服务器"><a href="#将本地服务器的文件上传到远端服务器" class="headerlink" title="将本地服务器的文件上传到远端服务器"></a>将本地服务器的文件上传到远端服务器</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r &#123;&#123;本地服务器目录&#125;&#125; &#123;&#123;远程服务器名称&#125;&#125;@&#123;&#123;远程服务器ip&#125;&#125;:&#123;&#123;远程服务器目录&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="备注：scp常用命令参数"><a href="#备注：scp常用命令参数" class="headerlink" title="备注：scp常用命令参数"></a>备注：scp常用命令参数</h4><ul><li>-r  递归复制整个目录。</li><li>-P port  注意是大写的P,port是指定数据传输用到的端口号</li></ul><h4 id="scp命令常见问题"><a href="#scp命令常见问题" class="headerlink" title="scp命令常见问题"></a>scp命令常见问题</h4><blockquote><p>WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED解决方法</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.ssh/known_hosts</span><br><span class="line">删除与无法识别的服务器的相关rsa的信息即可.</span><br></pre></td></tr></table></figure><h3 id="ssh命令"><a href="#ssh命令" class="headerlink" title="ssh命令"></a>ssh命令</h3><blockquote><p>远程登录Linux主机</p></blockquote><p>最常用命令<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh -p port user_name@hostname <span class="comment">// 在port端口指定用户登录host</span></span><br><span class="line"></span><br><span class="line">servive sshd restart <span class="comment">// 重启sshd服务</span></span><br><span class="line"></span><br><span class="line">vim /etc/ssh/sshd_config <span class="comment">//修改配置文件的端口号和其他配置</span></span><br></pre></td></tr></table></figure></p><h3 id="apt-get"><a href="#apt-get" class="headerlink" title="apt-get"></a>apt-get</h3><h4 id="安装无sudo权限的软件"><a href="#安装无sudo权限的软件" class="headerlink" title="安装无sudo权限的软件"></a>安装无sudo权限的软件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apt-get source your_package</span><br><span class="line">cd your_package</span><br><span class="line">./configure --prefix=$HOME</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><h3 id="用户相关命令"><a href="#用户相关命令" class="headerlink" title="用户相关命令"></a>用户相关命令</h3><h4 id="在root账户下："><a href="#在root账户下：" class="headerlink" title="在root账户下："></a>在root账户下：</h4><ul><li>useradd user_name 创建用户user_name</li><li>passwd user_name 给用户user_name设置密码</li><li>userdel user_name 删除用户user_name</li><li>su user_name 切换到（<strong>switch user</strong>）用户user_name</li></ul><blockquote><p>参考与引用</p><ol><li><a href="https://blog.csdn.net/ljianhui/article/details/11100625/">https://blog.csdn.net/ljianhui/article/details/11100625/</a></li><li><a href="https://www.cnblogs.com/ksguai/p/6090115.html">https://www.cnblogs.com/ksguai/p/6090115.html</a></li><li><a href="https://blog.csdn.net/kwu_ganymede/article/details/61199067">https://blog.csdn.net/kwu_ganymede/article/details/61199067</a></li><li><a href="http://blog.51cto.com/linuxme/375752">http://blog.51cto.com/linuxme/375752</a></li><li><a href="https://www.linuxidc.com/Linux/2017-06/144916.htm">https://www.linuxidc.com/Linux/2017-06/144916.htm</a></li><li><a href="https://zhidao.baidu.com/question/584292009.html">https://zhidao.baidu.com/question/584292009.html</a></li><li><a href="https://www.cnblogs.com/huangshiyu13/p/5923607.html">https://www.cnblogs.com/huangshiyu13/p/5923607.html</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> command </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用软件</title>
      <link href="/2017/03/01/Linux-softwares/"/>
      <url>/2017/03/01/Linux-softwares/</url>
      
        <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="vim最常用：-常用度从高至低"><a href="#vim最常用：-常用度从高至低" class="headerlink" title="vim最常用： 常用度从高至低"></a>vim最常用： 常用度从高至低</h2><h3 id="A-level"><a href="#A-level" class="headerlink" title="A level"></a>A level</h3><ul><li><strong>i</strong>:insert模式</li><li><strong>:wq</strong>:存盘退出</li><li><strong>yy</strong>:复制一整行</li><li><strong>dd</strong>:删除当前行，并且添加至剪贴板</li><li><strong>p</strong>:粘贴剪贴板</li><li><strong>o</strong>:当前行后插入行</li><li><strong>0</strong>:移动光标到行头</li><li><strong>$</strong>:移动光标到行尾</li><li><strong>gg</strong>:移动光标到第一行</li><li><strong>G</strong>:移动光标到最后一行</li><li><strong>u</strong>: undo</li><li><strong>C-r</strong>: redo</li><li><strong>搜索str1并且替换为str2</strong>：%s/str1/str2/g (这个比较好用)</li></ul><h3 id="B-level"><a href="#B-level" class="headerlink" title="B level"></a>B level</h3><ul><li><strong>gg</strong>:光标移动到页面顶部</li><li><strong>:e</strong>:打开一个文件</li><li><strong>:saveas</strong>:存盘</li><li><strong>:q!</strong>:退出不保存</li><li>多行复制</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">按v进入可视模式;按y复制;按p粘贴</span><br><span class="line"></span><br><span class="line">将第9行至第15行的数据，复制到第16行 </span><br><span class="line">：9，15 copy 16</span><br></pre></td></tr></table></figure><h3 id="vimium篇（浏览器工具）"><a href="#vimium篇（浏览器工具）" class="headerlink" title="vimium篇（浏览器工具）"></a>vimium篇（浏览器工具）</h3><p>gg:到页面顶部<br>G:到页面底部<br>d:向下滑动一页<br>u:向上滑动一页<br>f:在当前网页打开链接<br>F:在新页面打开链接</p><h2 id="多窗口管理工具-Screen"><a href="#多窗口管理工具-Screen" class="headerlink" title="多窗口管理工具 Screen"></a>多窗口管理工具 Screen</h2><h3 id="最常用命令"><a href="#最常用命令" class="headerlink" title="最常用命令"></a>最常用命令</h3><ul><li>创建新的窗口会话 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen 或者 screen -S yourname</span><br></pre></td></tr></table></figure></li><li>重新连接会话<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -r &lt;screen_pid&gt;</span><br></pre></td></tr></table></figure></li><li><p>补充：若意外断开后无法进入会话，则需要：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -D -r &lt;screen_pid&gt;</span><br></pre></td></tr></table></figure></li><li><p>查看所有screen会话 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -ls</span><br></pre></td></tr></table></figure></li><li>暂时断开会话 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">control+A d</span><br></pre></td></tr></table></figure></li><li>停止当前窗口 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">control+A k</span><br></pre></td></tr></table></figure></li><li>清除dead会话：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -wipe</span><br></pre></td></tr></table></figure></li></ul><h2 id="virtualenv环境管理"><a href="#virtualenv环境管理" class="headerlink" title="virtualenv环境管理"></a>virtualenv环境管理</h2><ol><li>安装<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br></pre></td></tr></table></figure></li><li><p>为工程创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> project_dir</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">virtualenv venv_name</span></span><br><span class="line">为环境选择python解释器：</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">virtualenv -p /usr/bin/python2.7 venv_name</span></span><br></pre></td></tr></table></figure></li><li><p>使用虚拟环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">source</span> venv_name/bin/activate</span></span><br></pre></td></tr></table></figure></li><li>停用虚拟环境<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">. venv_name/bin/deactivate</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="anaconda"><a href="#anaconda" class="headerlink" title="anaconda"></a>anaconda</h2><ol><li>下载anaconda<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">wget anaconda_latest_version_url</span></span><br></pre></td></tr></table></figure></li><li>安装anaconda<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bash anaconda_latest_version</span></span><br></pre></td></tr></table></figure></li><li>配置环境变量<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;export PATH=&quot;~/your_anaconda_version/bin:$PATH&quot;&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li>Conda环境管理<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建环境</span></span><br><span class="line">conda create --name your_env_name python=your_python_version</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">激活环境</span></span><br><span class="line">source activate your_env_name</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停用环境</span></span><br><span class="line">deactivate your_env_name</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除环境</span></span><br><span class="line">conda remove -name your_env_name --all</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看已安装的环境</span></span><br><span class="line">conda info -e</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="JupyterNotebook"><a href="#JupyterNotebook" class="headerlink" title="JupyterNotebook"></a>JupyterNotebook</h2><p>大名鼎鼎的软件。<br>我平时使用Pycharm作为IDE。然而最近学习Yjango的代码发现：演示教学方面JupyterNotebook有着得天独厚的优势（cell功能）。</p><h3 id="命令模式常用指令"><a href="#命令模式常用指令" class="headerlink" title="命令模式常用指令"></a>命令模式常用指令</h3><ul><li><code>jupyter notebook</code>: 从控制台打开jupyternotebook</li><li><code>Enter</code>: 进入编辑模式</li><li><code>Shift-Enter</code>: 运行cell，自动跳转下一个cell</li><li><code>Ctrl-Enter</code>: 运行cell</li><li><code>y</code>: cell进入代码状态</li><li><code>m</code>: cell进入markdown状态</li><li><code>a</code>: 上方插入cell</li><li><code>b</code>: 下方插入cell</li><li><code>z</code>: 恢复删除的最后一个cell</li><li><code>dd</code>: 删除选中的cell</li><li><code>s</code>: 保存文件</li></ul><h3 id="matplotlib集成"><a href="#matplotlib集成" class="headerlink" title="matplotlib集成"></a>matplotlib集成</h3><p>文件头加上如下代码可以在代码中显示图片。这个操作省去了一次性关闭一堆图片的麻烦。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p><h2 id="Commitizen"><a href="#Commitizen" class="headerlink" title="Commitizen"></a>Commitizen</h2><p>这个小工具专门负责减少<code>commit -m</code>时候的词穷感与code review时的无助感。</p><h3 id="1-安装-mac环境下"><a href="#1-安装-mac环境下" class="headerlink" title="1. 安装(mac环境下)"></a>1. 安装(mac环境下)</h3><ul><li><code>brew install node</code> 安装nodeJS</li><li><code>cnpm install -g commitizen</code> 使用cnpm全局安装commitizen</li><li><code>cnpm install -g</code></li></ul><h3 id="2-使用"><a href="#2-使用" class="headerlink" title="2. 使用"></a>2. 使用</h3><ul><li><code>cnpm install -g cz-conventional-changelog</code> 加载Angular规范模板文件</li><li><code>echo &#39;&#123; &quot;path&quot;: &quot;cz-conventional-changelog&quot; &#125;&#39; &gt; ~/.czrc</code> 使用Angular规范模板文件</li><li><code>git cz</code> 相当于 <code>git commit -m</code>了</li></ul><h3 id="3-使用进阶"><a href="#3-使用进阶" class="headerlink" title="3. 使用进阶"></a>3. 使用进阶</h3><ol><li>第一步填写<code>type</code><ul><li>feat :新功能 </li><li>fix :修复bug  </li><li>doc : 文档改变</li><li>style : 代码格式改变</li><li>refactor :某个已有功能重构</li><li>perf :性能优化</li><li>test :增加测试</li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsm760ekg7j20vg0g6wky.jpg" alt="实际使用效果图"></p><ol><li>第二步填写<code>scope</code>：此项为作用域，建议选项如下<ul><li>$all ：表示影响面大 ，如修改了项目框架会对整个程序产生影响。又或者全局文件</li><li>$loation： 表示影响小，某个小小的功能</li><li>$module：表示会影响某个模块 如登录模块、首页模块 、用户管理模块等等</li><li>自定义也是一个可选项，但要以<code>$</code>作为开头</li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsm7crl46cj20ve0biwia.jpg" alt="实际使用效果图"></p><ol><li>第三步填写其他信息：<ul><li><code>subject</code>:用来简要描述本次改动</li><li><code>body</code>:具体的修改信息 应该尽量详细</li><li><code>footer</code>:放置写备注啥的，如果是 bug ，可以把bug id放入</li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsm7i9aeztj20vi0h6ahg.jpg" alt="实际使用效果图"></p><h3 id="4-使用终阶"><a href="#4-使用终阶" class="headerlink" title="4. 使用终阶"></a>4. 使用终阶</h3><p>生成Change log:</p><ul><li><code>cnpm install -g conventional-changelog-cli</code> 首先安装客户端</li><li><code>conventional-changelog -p angular -i CHANGELOG.md -s -r 0</code> 直接生成log文件</li></ul><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fsmapxnl79j20va0eq0wt.jpg" alt="实际使用效果图"></p><h2 id="Linux数据库常用命令"><a href="#Linux数据库常用命令" class="headerlink" title="Linux数据库常用命令"></a>Linux数据库常用命令</h2><h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><ul><li>启动mysql服务<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql.server start</span><br><span class="line">```        </span><br><span class="line">- 打开mysql</span><br><span class="line">```    </span><br><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>参考与引用</p><ol><li><a href="https://www.cnblogs.com/technologylife/p/6635631.html">https://www.cnblogs.com/technologylife/p/6635631.html</a></li><li><a href="https://www.wangjingxian.cn/linux/41.html">https://www.wangjingxian.cn/linux/41.html</a></li><li><a href="https://www.cnblogs.com/ctaodream/p/6066694.html">https://www.cnblogs.com/ctaodream/p/6066694.html</a></li><li><a href="http://www.ruanyifeng.com/blog/2016/01/commit_message_change_log.html">http://www.ruanyifeng.com/blog/2016/01/commit_message_change_log.html</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim </tag>
            
            <tag> screen </tag>
            
            <tag> virtualenv </tag>
            
            <tag> anaconda </tag>
            
            <tag> jupyterNoteBook </tag>
            
            <tag> commitizen </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
